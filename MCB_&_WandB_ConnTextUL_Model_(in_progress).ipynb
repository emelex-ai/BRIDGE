{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emelex-ai/ConnTextUL/blob/main/MCB_%26_WandB_ConnTextUL_Model_(in_progress).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weyLFuSNnlGE"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DOWPm3x-nfyW"
      },
      "source": [
        "### Get Data\n",
        "\n",
        "Load some data into the content folder. This should be in our shared ConnTextUL folder. You may need to move the shared folder to a location in your drive with the same full path as indicated here. Or, we can devise a more efficient way to share data in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM3eWC-7h0Hi",
        "outputId": "6741b13b-49d8-4ec6-fa71-a60f64289f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "ln: failed to create symbolic link '/content/data': File exists\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "\n",
        "I_AM = 'Nathan'\n",
        "#I_AM = 'Tyler'\n",
        "\n",
        "if I_AM == 'Nathan':\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "  !ln -s \"/gdrive/My Drive/Projects/Modeling Reading Programs/ConnTextUL/data\" \"/content\"\n",
        "  DATA_PATH = \"/content/data/data.csv\"\n",
        "elif I_AM == 'Tyler':\n",
        "  drive.mount('/content/Hootie_Hoo')\n",
        "  DATA_PATH = \"/content/Hootie_Hoo/MyDrive/data_A.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HokPE-skVrf"
      },
      "source": [
        "# ConnTextUL Reading Model 001 (*with WandB logging & MCB's encoding*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHF8_MZV5Pz1"
      },
      "source": [
        "## Change Log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2TEtXU55LGX"
      },
      "source": [
        "-  (5/15/2023)\n",
        "   * Decoder inputs (necesary for teacher forcing) are now embedded properly in the *forward* function. This should resolve the attn_mask typing error.\n",
        "\n",
        "   --- Luke\n",
        "-  (05/14/2023)\n",
        "   * Modified both the character tokenizer and the phonology tokenizer to return more elaborate dictionaries with all of the input and masks needed for the encoders and decoders\n",
        "   * Completed a comprehensive walk through with examples for the orthography -> orthography pathway and the phonology -> phonology pathway in the **Examples** section at the bottom of the notebook.\n",
        "   * TODO: Take everything in the **Examples** section and put it in the appropriate places in the model and the training loop.\n",
        "   * ALMOST THERE!\n",
        "\n",
        "   --- Nathan\n",
        "-  (05/13/2023)\n",
        "   * Began to modify the phonemizer class and the dataset class to carry a _targets_ object that contains the binary class labels for the CrossEntropy criterion. \n",
        "   * TODO: We need to check the shapes of all objects being passed into the the CrossEntropy criterion and ensure that they all conform to the requirements specified in the documentation. This may include adding some additional padding. The necessary reshaping of the phonological vectors make reasoning in this higher dimensional space a bit difficult.\n",
        "   \n",
        "   ---Nathan\n",
        "-  (05/12/2023)\n",
        "   * Made numerous code refactoring changes\n",
        "   * Implemented the phonology_tokenizer and integrated it into the dataset class\n",
        "   * Reran the training loop code and worked out many bugs associated with shapes and batch size idiosyncrasies\n",
        "   * Added various try/except blocks and assert statements for shape and type constraints\n",
        "   * TODO: Finish the cross entropy loss function. Rerun the whole notebook and just keep rerunning the training loop and ironing out any remaining bugs\n",
        "   * NOTE: We need to think through carefully about how we are averaging the phonological vectors and whether to reinstert them as row or column vectors in the final phonological_input_tensor\n",
        "\n",
        "   --- Nathan\n",
        "-  (05/08/2023)\n",
        "   * Adjusted `__getitem__` method of `ConnTextUL` Dataset so that for batches (called with slice object), it returns a pair whose phoneme entry is an array (the batch) of phoneme arrays (corresponding to words) with SOS, EOS, and PAD index lists at front and back of array.\n",
        "\n",
        "   --- Tyler\n",
        "\n",
        "-  (05/07/2023)\n",
        "   * Finished/re-wrote `ConnTextUL` Dataset. I finished it once, but found that it would take 24+ hours to construct, because I had it querying CMUDict word-by-word. So I re-wrote the class to query CMUDict in a single batch. Because the `Traindata.traindata` dictionary re-orders/re-organizes all the phonological feature vectors differently than they would be ordered according to the input word list, I had to include some secondary re-organization to get the data back into the original input order. **One potential problem:** The `Traindata` constructor drops single-letter words and words with punctuation. The `__getitem__` method for the `ConnTextUL` class currently outputs a tuple consisting of a word string and its associated list-of-lists of the indices of nonzero entries in the phoneme feature vectors associated to the word. I included a instance of the `CharacterTokenizer` in the `ConnTextUL` class in case we want get item to output tokenized word instead of original word string. \n",
        "\n",
        "   --- Tyler\n",
        "\n",
        "-  (05/01/2023)\n",
        "   * Implemented the data ingestion/transformation code for training on the csv\n",
        "     file produced by Alia Pugh. \n",
        "   * Fixed the _init_ function in the CharacterTokenizer. Also, the encoder was using \n",
        "     [BOW], [EOW] but the _init_ function initialized [BOS], [EOS].\n",
        "   * Updated the encode routine in the CharacterTokenizer to add the [EOS] token before\n",
        "     the padding.\n",
        "   * Changed the GraphoneDataset class to be ConnTextULDataset class and incorporated\n",
        "     Alia's dataset and began incorporating Matt's Phonological vectors.\n",
        "\n",
        "   --- Nathan\n",
        "\n",
        "-  (4/29/2023)\n",
        "   *  Cloned Matt Cooper-Borkenhagen's Traindata repositroy and imported\n",
        "      associated library.\n",
        "   *  Did a little bit of testing to make sure I understand the dictionary\n",
        "      `Traindata.traindata` attribute of MCB's Traindata class.\n",
        "   *  Added testing and then comments throughout the notebook to make sure I\n",
        "      understand the model sturcture, in order to make necessary changes using\n",
        "      MCB's library.\n",
        "   *  Change outline (TODO): Replace phoneme tokensizer with a module that reutns an\n",
        "      array of MCB's vectors for each word, embeds these (linearly?) in a space of\n",
        "      dimension `d_model`, and then takes average per word.\n",
        "      HOW TO DO THIS: Redefine the phoneme embedding at the very beginning of GraphoneModel\n",
        "      to be a linear embedding from whatever space MCB avg-sums live into a space\n",
        "      of dimension d_model.\n",
        "\n",
        "   --- Tyler\n",
        "\n",
        "\n",
        "-  (4/25/2023)\n",
        "   *  Included WandB functionality throughout. Notebook now logs data in our\n",
        "      WandB `emelex/ConnTextUL_WandB_trial` project.\n",
        "\n",
        "   --- Tyler\n",
        "\n",
        "\n",
        "-  (4/2/2023)\n",
        "   * Implemented global attention for producing orthography/phoneme embedding. The previous\n",
        "     approach (concatenate all timesteps into a single vector + feedforward) was\n",
        "     memory-intensive and risked contaminating the embeddings w/ \n",
        "     padding.\n",
        "   * Removed some redundant layers in both encoder and decoder\n",
        "   * _generate_ still doesn't work; need to discuss tokenizer w/ group\n",
        "\n",
        "   --- Luke \n",
        "\n",
        "\n",
        "- (4/1/2023)\n",
        "  * Spin up Spot Instance with hosted notebook: Nathan\n",
        "  * Training Loop must be completed: Luke\n",
        "  * Finish the Generate Function\n",
        "  * Tensorboard probably to be used in some capacity: Nathan/Luke in the future\n",
        "\n",
        "  --- Nathan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpwcWFVDpqLv"
      },
      "source": [
        "## Dependencies and Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTnJA6mTdvf"
      },
      "source": [
        "### Import basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zixda65wfwMe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch as pt\n",
        "pt.manual_seed(57713)\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import tqdm\n",
        "import sys\n",
        "\n",
        "import contextlib\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frnAD1ZT7u7L"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukhxICTd3e2B"
      },
      "source": [
        "### Cloning/importing MCB's `Traindata` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRhOpcba3iOS",
        "outputId": "0ad9d2f8-80ee-4c0a-db0c-6a4e6b422e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.25.1-1ubuntu3.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_FWT7cp3vSi",
        "outputId": "b672032c-3d11-4c0c-ed2a-314ace7d249a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Traindata' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MCooperBorkenhagen/Traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpM8hE9u5qpF"
      },
      "source": [
        "Checking which of the library's dependencies are already present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wjyBHGK5vCV",
        "outputId": "073f39bc-4002-47d4-b0e5-ec9788f09411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fastjsonschema==2.16.3\n",
            "jsonpickle==3.0.1\n",
            "jsonschema==4.3.3\n",
            "nltk==3.8.1\n",
            "numpy==1.22.4\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep 'numpy\\|json\\|nltk\\|copy\\|string\\|utilities'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9io0F4rxS3HA",
        "outputId": "69b30f3b-ee10-4131-d37e-dcf33623bf69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Traindata\n"
          ]
        }
      ],
      "source": [
        "%cd Traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1In8OyGU4-uM"
      },
      "source": [
        "Import MCB's `Traindata` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjgx4O_M6RP4"
      },
      "outputs": [],
      "source": [
        "import Traindata\n",
        "from Traindata import Traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wQoCjn8yfZ"
      },
      "source": [
        "In order to use the `Traindata` library, we need the resourse `cmudict`. This is an online dictionary called the *The Carnegie Mellon University Pronouncing Dictionary*, which provides a standardized phonetic 'pronounciation' of words in a 134,000-word set. The following cell obtains this resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kejPMaVF93Lc",
        "outputId": "724524e5-f8d2-4314-cec9-3d0e858195ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Kitt247xHI"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3DErXcISanK"
      },
      "source": [
        "### Setup WandB\n",
        "These next 2 cells install WandB and connect the present notebook to WandB for logging and tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnkLwx-rSezH"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3S5SqKkUqM9",
        "outputId": "83324266-40ee-4fde-e63e-94da155b0068"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnathan-crock\u001b[0m (\u001b[33memelex\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUrJGG67yzu"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42SPhj0WiS7I"
      },
      "source": [
        "### Tokenizers\n",
        "\n",
        "**Endpoints:** The `encode` method of the `CharacterTokenizer` class takes as input a list of strings. I'm uncertain about requirements on the strings in this list. It seems to be fine if the strings are nonsense words or contain numerical characters.\n",
        "\n",
        "If we input a list of length $N$, then the `encode` method of this class outputs a tensor fo size $N\\!\\times\\!M$, where $M$ is related to the maximum length of the strings in our input list. That said, *it's not clear to me exactly how the dimension value $M$ is determined using the lengths of the strings in our list.*\n",
        "\n",
        "**Note.** Comments in cell immediately below explain what the `encode` method does. -- Tyler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebGlw6wpnIco"
      },
      "outputs": [],
      "source": [
        "class CUDA_Dict(dict):\n",
        "\n",
        "  def to(self, device):\n",
        "\n",
        "    output = {}\n",
        "    for key in self.keys():\n",
        "      batches = self[key]\n",
        "      if isinstance(batches, list):\n",
        "        try:\n",
        "          output[key] = [[val.to(device) for val in batch] for batch in batches]\n",
        "        except:\n",
        "          print(f\"batches = {batches}\")\n",
        "          raise\n",
        "      elif isinstance(batches, pt.Tensor):\n",
        "        output[key] = batches.to(device)\n",
        "      else:\n",
        "        raise TypeError(\"Must be list or torch tensor\")\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNJWIrk7ULS_"
      },
      "source": [
        "#### Character Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb6AGvZfIHel"
      },
      "outputs": [],
      "source": [
        "class CharacterTokenizer:\n",
        "    def __init__(self, list_of_characters):\n",
        "\n",
        "        # Include custom tokens into vocabulary\n",
        "        self.vocab = ['[BOS]', '[EOS]', '[CLS]', '[UNK]', '[PAD]']\n",
        "        self.vocab.extend(list_of_characters)\n",
        "\n",
        "        self.char_2_idx, self.idx_2_char = {}, {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "          self.char_2_idx[ch] = i \n",
        "          self.idx_2_char[i] = ch\n",
        "        # Reuse index from previous for loop to save computation\n",
        "        self.size = i+1\n",
        "\n",
        "\n",
        "    def __len__(self): return self.size\n",
        "\n",
        "\n",
        "    def encode(self, list_of_strings):\n",
        "        assert isinstance(list_of_strings, str) or (isinstance(list_of_strings, list) \\\n",
        "                 and all(isinstance(string, str) for string in list_of_strings))\n",
        "        if isinstance(list_of_strings, str): list_of_strings = [list_of_strings]\n",
        "\n",
        "        lengths = [len(string) for string in list_of_strings]\n",
        "        max_length = max(lengths)\n",
        "\n",
        "        # Padding function, puts beginning-of-string and end-of-string tokens\n",
        "        # on beginning and end of string, after padding the original string\n",
        "        # up to max length of string.\n",
        "        # This function converts the output to a list rather than string.\n",
        "        enc_pad = lambda string: ['[BOS]'] + list(string) + ['[EOS]'] + (max_length - len(string)) * ['[PAD]']\n",
        "        dec_pad = lambda string: ['[BOS]'] + list(string) + (max_length - len(string)) * ['[PAD]']\n",
        "\n",
        "        list_of_enc_strings = list(map(enc_pad, list_of_strings))\n",
        "        list_of_dec_strings = list(map(dec_pad, list_of_strings))\n",
        "\n",
        "        # Initiate encoder tokens tensor as tensor of zeros, \n",
        "        enc_input_ids = pt.zeros((len(list_of_enc_strings), 2 + max_length), dtype=pt.long)\n",
        "        for idx, string in enumerate(list_of_enc_strings):\n",
        "            for jdx, char in enumerate(string):\n",
        "                enc_input_ids[idx, jdx] = self.char_2_idx.get(char, 3) # Default to [UNK]\n",
        "\n",
        "        # Initiate decoder tokens tensor as tensor of zeros, \n",
        "        dec_input_ids = pt.zeros((len(list_of_dec_strings), 1 + max_length), dtype=pt.long)\n",
        "        for idx, string in enumerate(list_of_dec_strings):\n",
        "            for jdx, char in enumerate(string):\n",
        "                dec_input_ids[idx, jdx] = self.char_2_idx.get(char, 3) # Default to [UNK]                \n",
        "\n",
        "        PAD_TOKEN = self.char_2_idx['[PAD]']\n",
        "        enc_pad_mask = (enc_input_ids == PAD_TOKEN)\n",
        "        dec_pad_mask = (dec_input_ids == PAD_TOKEN)\n",
        "        return CUDA_Dict({'enc_input_ids':enc_input_ids,\n",
        "                          'dec_input_ids':dec_input_ids,\n",
        "                          'enc_pad_mask':enc_pad_mask.bool(),\n",
        "                          'dec_pad_mask':dec_pad_mask.bool()})\n",
        "            \n",
        "\n",
        "    def decode(self, list_of_ints):\n",
        "        assert isinstance(list_of_ints,int) or (isinstance(list_of_ints,int) \\\n",
        "                 and all(isinstance(ints,int) for ints in list_of_ints))\n",
        "        if isinstance(list_of_ints,int): list_of_ints = [list_of_ints]\n",
        "\n",
        "        outputs = [''.join([self.idx_2_char.get(i) for i in ints]) for ints in list_of_ints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hko4ro_yUMwg",
        "outputId": "74d5fe79-8887-45d4-f3b4-d0b6566e2fa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'[BOS]': 0,\n",
              " '[EOS]': 1,\n",
              " '[CLS]': 2,\n",
              " '[UNK]': 3,\n",
              " '[PAD]': 4,\n",
              " 'r': 5,\n",
              " 't': 6,\n",
              " 'a': 7,\n",
              " 'e': 8,\n",
              " 'p': 9,\n",
              " 'n': 10,\n",
              " 'f': 11,\n",
              " 'c': 12,\n",
              " 'u': 13,\n",
              " 's': 14,\n",
              " 'd': 15,\n",
              " 'h': 16,\n",
              " 'w': 17,\n",
              " 'i': 18,\n",
              " 'l': 19,\n",
              " 'o': 20,\n",
              " 'I': 21,\n",
              " 'g': 22,\n",
              " 'k': 23,\n",
              " 'm': 24,\n",
              " 'b': 25}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First create the list_of_characters object to initialize the tokenizer\n",
        "vocab = ['one', 'thing', 'I', 'know', 'two', 'three', 'four', 'what', 'hercules', 'theoretical', 'blegrid', 'phrempral']\n",
        "list_of_characters = set(''.join([c for word in vocab for c in word])) # Don't use this if vocab is large\n",
        "character_tokenizer = CharacterTokenizer(list_of_characters)\n",
        "character_tokenizer.char_2_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTY5BH5-UZNC",
        "outputId": "68f806ac-e727-465a-bb10-6dd0f9397656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enc_input_ids\n",
            "tensor([[ 0, 16,  8,  5, 12, 13, 19,  8, 14,  1],\n",
            "        [ 0, 23, 18, 15,  1,  4,  4,  4,  4,  4],\n",
            "        [ 0,  7,  1,  4,  4,  4,  4,  4,  4,  4],\n",
            "        [ 0, 17, 16,  7, 19,  8,  1,  4,  4,  4]])\n",
            "enc_pad_mask\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
            "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False,  True,  True,  True]])\n",
            "dec_input_ids\n",
            "tensor([[ 0, 16,  8,  5, 12, 13, 19,  8, 14],\n",
            "        [ 0, 23, 18, 15,  4,  4,  4,  4,  4],\n",
            "        [ 0,  7,  4,  4,  4,  4,  4,  4,  4],\n",
            "        [ 0, 17, 16,  7, 19,  8,  4,  4,  4]])\n",
            "dec_pad_mask\n",
            "tensor([[False, False, False, False, False, False, False, False, False],\n",
            "        [False, False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False,  True,  True,  True]])\n"
          ]
        }
      ],
      "source": [
        "Y = character_tokenizer.encode(['hercules', 'kid', 'a', 'whale'])\n",
        "enc_input_ids = Y['enc_input_ids']\n",
        "print(f\"enc_input_ids\\n{enc_input_ids}\")\n",
        "enc_pad_mask = Y['enc_pad_mask']\n",
        "print(f\"enc_pad_mask\\n{enc_pad_mask}\")\n",
        "dec_input_ids = Y['dec_input_ids']\n",
        "print(f\"dec_input_ids\\n{dec_input_ids}\")\n",
        "dec_pad_mask = Y['dec_pad_mask']\n",
        "print(f\"dec_pad_mask\\n{dec_pad_mask}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v62siOBnm0VC"
      },
      "source": [
        "#### Phonology Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhiL3REbmvt2"
      },
      "outputs": [],
      "source": [
        "# We may want to calculate all of the phonological vectors for the entire dataset ahead of time here.\n",
        "# This way, we can just look up the vector instead of calculating it on the fly. That will save time\n",
        "# during training.\n",
        "\n",
        "class Phonemizer():\n",
        "\n",
        "  def __init__(self, wordlist):\n",
        "\n",
        "    self.PAD = 33\n",
        "\n",
        "    traindata = Traindata(wordlist, \n",
        "                          phonpath='raw/phonreps.csv',  \n",
        "                          terminals=True,\n",
        "                          oneletter=True,\n",
        "                          verbose=False).traindata\n",
        "    self.enc_inputs = {}\n",
        "    self.dec_inputs = {}\n",
        "    self.targets = {}\n",
        "    for length in traindata.keys():\n",
        "      for word_num, (phon_vec_sos, phon_vec_eos) in enumerate(zip(traindata[length]['phonSOS'], traindata[length]['phonEOS'])):\n",
        "        word = traindata[length]['wordlist'][word_num]\n",
        "        # The encoder receives the entire phonological vector include the BOS and EOS tokens\n",
        "        self.enc_inputs[word] = [pt.tensor(np.where(vec)[0], dtype=pt.long) for vec in phon_vec_sos] + [pt.tensor([32])] # 32 is the EOS token location\n",
        "        # The decoder received the entire phonological vectors including the BOS token, but not the EOS token\n",
        "        self.dec_inputs[word] = [pt.tensor(np.where(vec)[0], dtype=pt.long) for vec in phon_vec_sos]\n",
        "        # The target for the decoder is all phonological vectors including the EOS token, but excluding the BOS token\n",
        "        self.targets[word] = phon_vec_eos\n",
        "\n",
        "    del traindata\n",
        "\n",
        "  def __len__(self):\n",
        "    \n",
        "    return 34\n",
        "\n",
        "  def encode(self, wordlist):\n",
        "\n",
        "    enc_input_ids = []\n",
        "    dec_input_ids = []\n",
        "    targets = []\n",
        "\n",
        "    if isinstance(wordlist, list):\n",
        "      max_length = 0\n",
        "      for word in wordlist:\n",
        "        # Make sure all words are in the phonological dictionary\n",
        "        enc_input = self.enc_inputs.get(word, None)\n",
        "        dec_input = self.dec_inputs.get(word, None)\n",
        "        target = self.targets.get(word, None)\n",
        "        # If any word is not in the dictionary, skip the batch\n",
        "        if enc_input is None or dec_input is None or target is None:\n",
        "          return None\n",
        "        # Collect all token lists in a larger list while calculating the max length of this batch\n",
        "        enc_input_ids.append(enc_input.copy())\n",
        "        dec_input_ids.append(dec_input.copy())\n",
        "        targets.append(pt.tensor(target.copy(), dtype=pt.long))\n",
        "        # All three, enc_input, dec_input, and target should be the same length. So all share the same max_length\n",
        "        # (though we subtract 1 from the decoder input and targets because the BOS/EOS tokens were removed)\n",
        "        max_length = max(max_length, len(enc_input))\n",
        "      # Now that we know the max length of this batch, we pad the encoder and decoder input token list with PAD tokens\n",
        "      for epv, dpv in zip(enc_input_ids, dec_input_ids):\n",
        "        epv.extend([pt.tensor([self.PAD])]*(max_length-len(epv)))\n",
        "        dpv.extend([pt.tensor([self.PAD])]*(max_length-1-len(dpv)))\n",
        "      # We then include padding, or indices in the targets to be passed to the 'ignore_index' parameter in the CrossEntropyLoss\n",
        "      # Since each phonological vector is either on or off, it is a binary classification problem, so valid labels are either 0, or 1.\n",
        "      # We will include labels of '2' where the padding is in the target vectors\n",
        "      #print(\"targets = \", targets)\n",
        "      for i in range(len(targets)):\n",
        "        tv = targets[i]\n",
        "        targets[i] = pt.concat((tv, pt.tensor([[2]*33]*(max_length-1-len(tv)), dtype=pt.long)))\n",
        "        #print(\"len(tv) = \", len(tv))\n",
        "        #tv = pt.concat((tv, pt.tensor([[2]*33]*(max_length-len(tv)))))\n",
        "        #print(\"tv = \", tv)\n",
        "        #sys.exit()\n",
        "    else:\n",
        "      raise TypeError('encode only accepts lists or a single string as input')\n",
        "\n",
        "    enc_pad_mask = pt.tensor([[all(val == pt.tensor([self.PAD])) for val in token] for token in enc_input_ids])\n",
        "    dec_pad_mask = pt.tensor([[all(val == pt.tensor([self.PAD])) for val in token] for token in dec_input_ids])\n",
        "    #dec_pad_mask = pt.tensor([1])\n",
        "\n",
        "    # Ensure that the number of tokens matches the number of boolean values in the mask\n",
        "    assert len(enc_input_ids) == len(enc_pad_mask), f\"tokens is length {len(enc_input_ids)}, enc_pad_mask is length {len(enc_pad_mask)}. They must be equal\"\n",
        "\n",
        "    # Do we need to pad the targets? We do to convert it to a tensor which is needed for the CrossEntropy criterion\n",
        "    return CUDA_Dict({'enc_input_ids':enc_input_ids, \n",
        "                      'enc_pad_mask':enc_pad_mask.bool(), \n",
        "                      'dec_input_ids':dec_input_ids,\n",
        "                      'dec_pad_mask':dec_pad_mask.bool(),\n",
        "                      'targets':pt.stack(targets, 0)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBqaR9sRnOQA"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(DATA_PATH)\n",
        "words = dataset['word_raw'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nov9EeDxs_n"
      },
      "outputs": [],
      "source": [
        "phonemizer = Phonemizer(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTQ_CkSE218_",
        "outputId": "0b4f8fcc-0274-4b72-e451-cd9f1be19bb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'enc_input_ids': [[tensor([31]),\n",
              "   tensor([ 0, 11]),\n",
              "   tensor([14, 15, 18, 19, 24, 27, 29]),\n",
              "   tensor([ 2, 10, 14]),\n",
              "   tensor([32])],\n",
              "  [tensor([31]),\n",
              "   tensor([5, 7]),\n",
              "   tensor([14, 15, 23, 27, 29]),\n",
              "   tensor([32]),\n",
              "   tensor([33])]],\n",
              " 'enc_pad_mask': tensor([[False, False, False, False, False],\n",
              "         [False, False, False, False,  True]]),\n",
              " 'dec_input_ids': [[tensor([31]),\n",
              "   tensor([ 0, 11]),\n",
              "   tensor([14, 15, 18, 19, 24, 27, 29]),\n",
              "   tensor([ 2, 10, 14])],\n",
              "  [tensor([31]), tensor([5, 7]), tensor([14, 15, 23, 27, 29]), tensor([33])]],\n",
              " 'dec_pad_mask': tensor([[False, False, False, False],\n",
              "         [False, False, False,  True]]),\n",
              " 'targets': tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "           0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n",
              "          [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
              " \n",
              "         [[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "           1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
              "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "          [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "           2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]])}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val = phonemizer.encode(['whale', 'hi']); val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeW3n7JRXixS",
        "outputId": "ba7c5029-827d-4628-8195-1671b417fc02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 33])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val['targets'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYTPryWBiU5y"
      },
      "source": [
        "### ConnTextUL Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXOnczXOgsoj"
      },
      "outputs": [],
      "source": [
        "class ConnTextULDataset(Dataset):\n",
        "  \"\"\"ConnTextULDataset\n",
        "\n",
        "  Dataset of \n",
        "\n",
        "  For Matt's Phonoligical Feature Vectors, we will use (31, 32, 33) to represent ('[BOS]', '[EOS]', '[PAD]')\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "      self.dataset = pd.read_csv(DATA_PATH)\n",
        "      tmp_words = self.dataset['word_raw'].str.lower() # Series of all lowercased words\n",
        "      self.phonology_tokenizer = Phonemizer(tmp_words)\n",
        "      # self.listed_words = [word for word in self.words]\n",
        "\n",
        "      # Notice I created a tokenizer in this class.\n",
        "      # We can use it to tokenize word output of __getitem__ below,\n",
        "      # although I haven't implemented yet.\n",
        "      list_of_characters = set(''.join([c for word in tmp_words for c in word]))\n",
        "      self.character_tokenizer = CharacterTokenizer(list_of_characters)\n",
        "\n",
        "      final_words = []\n",
        "      for word in tmp_words:\n",
        "        if word == \"\" or word == None or word == []:\n",
        "          continue\n",
        "        if self.phonology_tokenizer.encode([word]): #check if in phoneme_dict\n",
        "          final_words.append(word)\n",
        "\n",
        "      self.words = final_words\n",
        "\n",
        "          \n",
        "  def __len__(self):\n",
        "      length = len(self.words)  \n",
        "      return length  \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "      string_input = self.words[idx]\n",
        "      # tokenized orthographic output. character_tokenizer pads and wraps in tensor\n",
        "      orth_tokenized = self.character_tokenizer.encode(string_input)\n",
        "      phon_tokenized = self.phonology_tokenizer.encode(string_input)\n",
        "\n",
        "      return {'orthography': orth_tokenized, 'phonology': phon_tokenized}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4H8CfEUvZrh"
      },
      "source": [
        "Testing:\n",
        "\n",
        "*The construciton of the `ConnTextULDataset` instance here takes about 30 seconds.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-onTesCl8nV"
      },
      "outputs": [],
      "source": [
        "conntextul_dataset = ConnTextULDataset()\n",
        "length = len(conntextul_dataset)\n",
        "print('\\nDataset length:', length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-MaiR0uNJ3G",
        "outputId": "623f9f76-f0e4-40ed-ae31-8ed311fd3dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Some random examples, in the form (word, phoneme_feature_indices)\n",
            "\n",
            "\n",
            "conntextul_dataset[slice(29425, 29427, None)]['orthography']['enc_input_ids']\n",
            " tensor([[ 0,  8, 36, 43, 46,  9,  1],\n",
            "        [ 0, 24, 26, 12,  1,  4,  4]])\n",
            "\n",
            "conntextul_dataset[slice(29425, 29427, None)]['orthography']['enc_pad_mask']\n",
            " tensor([[False, False, False, False, False, False, False],\n",
            "        [False, False, False, False, False,  True,  True]])\n",
            "\n",
            "conntextul_dataset[slice(29425, 29427, None)]['phonology']['enc_input_ids']\n",
            " [[tensor([31]), tensor([14, 16, 21]), tensor([ 0, 11]), tensor([14, 17, 19, 24, 26, 28, 29]), tensor([4, 6]), tensor([32])], [tensor([31]), tensor([4, 6]), tensor([14, 16, 21, 29]), tensor([0, 6]), tensor([32]), tensor([33])]]\n",
            "\n",
            "conntextul_dataset[slice(29425, 29427, None)]['phonology']['enc_pad_mask']\n",
            " tensor([[False, False, False, False, False, False],\n",
            "        [False, False, False, False, False,  True]])\n",
            "\n",
            "conntextul_dataset[slice(63034, 63036, None)]['orthography']['enc_input_ids']\n",
            " tensor([[ 0, 25,  8, 37, 29,  1],\n",
            "        [ 0, 22, 43,  6,  1,  4]])\n",
            "\n",
            "conntextul_dataset[slice(63034, 63036, None)]['orthography']['enc_pad_mask']\n",
            " tensor([[False, False, False, False, False, False],\n",
            "        [False, False, False, False, False,  True]])\n",
            "\n",
            "conntextul_dataset[slice(63034, 63036, None)]['phonology']['enc_input_ids']\n",
            " [[tensor([31]), tensor([2, 7]), tensor([14, 15, 21, 23, 24, 29]), tensor([ 2,  6, 14]), tensor([32]), tensor([33])], [tensor([31]), tensor([0, 1, 7]), tensor([14, 17, 23, 29]), tensor([4, 6]), tensor([2, 7]), tensor([32])]]\n",
            "\n",
            "conntextul_dataset[slice(63034, 63036, None)]['phonology']['enc_pad_mask']\n",
            " tensor([[False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]])\n"
          ]
        }
      ],
      "source": [
        "print('\\nSome random examples, in the form (word, phoneme_feature_indices)\\n')\n",
        "NUM = 2\n",
        "for _ in range(NUM):\n",
        "  start = random.randrange(length-NUM)\n",
        "  idx = slice(start, start+NUM) \n",
        "  orth, phon = conntextul_dataset[idx]\n",
        "  print(f\"\\nconntextul_dataset[{idx}]['orthography']['enc_input_ids']\\n\", conntextul_dataset[idx]['orthography']['enc_input_ids'])\n",
        "  print(f\"\\nconntextul_dataset[{idx}]['orthography']['enc_pad_mask']\\n\", conntextul_dataset[idx]['orthography']['enc_pad_mask'])\n",
        "  print(f\"\\nconntextul_dataset[{idx}]['phonology']['enc_input_ids']\\n\", conntextul_dataset[idx]['phonology']['enc_input_ids'])\n",
        "  print(f\"\\nconntextul_dataset[{idx}]['phonology']['enc_pad_mask']\\n\", conntextul_dataset[idx]['phonology']['enc_pad_mask']) \n",
        "  #print('Dataset item {}:'.format(idx), conntextul_dataset[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTh0uT9qAxn"
      },
      "source": [
        "## The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41Yt3jGBn2S-"
      },
      "outputs": [],
      "source": [
        "d_model=512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvAw7jONiYTx"
      },
      "source": [
        "### Encoder module\n",
        "**Endpoints:** The `Encoder` module takes in a torch tensor of size either $N\\!\\times\\!d_{model}$ or $B\\!\\times\\!N\\!\\times d_{model}$, where $B$ denotes batch size and $N$ denotes sequence length, and then outputs a torch tensor of corresponding size $N\\!\\times\\!d_{model}$ or $B\\!\\times\\!N\\!\\times d_{model}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5HHY76YNWDT"
      },
      "outputs": [],
      "source": [
        "class Encoder(pt.nn.Module):\n",
        "    def __init__(self, d_model=512, nhead=1, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        encoder_layer = pt.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer_encoder = pt.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J8xw3kdYZaf"
      },
      "source": [
        "### Decoder module\n",
        "**Endpoints:** The `Decoder` module takes in a pair of torch tensors. The first tensor argument, denoted `tgt` in the code below, is the sequence to the decoder. The second tensor argument, denoted `memory` in the code below, is  the sequence from the last layer of the encoder.\n",
        "\n",
        "Each of the `tgt` and `memory` arguments is a torch tensor of size either $N\\!\\times\\!d_{model}$ or $B\\!\\times\\!N\\!\\times d_{model}$, where $B$ denotes batch size and $N$ denotes sequence length. The `Decoder` module outputs a torch tensor of corresponding size $N\\!\\times\\!d_{model}$ or $B\\!\\times\\!N\\!\\times d_{model}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgqgdUOlYUws"
      },
      "outputs": [],
      "source": [
        "class Decoder(pt.nn.Module):\n",
        "    def __init__(self, d_model=512, nhead=1, num_layers=1):\n",
        "        super().__init__()\n",
        "        decoder_layer = pt.nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer_decoder = pt.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        output = self.transformer_decoder(tgt, memory, \n",
        "                                            tgt_mask=tgt_mask, \n",
        "                                            memory_mask=memory_mask, \n",
        "                                            tgt_key_padding_mask=tgt_key_padding_mask, \n",
        "                                            memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puvCpSGWapbs"
      },
      "source": [
        "Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW5dndubYkpA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP9yaL9fayA2",
        "outputId": "e42333f1-008a-4657-dceb-54467492dadc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerDecoder(\n",
              "  (layers): ModuleList(\n",
              "    (0): TransformerDecoderLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (multihead_attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      (dropout3): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder.transformer_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSiA9P8aa5FX",
        "outputId": "a34cc107-4833-405b-cc1b-408453a63aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " <class 'torch.Tensor'>\n",
            "torch.Size([64, 512])\n",
            "\n",
            " <class 'torch.Tensor'>\n",
            "torch.Size([64, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "a = pt.rand(64, 512)\n",
        "b = pt.rand(64, 512)\n",
        "y = decoder(a, b)\n",
        "print('\\n', type(y))\n",
        "print(y.shape)\n",
        "\n",
        "a = pt.rand(64, 10, 512)\n",
        "b = pt.rand(64, 10, 512)\n",
        "y = decoder(a, b)\n",
        "print('\\n', type(y))\n",
        "print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAImqVNiaCB"
      },
      "source": [
        "### The Model\n",
        "This is the PyTorch `nn` class that puts everything together to produce our ConnTextUL model. Comments within the code describe how all the class attributes correspond to segments in [our model diagram](https://drive.google.com/file/d/1QVzH68d4qowdYgW5YzQI1ADN-p9OeBDF/view?usp=share_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwf0vKAPFF9k"
      },
      "outputs": [],
      "source": [
        "class Model(pt.nn.Module):\n",
        "    def __init__(self,\n",
        "                 orth_vocab_size,\n",
        "                 phon_vocab_size,\n",
        "                 d_model=512,\n",
        "                 nhead=1,\n",
        "                 num_layers=1,\n",
        "                 max_seq_len=20):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial embeddings for orthography, phonology, and position\n",
        "        # Emebdding for orthography\n",
        "        self.orthography_embedding = pt.nn.Embedding(orth_vocab_size, d_model)\n",
        "        # Embedding for phonology\n",
        "        self.phonology_embedding = pt.nn.Embedding(phon_vocab_size, d_model)\n",
        "        self.position_embedding = pt.nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        self.vocab_sizes = {'orth_vocab_size': orth_vocab_size, \n",
        "                            'phon_vocab_size': phon_vocab_size}\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # A 1×1×d_model tensor of model parameters, rescaled by √d_model\n",
        "        self.global_embedding = pt.nn.Parameter(pt.randn((1,1,self.d_model))/self.d_model**.5,requires_grad=True)\n",
        "\n",
        "        # Initial, encoding segment of our ConnTextUL model:\n",
        "        # Instance of our Encoder module (defined above), for encoding orthography\n",
        "        self.orthography_encoder = Encoder(d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
        "        # Instance of our Encoder module (defined above), for encoding phonology\n",
        "        self.phonology_encoder = Encoder(d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
        "\n",
        "        # Criss-crossing orthography/phonology cross-attenion segment of ConnTextUL model\n",
        "        self.gp_multihead_attention = pt.nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
        "        self.pg_multihead_attention = pt.nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
        "\n",
        "        # Segment of ConnTextUL model that mixes orthography/phonology representation\n",
        "        self.transformer_mixer = Encoder(d_model=self.d_model, nhead=nhead, num_layers=num_layers)\n",
        "        self.reduce = pt.nn.Linear(self.d_model,self.d_model)\n",
        "\n",
        "        # Decoder segment of ConnTextUL model\n",
        "        # Orthography component of Decoder segment\n",
        "        self.orthography_decoder = Decoder(d_model=self.d_model, nhead=nhead, num_layers=num_layers)\n",
        "        self.linear_orthography_decoder = pt.nn.Linear(self.d_model, self.vocab_sizes['orth_vocab_size'])\n",
        "        # Phonology component of Decoder segment\n",
        "        self.phonology_decoder = Decoder(d_model=self.d_model, nhead=nhead, num_layers=num_layers)\n",
        "        self.linear_phonology_decoder = pt.nn.Linear(self.d_model, 2*(self.vocab_sizes['phon_vocab_size']-1))\n",
        "\n",
        "    # Returns a size×size, strictly upper-triangular Boolean tensor\n",
        "    def generate_triangular_mask(self, size, device):\n",
        "        mask = pt.triu(pt.ones((size, size), dtype=pt.bool, device=device), 1)\n",
        "        return mask\n",
        "\n",
        "    def embed_tokens(self, tokens, mode='o'):\n",
        "        assert mode in ['o','p']\n",
        "\n",
        "        if mode == 'o':\n",
        "           assert isinstance(tokens, pt.Tensor), \"For orthographic embeddings, tokens must be a pytorch tensor of integers (indices of orthography_embedding)\"\n",
        "           return self.orthography_embedding(tokens) + self.position_embedding.weight[None,:tokens.shape[1]]\n",
        "        # This is where we need to average the phonological embedding vectors\n",
        "        else:\n",
        "           try:\n",
        "             isinstance(tokens, list)\n",
        "           except:\n",
        "             raise TypeError(f\"For phonological vectors, tokens must be a list where each element is a pytorch tensor of integers (indices), but is type: {type(tokens)}\")\n",
        "           try:\n",
        "             all(isinstance(token, pt.Tensor) for token in tokens)\n",
        "           except:\n",
        "             for token in tokens:\n",
        "               print(f\"type(token) = {type(token)}\")\n",
        "               print(f\"token = {token}\")\n",
        "             raise TypeError(\"For phonological vectors, each element of the list must be a pytorch tensor of integers (indices)\")\n",
        "           # Here we average the embeddings for each feature in a phonological vector\n",
        "           # Each row of indices will become of batch once we extract rows from the embedding matrix\n",
        "           # So the size of the resulting 'output_embedding' tensor should be (batch_size, max_phon_len, d_model)\n",
        "           output_embedding = pt.zeros((len(tokens), len(tokens[0]), self.d_model))\n",
        "           for batch_num, batch in enumerate(tokens):\n",
        "             for indx, tokes in enumerate(batch):\n",
        "               # Here tokens should be a pytorch tensor of integers. It extracts the indicated rows from self.phonology_embedding\n",
        "               avg_embedding = self.phonology_embedding(tokes).mean(axis=0)\n",
        "               # Insert the resulting averaged embedding vector into the output_embedding tensor as a new row\n",
        "               output_embedding[batch_num, indx, :] = avg_embedding\n",
        "           return output_embedding + self.position_embedding.weight[None,:len(tokens[0])] \n",
        "\n",
        "    def embed(self, orthography, orthography_padding_mask, phonology, phonology_padding_mask):\n",
        "        orthography, phonology = self.embed_tokens(orthography, 'o'), self.embed_tokens(phonology, 'p')\n",
        "\n",
        "        orthography_encoding = self.orthography_encoder(orthography, src_key_padding_mask=orthography_padding_mask)\n",
        "        try:\n",
        "          phonology_encoding = self.phonology_encoder(phonology, src_key_padding_mask=phonology_padding_mask)\n",
        "        except:\n",
        "          print(f\"phonology = {phonology}\")\n",
        "          print(f\"phonology_padding_mask = {phonology_padding_mask}\")\n",
        "          raise\n",
        "          \n",
        "\n",
        "        # Query = orthography_encoding, Key = phonology_encoding\n",
        "        gp_encoding = self.gp_multihead_attention(orthography_encoding, phonology_encoding, phonology_encoding,\n",
        "                                                  key_padding_mask = phonology_padding_mask)[0]\n",
        "        # Query = phonology_encoding, Key = orthography_encoding\n",
        "        pg_encoding = self.pg_multihead_attention(phonology_encoding, orthography_encoding, orthography_encoding,\n",
        "                                                  key_padding_mask = orthography_padding_mask)[0]\n",
        "\n",
        "        # Concatenate outputs of cross-attention modules\n",
        "        gp_pg = pt.cat((gp_encoding, pg_encoding), dim=1) + pt.cat((orthography_encoding, phonology_encoding), dim=1)\n",
        "        # Concatenate padding masks\n",
        "        gp_pg_padding_mask = pt.cat((orthography_padding_mask, phonology_padding_mask),dim=-1)\n",
        "\n",
        "        # Here `global_embedding` is just a torch Parameter tensor.\n",
        "        # If I'm not mistaken, the `repeat` method is dictating that the same\n",
        "        # parameter tensor be applied to every entry in the batch.\n",
        "        gp_pg = pt.cat((self.global_embedding.repeat(gp_pg.shape[0], 1, 1), gp_pg), dim=1)\n",
        "        gp_pg_padding_mask = pt.cat((pt.zeros((gp_pg.shape[0], 1), device=gp_pg.device, dtype=pt.bool), gp_pg_padding_mask), dim=-1)\n",
        "        mixed_encoding = self.transformer_mixer(gp_pg, src_key_padding_mask=gp_pg_padding_mask) \n",
        "\n",
        "        final_encoding = self.reduce(mixed_encoding[:,0]).unsqueeze(-2)\n",
        "\n",
        "        return final_encoding\n",
        "\n",
        "\n",
        "    def forward(self, orth_enc_input, orth_enc_pad_mask, orth_dec_input, orth_dec_pad_mask,\n",
        "                      phon_enc_input, phon_enc_pad_mask, phon_dec_input, phon_dec_pad_mask):\n",
        "        mixed_encoding = self.embed(orth_enc_input, orth_enc_pad_mask, \n",
        "                                    phon_enc_input, phon_enc_pad_mask)\n",
        "\n",
        "        orth_ar_mask = self.generate_triangular_mask(orth_dec_input.shape[1], orth_dec_input.device)\n",
        "        orth_dec_input = self.embed_tokens(orth_dec_input,'o')\n",
        "\n",
        "        #print(\"Type of orth_dec_input = \", orth_dec_input.dtype)\n",
        "        #print(\"Type of orth_ar_mask = \", orth_ar_mask.dtype)\n",
        "        #print(\"Type of orth_pec_pad_mask = \", orth_dec_pad_mask.dtype)\n",
        "        #print(\"Type of mixed_encoding = \", mixed_encoding.dtype)\n",
        "\n",
        "        orth_output = self.orthography_decoder(tgt=orth_dec_input, \n",
        "                                               tgt_mask=orth_ar_mask,\n",
        "                                               tgt_key_padding_mask=orth_dec_pad_mask,\n",
        "                                               memory=mixed_encoding)\n",
        "\n",
        "        phon_dec_input = self.embed_tokens(phon_dec_input,'p')\n",
        "        phon_ar_mask = self.generate_triangular_mask(phon_dec_input.shape[1], phon_dec_input.device)\n",
        "\n",
        "        #print(\"Type of phon_dec_input = \", phon_dec_input.dtype)\n",
        "        #print(\"Type of phon_ar_mask = \", phon_ar_mask.dtype)\n",
        "        #print(\"Type of phon_pec_pad_mask = \", phon_dec_pad_mask.dtype)\n",
        "        phon_output = self.phonology_decoder(tgt=phon_dec_input, \n",
        "                                              tgt_mask=phon_ar_mask,\n",
        "                                              tgt_key_padding_mask=phon_dec_pad_mask,\n",
        "                                              memory=mixed_encoding)\n",
        "        \n",
        "        B, OC, E = orth_output.shape\n",
        "        orth_output = orth_output.view(B*OC, E)\n",
        "        orth_token_logits = self.linear_orthography_decoder(orth_output)\n",
        "        B, PC, E = phon_output.shape\n",
        "        phon_output = phon_output.view(B*PC, E)\n",
        "        phon_token_logits = self.linear_phonology_decoder(phon_output)\n",
        "        #print(\"(B, C))\n",
        "        #print(\"orth_token_logits.shape = \", orth_token_logits.shape)\n",
        "        #print(\"phon_token_logits.shape = \", phon_token_logits.shape)\n",
        "\n",
        "        return {'orth': orth_token_logits.view(B, self.vocab_sizes['orth_vocab_size'], OC), \n",
        "                'phon': phon_token_logits.view(B, 2, PC, self.vocab_sizes['phon_vocab_size'] - 1)} # -1 because targets do not contain PAD\n",
        "\n",
        "\n",
        "    def generate(self, orthography, orthography_mask, phonology, phonology_mask, max_new_tokens=21):\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        with pt.no_grad():\n",
        "            prompt_encoding = self.embed(orthography, orthography_mask, phonology, phonology_mask)[0]\n",
        "\n",
        "        mask = self.generate_triangular_mask(self.max_seq_len, device)\n",
        "\n",
        "        generated_tokens = pt.zeros((2,prompt_encoding.shape[0]),dtype=pt.long)\n",
        "        generated_embeddings = self.embed_tokens(generated_tokens)[:,:,None]\n",
        "        generated_tokens = generated_tokens[:,:,None]\n",
        "\n",
        "        dummy_mask = pt.zeros((1,15),device=device)\n",
        "        dummy_mask[0,0] = 1\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            step_mask = mask[:step+1, :step+1]\n",
        "\n",
        "            with pt.no_grad():\n",
        "                orthography_token_logits = self.linear_orthography_decoder(self.orthography_decoder(generated_embeddings[0], prompt_encoding, tgt_mask=step_mask))\n",
        "                phonology_token_logits = self.linear_phonology_decoder(self.phonology_decoder(generated_embeddings[1], prompt_encoding, tgt_mask=step_mask))\n",
        "\n",
        "            last_token_logits = (gorthography_token_logits[:, -1, :], phonology_token_logits[:, -1, :])\n",
        "            last_token_probs = (\n",
        "                                    pt.softmax(last_token_logits[0], dim=-1),\n",
        "                                    pt.softmax(last_token_logits[1], dim=-1)\n",
        "                                )\n",
        "\n",
        "            new_orthography_token = pt.multinomial(last_token_probs[0], num_samples=1)\n",
        "            new_phonology_token = pt.multinomial(last_token_probs[1], num_samples=1)\n",
        "\n",
        "            generated_tokens = pt.cat((generated_tokens, pt.stack((new_orthography_token,new_phonology_token), dim=0)), dim=2)\n",
        "\n",
        "            generated_embeddings = pt.cat((generated_embeddings,pt.stack(\n",
        "                                                (\n",
        "                                                    self.embed_tokens(new_orthography_token,'o'),\n",
        "                                                    self.embed_tokens(new_phonology_token,'p')\n",
        "                                                  ),\n",
        "                                            dim=0)), dim=2)\n",
        "            \n",
        "        return generated_tokens\n",
        "\n",
        "    def size(self):\n",
        "\n",
        "        param_num = 0\n",
        "        param_size = 0\n",
        "        for param in self.parameters():\n",
        "            param_num += param.nelement()\n",
        "            param_size += param.nelement() * param.element_size()\n",
        "        buffer_num = 0\n",
        "        buffer_size = 0\n",
        "        for buffer in self.buffers():\n",
        "            buffer_num += buffer.nelement()\n",
        "            buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "        param_num_all = param_num + buffer_num\n",
        "        size_all_mb = round((param_size + buffer_size) / 1024**2,3)\n",
        "        result = {'parameters': param_num_all,\n",
        "                  'size in MB': size_all_mb}\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbwEmU4MqF3y"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3nJp9VAUvfK"
      },
      "source": [
        "#### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH70wzl6YOjO"
      },
      "outputs": [],
      "source": [
        "ds = ConnTextULDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsWRMFF_iiKx"
      },
      "source": [
        "### Training loop with WandB logging\n",
        "The cell immediately below runs successive training loops for our model, logging data from each training loop run (called an \"experiment\") to `emelex`\n",
        "\n",
        "**Note** In order for each WandB run to log a new instantiation of our model, we need to include the code from the above cell into the outermost, \"experiment count\" loop below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_jBBFtRaW5l"
      },
      "outputs": [],
      "source": [
        "if pt.cuda.is_available():\n",
        "   device = pt.device('cuda:0')\n",
        "else:\n",
        "   device = pt.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI08-WKRniau"
      },
      "outputs": [],
      "source": [
        "exp_count = 1\n",
        "num_epochs = 10\n",
        "learning_rate = 5e-4\n",
        "batch_size = 32\n",
        "train_test_split = 0.8\n",
        "cutpoint = int(train_test_split * len(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI2MSV9pHBvC"
      },
      "outputs": [],
      "source": [
        "train_dataset_slices = []\n",
        "for batch in range(math.ceil(cutpoint/batch_size)):\n",
        "  train_dataset_slices.append(slice(batch*batch_size, min((batch+1)*batch_size, cutpoint)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L24LlkLwntcy",
        "outputId": "fb682b37-7b1f-41a7-c702-c1746e4113f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2223"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset_slices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOz7_WHCnnZD"
      },
      "outputs": [],
      "source": [
        "val_dataset_slices = []\n",
        "for batch in range(math.ceil((len(ds)-cutpoint)/batch_size)):\n",
        "  val_dataset_slices.append(slice(cutpoint+batch*batch_size, min(cutpoint+(batch+1)*batch_size, len(ds))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywCXOUCsOay",
        "outputId": "236fc206-94e2-4561-bce5-ccec24dc0db1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "556"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_dataset_slices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e14f00567e2a481aa0c697b9b5abe4e5",
            "15d6d5dcd4a042dbaf526d917ca50b11",
            "a062ef9f1df64b3cb34a9a8a72ffb103",
            "b9c497ed381e4cc4bca14f0312849e6a",
            "033ef39e998640fc8433a603c6fa2a3b",
            "3760fab70185452dbeab205e57ca7617",
            "ca5c0d950f3c46a980e51ad43b69106d",
            "5eba199c3b5240cca6f4440f0e25845c"
          ]
        },
        "id": "uw7e1xGhYSIq",
        "outputId": "bbbfc155-d3ca-44bd-91e6-041295e08e7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:vmd8p00d) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e14f00567e2a481aa0c697b9b5abe4e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/orth_loss</td><td>▅▅▅▄▃▃▄▄▄▂▅▃▄▂▁█▃▂▂▂▁▂▂▂▂▂▂▃▃▃▂▂▂▂▄▂▂▂▁▂</td></tr><tr><td>train/phon_loss</td><td>▄▅▄▄▄▅▃▃▃▄▄▃▄▄▃█▂▂▁▂▁▂▁▁▂▂▁▂▁▃▁▂▂▂▂▂▂▁▂▁</td></tr><tr><td>train/train_loss</td><td>▅▅▅▄▃▃▄▄▄▂▄▃▄▂▁█▃▂▂▂▁▂▂▂▂▂▁▃▃▃▂▂▂▂▄▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0</td></tr><tr><td>train/example_ct</td><td>3196</td></tr><tr><td>train/orth_loss</td><td>1.88152</td></tr><tr><td>train/phon_loss</td><td>0.08636</td></tr><tr><td>train/train_loss</td><td>1.96788</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-jazz-41</strong> at: <a href='https://wandb.ai/emelex/ConnTextUL_WandB_trial/runs/vmd8p00d' target=\"_blank\">https://wandb.ai/emelex/ConnTextUL_WandB_trial/runs/vmd8p00d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230516_022802-vmd8p00d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:vmd8p00d). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/Traindata/wandb/run-20230516_024314-nfjcqaq4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/emelex/ConnTextUL_WandB_trial/runs/nfjcqaq4' target=\"_blank\">generous-river-42</a></strong> to <a href='https://wandb.ai/emelex/ConnTextUL_WandB_trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/emelex/ConnTextUL_WandB_trial' target=\"_blank\">https://wandb.ai/emelex/ConnTextUL_WandB_trial</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/emelex/ConnTextUL_WandB_trial/runs/nfjcqaq4' target=\"_blank\">https://wandb.ai/emelex/ConnTextUL_WandB_trial/runs/nfjcqaq4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Loop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:287: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:544: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._transformer_encoder_layer_fwd(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            " 10%|█         | 1/10 [1:04:01<9:36:14, 3841.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loop...\n",
            "\n",
            "Validation Loop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [2:35:58<10:43:35, 4826.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loop...\n"
          ]
        }
      ],
      "source": [
        "# A number for WandB:\n",
        "n_steps_per_epoch = len(train_dataset_slices)\n",
        "\n",
        "# Launch `exp_count` experiments, trying different dropout rates\n",
        "for _ in range(exp_count):\n",
        "    # 🐝 initialise a wandb run\n",
        "    wandb.init(\n",
        "        project=\"ConnTextUL_WandB_trial\",\n",
        "        config={\n",
        "            \"epochs\": num_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": learning_rate,\n",
        "            #\"dropout\": random.uniform(0.01, 0.80),\n",
        "            })\n",
        "    \n",
        "    # Copy your config \n",
        "    config = wandb.config\n",
        "\n",
        "    pbar = tqdm.tqdm(range(num_epochs), position=0)\n",
        "\n",
        "    gm = Model(len(ds.character_tokenizer), len(ds.phonology_tokenizer), d_model=512, nhead=8)\n",
        "\n",
        "    gm.to(device)\n",
        "    opt = pt.optim.Adam(gm.parameters(), learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    #Training\n",
        "    example_ct = 0\n",
        "    for epoch in pbar:\n",
        "        gm.train()\n",
        "        print(\"\\nTraining Loop...\")\n",
        "        for step, batch_slice in enumerate(train_dataset_slices):\n",
        "            batch = ds[batch_slice]\n",
        "            orthography, phonology = batch['orthography'].to(device), batch['phonology'].to(device)\n",
        "            logits = gm(orthography['enc_input_ids'], orthography['enc_pad_mask'].bool(),\n",
        "                        orthography['dec_input_ids'], orthography['dec_pad_mask'].bool(),\n",
        "                        phonology['enc_input_ids'], phonology['enc_pad_mask'].bool(),\n",
        "                        phonology['dec_input_ids'], phonology['dec_pad_mask'].bool())\n",
        "            if step == -1:\n",
        "                print(\"---Predictions---\\n\")\n",
        "                print(\"logits['orth'].shape = \", logits['orth'].shape)\n",
        "                print(\"logits['orth'].dtype = \", logits['orth'].dtype)\n",
        "                print(\"logits['phon'].shape = \", logits['phon'].shape)\n",
        "                print(\"logits['phon'].dtype = \", logits['phon'].dtype)\n",
        "                print(\"---Targets---\\n\")\n",
        "                print(\"orthography['enc_input_ids'][1:].shape = \", orthography['enc_input_ids'][:,1:].shape)\n",
        "                print(\"orthography['enc_input_ids'][1:].dtype = \", orthography['enc_input_ids'][:,1:].dtype)\n",
        "                print(\"phonology['targets'].shape = \", phonology['targets'].shape) \n",
        "                print(\"phonology['targets'].dtype = \", phonology['targets'].dtype)                           \n",
        "            orth_loss = pt.nn.CrossEntropyLoss(ignore_index=4)(logits['orth'] , orthography['enc_input_ids'][:,1:]) \n",
        "            phon_loss = pt.nn.CrossEntropyLoss(ignore_index=2)(logits['phon'], phonology['targets'])\n",
        "            loss = orth_loss + phon_loss\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            \n",
        "            example_ct += len(batch['orthography'])\n",
        "            metrics = {\"train/orth_loss\": orth_loss,\n",
        "                       \"train/phon_loss\": phon_loss,\n",
        "                       \"train/train_loss\": loss, \n",
        "                       \"train/epoch\": epoch, \n",
        "                       \"train/example_ct\": example_ct\n",
        "                      }\n",
        "            \n",
        "            wandb.log(metrics)\n",
        "\n",
        "        gm.eval()\n",
        "        with pt.no_grad():\n",
        "          print(\"\\nValidation Loop...\")\n",
        "          for step, batch_slice in enumerate(val_dataset_slices):\n",
        "              batch = ds[batch_slice]\n",
        "              orthography, phonology = batch['orthography'].to(device), batch['phonology'].to(device)\n",
        "\n",
        "              logits = gm(orthography['enc_input_ids'], orthography['enc_pad_mask'],\n",
        "                          orthography['dec_input_ids'], orthography['dec_pad_mask'],\n",
        "                          phonology['enc_input_ids'], phonology['enc_pad_mask'],\n",
        "                          phonology['dec_input_ids'], phonology['dec_pad_mask'])\n",
        "            \n",
        "              val_loss = pt.nn.CrossEntropyLoss(ignore_index=4)(logits['orth'] , orthography['enc_input_ids'][:,1:]) \n",
        "              val_loss = val_loss + pt.nn.CrossEntropyLoss(ignore_index=2)(logits['phon'], phonology['targets'])              \n",
        "              more_metrics = {\"val/val_loss\": val_loss}             \n",
        "              wandb.log(more_metrics)\n",
        "\n",
        "# 🐝 Close your wandb run \n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQ7oPFOiZYN"
      },
      "source": [
        "## Model Introspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz3Sz2UUuY4E"
      },
      "outputs": [],
      "source": [
        "gm = Model(len(ds.character_tokenizer), len(ds.phonology_tokenizer), d_model=512, nhead=8)\n",
        "gm.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYpCkm6m3c62"
      },
      "source": [
        "## Examples\n",
        "\n",
        "Here we will walk through formatting the data for ingestion into the model, and examining the structure of the model output for ingestion into the CrossEntropy loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1hKmgO1-ohk"
      },
      "source": [
        "### Orthography -> Orthography\n",
        "1) We will pass the words **whale** and **hop** through the model. This means we have a batch size of 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOTtzf1h-jfV"
      },
      "outputs": [],
      "source": [
        "inpt_words = ['whale', 'hop'] # Batch size 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAX60yK_nXx"
      },
      "source": [
        "2) Focusing on the orthopgraphy -> orthography pathway first, let's explore the character-based tokenizer. Our final character vocab contains 7 unique characters from 2 words ('h', 'o', 'l', 'p', 'a', 'e', 'w') and 5 special tokens for a full vocab size of 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2MEUP_n_hyZ"
      },
      "outputs": [],
      "source": [
        "vocab = ['whale', 'hop'] # vocab size 7 unique characters and 5 special characters (12 tokens)\n",
        "list_of_characters = set(''.join([c for word in vocab for c in word]))\n",
        "character_tokenizer = CharacterTokenizer(list_of_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCRLKzJLALmS"
      },
      "outputs": [],
      "source": [
        "ORTH_VOCAB_SIZE = len(character_tokenizer.char_2_idx)\n",
        "character_tokenizer.char_2_idx # Contains 14 elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eEJilcVCY6H"
      },
      "source": [
        "3) When we pass the inpt_words into the tokenizer, we receive a dictionary containing data formated for ingestion into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Jqr2vrCLwB"
      },
      "outputs": [],
      "source": [
        "orth_data = character_tokenizer.encode(inpt_words)\n",
        "print(f\"--- tokens['enc_input_ids'] ---\\n  shape: {orth_data['enc_input_ids'].shape}\\n  {orth_data['enc_input_ids']}\")\n",
        "print(f\"\\n\\n--- tokens['enc_pad_mask'] ---\\n shape: {orth_data['enc_pad_mask'].shape}\\n {orth_data['enc_pad_mask']}\")\n",
        "print(f\"\\n\\n--- tokens['dec_input_ids'] ---\\n  shape: {orth_data['dec_input_ids'].shape}\\n  {orth_data['dec_input_ids']}\")\n",
        "print(f\"\\n\\n--- tokens['dec_pad_mask'] ---\\n shape: {orth_data['dec_pad_mask'].shape}\\n {orth_data['dec_pad_mask']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Tw1Umnsc2I"
      },
      "source": [
        "We now extract the embedding vectors from the Embedding matrix for input into the transformer encoder layer. Output from the embedding matrix is a tensor ready to consumed by the transformer encoder. The shape is (batch_size, max_batch_len, d_model). Batch_size is specified by the number of words in the input list. max_batch_len, also known as context_length or block_size, is determined by the length of the longest word in the input list (plus 2). Some architectures manually set this to limit memory allocation for hardware constraints. Since our longest words will not begin to challenge our memory banks, we leave this value unbounded. The d_model parameter is a hyperparameter that we choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0TvXagVAJM4"
      },
      "outputs": [],
      "source": [
        "D_MODEL = 10\n",
        "orth_embed = pt.nn.Embedding(ORTH_VOCAB_SIZE, D_MODEL) # Creates Embedding Matrix of shape (ORTH_VOCAB_SIZE, d_model=10)\n",
        "enc_input_tensors = orth_embed(orth_data['enc_input_ids']) # Grabs 2 batches of 2 rows from the Embedding Matrix\n",
        "print(f\"enc_input_tensors.shape = {enc_input_tensors.shape}\") # (batch_size, max_batch_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMTBQa3BgVb"
      },
      "source": [
        "Notice the context length for this input is 5 + 2 = 7, (because 5 is longest word in the batch _'whale'_ and we add the BOS and EOS tokens). This results in a full context length, or block_size, of 7. Next we pass the input tensors for the encoder **enc_input_tensors** along with their padding masks **input_data['enc_pad_mask']** into a transformer encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcU0Ua0Ut-VZ"
      },
      "outputs": [],
      "source": [
        "encoder_layer = pt.nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=1, batch_first=True)\n",
        "enc_output = encoder_layer(enc_input_tensors, src_key_padding_mask=orth_data['enc_pad_mask'])\n",
        "print(f\"enc_output.shape = {enc_output.shape}\") # (batch_size, context_length, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENpnqyoyvJb2"
      },
      "source": [
        "The **enc_output** will act as the memory in the orthography decoder. In our full implementation above, this will be the global mixed embedding vector. We may need to do a little reshaping to make things work there, but nothing complicated. Now we create the decoder instance and pass in the decoder input, the decoder pad mask, the memory, the memory mask, and the 'no look ahead' upper triangular mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Is6Eebvhp4"
      },
      "outputs": [],
      "source": [
        "decoder_layer = pt.nn.TransformerDecoderLayer(d_model=10, nhead=1, batch_first=True)\n",
        "dec_input_tensors = orth_embed(orth_data['dec_input_ids'])\n",
        "context_length = dec_input_tensors.shape[1]\n",
        "dec_look_ahead_mask = pt.triu(pt.ones((context_length, context_length), dtype=pt.bool, device='cpu'), 1)\n",
        "dec_pad_mask = orth_data['dec_pad_mask']\n",
        "mem_pad_mask = orth_data['enc_pad_mask']\n",
        "out = decoder_layer(tgt=dec_input_tensors, \n",
        "                    tgt_key_padding_mask=dec_pad_mask,\n",
        "                    tgt_mask=dec_look_ahead_mask,\n",
        "                    memory=enc_output,\n",
        "                    memory_key_padding_mask=mem_pad_mask)\n",
        "print(f\"out.shape = {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJflrzcP1T6z"
      },
      "source": [
        "We next reshape the decoder output and pass it through a linear layer to predict the output tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yb-kWOp1SIQ"
      },
      "outputs": [],
      "source": [
        "B, C, E = out.shape\n",
        "out_reshaped = out.view(B*C, E)\n",
        "linear = pt.nn.Linear(E, ORTH_VOCAB_SIZE)\n",
        "logits = linear(out_reshaped)\n",
        "print(f\"logits.shape = {logits.shape}\")\n",
        "logits_reshaped = logits.view(B,ORTH_VOCAB_SIZE,C)\n",
        "print(f\"logits_reshaped.shape = {logits_reshaped.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qP0IQqZ2d1w"
      },
      "outputs": [],
      "source": [
        "orth_data['enc_input_ids'][:,1:] # We slice from 1: becaues these are the targets of the input. i.e. the next character prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kppoi47e3gSA"
      },
      "outputs": [],
      "source": [
        "criterion = pt.nn.CrossEntropyLoss(ignore_index=4)\n",
        "criterion(logits_reshaped, orth_data['enc_input_ids'][:,1:].long())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9pgPJff4Fwy"
      },
      "outputs": [],
      "source": [
        "-pt.log(pt.tensor(1/14)) # This is roughly the loss value we should expect from a well initialized model. CLOSE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nl2bNAz4qmQ"
      },
      "source": [
        "### Phonology -> Phonology\n",
        "\n",
        "We now examine the p -> p pathway. In particular we look at how to format the input data for ingestion, the shapes of tensors along the computational graph, and then how to format the data for input into the loss function.\n",
        "\n",
        "We begin by creating the phonological vectors for the corresponding input words. The vocab for phonology will be composed of the features of the phonological vectors. In otherwords, each of the elements in a single phonological vector (33 of them) will have its own row in the Phonology Embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fd1tOAT4v_9"
      },
      "outputs": [],
      "source": [
        "vocab = ['whale', 'hi']\n",
        "phon_data = phonemizer.encode(vocab) # Produce the tokens for each phonological feature vector and decoder inputs/targets\n",
        "PHON_VOCAB_SIZE = len(phonemizer) # The phonological vocab is 34 dimensional\n",
        "print(f\"PHON_VOCAB_SIZE = {PHON_VOCAB_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNLv5Eeg8tnH"
      },
      "outputs": [],
      "source": [
        "phon_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ycQASjfHSU"
      },
      "outputs": [],
      "source": [
        "phon_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRmjPkSO5ZGR"
      },
      "outputs": [],
      "source": [
        "# Create a phonological embedding matrix of size (PHON_VOCAB_SIZE, d_model)\n",
        "phon_embed = pt.nn.Embedding(PHON_VOCAB_SIZE, D_MODEL)\n",
        "print(f\"phon_embed = \", phon_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKwZSMTJgFFn"
      },
      "source": [
        "We now go through the process of creating the phonological input vectors. We need a single vembedding vector to represent the phonological feature vector, which is a distributed representation of phonological features. Therefore, each phonological feature that is present in the feature vector will be extracted from the phonology embedding matrix and averaged together, creating the single phonology embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoYAtMxF8jTF"
      },
      "outputs": [],
      "source": [
        "# Create tensor of zeros of shape (batch_size, context_length, d_model)\n",
        "phon_enc_input_tensors = pt.zeros((len(phon_data['enc_input_ids']), len(phon_data['enc_input_ids'][0]), D_MODEL))\n",
        "print(f\"phon_enc_input_tensors\\n Shape: {phon_enc_input_tensors.shape}\")\n",
        "# We loop through each tensor in the enc_input_ids, extract all the embedding rows from the phon_embed and average them together\n",
        "print(f\"Loop over the lists of phonological feature tokens (elements in the batch)\\n len(phon_data['enc_input_ids']) = {len(phon_data['enc_input_ids'])}\")\n",
        "for batch_num, batch_elem in enumerate(phon_data['enc_input_ids']):\n",
        "  for indx, tokens in enumerate(batch_elem):\n",
        "    # Here tokens should be a pytorch tensor of integers. It extracts the indicated rows from phon_embed\n",
        "    avg_embedding = phon_embed(tokens).mean(axis=0)\n",
        "    # We then insert the new average embedding vector into the phon_embedding_input tensor as a new row\n",
        "    phon_enc_input_tensors[batch_num, indx, :] = avg_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-StFvJ6WikPf"
      },
      "source": [
        "The final phonological features tensor for input into the encoder has the shape (batch_size, max_batch_length, d_model). Where once again, the batch_size corresponds to the number of words input to the model, the max_batch_length, also known as context_length or block_size, is the maximum number of phonological features active in any one feature vector within the entire batch, and d_model is a hyperparameter set during model creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_YC5P50QFkI"
      },
      "outputs": [],
      "source": [
        "phon_enc_input_tensors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OzO3bavSh8x"
      },
      "outputs": [],
      "source": [
        "phon_enc_input_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc57lVTwjavk"
      },
      "source": [
        "We must mask out the embedding vector associated with the padding feature vectors. Let us look at the mask created in the phonology tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v0vcV0ojk9A"
      },
      "outputs": [],
      "source": [
        "phon_data['enc_pad_mask'] # We see only the last token in the second word needs to be masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52vj9S3AQq9G"
      },
      "outputs": [],
      "source": [
        "encoder_layer = pt.nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=1, batch_first=True)\n",
        "phon_enc_output = encoder_layer(phon_enc_input_tensors, src_key_padding_mask=phon_data['enc_pad_mask'])\n",
        "print(f\"phon_enc_output.shape = {phon_enc_output.shape}\") # (batch_size, context_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6ItY2OESqHi"
      },
      "outputs": [],
      "source": [
        "phon_enc_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFZFZKudSy1v"
      },
      "source": [
        "Now we create the decoder layer and pass all the decoder data created in the phonemizer through. First we must embed the decoder input, by calculating the average of the embedding vectors for each of the phonological features again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8JpWFX3Tazl"
      },
      "outputs": [],
      "source": [
        "# Create tensor of zeros of shape (batch_size, context_length, d_model)\n",
        "context_length = len(phon_data['dec_input_ids'][0])\n",
        "phon_dec_input_tensors = pt.zeros((len(phon_data['dec_input_ids']), context_length, D_MODEL))\n",
        "# We loop through each tensor in the dec_input_ids, extract all the embedding rows from the phon_embed and average them together\n",
        "for batch_num, batch_elem in enumerate(phon_data['dec_input_ids']):\n",
        "  for indx, tokens in enumerate(batch_elem):\n",
        "    # Here tokens should be a pytorch tensor of integers. It extracts the indicated rows from phon_embed\n",
        "    avg_embedding = phon_embed(tokens).mean(axis=0)\n",
        "    # We then insert the new average embedding vector into the phon_embedding_input tensor as a new row\n",
        "    phon_dec_input_tensors[batch_num, indx, :] = avg_embedding\n",
        "phon_dec_input_tensors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNidpyYOkJYB"
      },
      "source": [
        "Notice this resulting input tensor has a context_length or max_batch_length of 4, rather than 5 in the encoder input. That is because when decoding there are N-1 labels in a sequence of N tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swqgy8UmT2fF"
      },
      "outputs": [],
      "source": [
        "phon_dec_input_tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KVIruWASsko"
      },
      "outputs": [],
      "source": [
        "decoder_layer = pt.nn.TransformerDecoderLayer(d_model=D_MODEL, nhead=1, batch_first=True)\n",
        "dec_look_ahead_mask = pt.triu(pt.ones((context_length, context_length), dtype=pt.bool, device='cpu'), 1)\n",
        "dec_pad_mask = phon_data['dec_pad_mask']\n",
        "mem_pad_mask = phon_data['enc_pad_mask']\n",
        "phon_dec_out = decoder_layer(tgt=phon_dec_input_tensors, \n",
        "                    tgt_key_padding_mask=dec_pad_mask,\n",
        "                    tgt_mask=dec_look_ahead_mask,\n",
        "                    memory=phon_enc_output,\n",
        "                    memory_key_padding_mask=mem_pad_mask)\n",
        "print(f\"out.shape = {phon_dec_out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmH0Zv8STKE_"
      },
      "outputs": [],
      "source": [
        "# Now we evaluate the prediction of the target phonological vectors with the CrossEntropyLoss\n",
        "B, C, E = phon_dec_out.shape\n",
        "phon_dec_out_reshaped = phon_dec_out.view(B*C, E)\n",
        "# The output of the linear layer is 2*C*(PHON_VOCAB_SIZE-1). The 2 is because for each phonological feature we are predicting \n",
        "# whether it is on or off, active or inactive. It is a binary random variable. The C is because we need to predict the entire\n",
        "# feature vector for each 'phoneme' in the input, i.e. the context_length. The PHON_VOCAB_SIZE-1 is because our target vector\n",
        "# that we are attempting to predict does not contain the PAD feature and it will not be predicted, therefore we subtract 1.\n",
        "linear = pt.nn.Linear(E, 2*(PHON_VOCAB_SIZE-1)) \n",
        "logits = linear(phon_dec_out_reshaped)\n",
        "print(f\"logits.shape = {logits.shape}\")\n",
        "logits_reshaped = logits.view(B, 2, C, (PHON_VOCAB_SIZE-1))\n",
        "print(f\"logits_reshaped.shape = {logits_reshaped.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZu8W6G7lllS"
      },
      "source": [
        "Before we calculate the loss, we verify that the output of our linear layer matches the shape of our target vector (minus the 2 dimensions for the probabilities of each class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYyOTgB5leW7"
      },
      "outputs": [],
      "source": [
        "phon_data['targets'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_PZpZ6mUojH"
      },
      "outputs": [],
      "source": [
        "# Recall we set 2s in every rows that contained a padding vector. So we ask our loss function to ingore them\n",
        "# with the ignore_index=2 parameter\n",
        "criterion = pt.nn.CrossEntropyLoss(ignore_index=2)\n",
        "criterion(logits_reshaped, phon_data['targets'].long())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdWOmFSQU4x9"
      },
      "outputs": [],
      "source": [
        "-pt.log(pt.tensor(1/2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep6_ypgMsPQN"
      },
      "outputs": [],
      "source": [
        "print(pt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejgyi3Y8OzFf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oHF8_MZV5Pz1"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033ef39e998640fc8433a603c6fa2a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d6d5dcd4a042dbaf526d917ca50b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033ef39e998640fc8433a603c6fa2a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_3760fab70185452dbeab205e57ca7617",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "3760fab70185452dbeab205e57ca7617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eba199c3b5240cca6f4440f0e25845c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a062ef9f1df64b3cb34a9a8a72ffb103": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca5c0d950f3c46a980e51ad43b69106d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eba199c3b5240cca6f4440f0e25845c",
            "value": 1
          }
        },
        "b9c497ed381e4cc4bca14f0312849e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5c0d950f3c46a980e51ad43b69106d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14f00567e2a481aa0c697b9b5abe4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15d6d5dcd4a042dbaf526d917ca50b11",
              "IPY_MODEL_a062ef9f1df64b3cb34a9a8a72ffb103"
            ],
            "layout": "IPY_MODEL_b9c497ed381e4cc4bca14f0312849e6a"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
