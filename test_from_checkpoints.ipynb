{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import csv\n",
    "from time import time\n",
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache folder: /workspaces/ConnTextUL/data/.cache already exists\n"
     ]
    }
   ],
   "source": [
    "# Load in a dataset to access Matt's Traindata class which\n",
    "# is embedded inside the phonology tokenizer (consider changing this\n",
    "from pathlib import Path\n",
    "\n",
    "config = type(\n",
    "    \"config\",\n",
    "    (object,),\n",
    "    {\"dataset_filename\": Path(\"data/random_kid_10140_021624.csv\")},\n",
    ")\n",
    "ds = ConnTextULDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints=['models/root_2024-04-16_04h11m47948ms_chkpt001.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt002.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt003.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt004.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt005.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt006.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt007.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt008.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt009.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt010.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt011.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt012.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt013.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt014.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt015.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt016.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt017.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt018.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt019.pth', 'models/root_2024-04-16_04h11m47948ms_chkpt020.pth']\n"
     ]
    }
   ],
   "source": [
    "# read in the checkpoint file names. We will load one at a time\n",
    "# and generate predictions of the wj3 assessments one at at ime\n",
    "checkpoints = glob.glob(\"models/root_2024-04-16_04h11m47948ms_chkpt*.pth\")\n",
    "checkpoints.sort()\n",
    "print(f\"{checkpoints=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "all_words=set()\n",
    "all_words.update(ds.words)\n",
    "batches = [list(all_words)[i:i+batch_size] for i in range(0, len(all_words), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/workspaces/ConnTextUL/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    }
   ],
   "source": [
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "csv_name = f\"model_{checkpoints[-1][-42:-13]}.xlsx\"\n",
    "writer = pd.ExcelWriter(csv_name, engine='openpyxl')\n",
    "\n",
    "with pd.ExcelWriter(csv_name, engine='openpyxl') as writer:\n",
    "    for checkpoint in tqdm.tqdm(checkpoints[:1]):\n",
    "        chkpt = torch.load(checkpoint)\n",
    "        dfa = pd.DataFrame(columns=[\"phon_prediction\"])\n",
    "        model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "        model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        sheet_name = \"epoch \" + checkpoint[-7:-4]\n",
    "        start_row = 0  # Initialize starting row for each new sheet\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx in range(len(batches)):\n",
    "                batch = batches[batch_idx]\n",
    "                new_row = {}\n",
    "                datum = ds.character_tokenizer.encode(batch)\n",
    "                pred = model.generate(\n",
    "                    \"o2p\",\n",
    "                    datum['enc_input_ids'],\n",
    "                    datum['enc_pad_mask'],\n",
    "                    None,\n",
    "                    None,\n",
    "                    deterministic=True,\n",
    "                )\n",
    "                for idx, orth in enumerate(batch):\n",
    "                    # Save the original input orthography\n",
    "                    new_row[\"word_raw\"] = orth\n",
    "                    # Save the target phonology for the above input orthography\n",
    "                    phon = ds.cmudict[orth]\n",
    "                    new_row[\"phon_target\"] = \":\".join(phon)\n",
    "                    # Remove the start and end tokens from each phonological vector\n",
    "                    # and convert them from tensors to lists\n",
    "                    phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][idx][1:-1]]\n",
    "                    # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "                    phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "                        phon_pred_features, phonology=True, hot_nodes=True\n",
    "                    )\n",
    "                    # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "                    # separated by colons\n",
    "                    phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "                    new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "                    # Save a boolean indicating whether the prediction was correct\n",
    "                    new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "                    # Save the phonological features for the target phonology\n",
    "                    phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "                    if phon_target_features:\n",
    "                        phon_target_features = \";\".join(\n",
    "                            [\n",
    "                                \":\".join([str(v.item()) for v in vector])\n",
    "                                for vector in phon_target_features[\"targets\"][0]\n",
    "                            ]\n",
    "                        )\n",
    "                    else:\n",
    "                        phon_target_features = \"None\"\n",
    "                    new_row[\"phon_target_features\"] = phon_target_features\n",
    "                    # Save the phonological features for the predicted phonology\n",
    "                    phon_prediction_features = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(int(v.item())) for v in vector])\n",
    "                            for vector in pred[\"phon_vecs\"][idx][1:-1]\n",
    "                        ]\n",
    "                    )\n",
    "                    new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "                    # Save the phonological probabilities for the predicted phonology\n",
    "                    phon_prediction_probabilities = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(v.item()) for v in vector])\n",
    "                            for vector in pred[\"phon_probs\"][idx][1:-1]\n",
    "                        ]\n",
    "                    )\n",
    "                    new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "                    # Save the global encoding vector for the predicted phonology\n",
    "                    global_encoding = \":\".join(\n",
    "                        [str(v.item()) for v in pred[\"global_encoding\"][idx].squeeze()]\n",
    "                    )\n",
    "                    new_row[\"global_encoding\"] = global_encoding\n",
    "                    dfa = pd.DataFrame([new_row])\n",
    "                    dfa.to_excel(writer, sheet_name=sheet_name, startrow=start_row, header=(start_row == 0), index=False)\n",
    "                    start_row += 1  # Increment the start row for the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orth_probs': None,\n",
       " 'orth_tokens': None,\n",
       " 'phon_probs': [[tensor([0.1235, 0.0301, 0.2751, 0.0970, 0.0729, 0.0163, 0.2480, 0.1374, 0.0356,\n",
       "           0.0988, 0.0781, 0.0327, 0.0741, 0.0366, 0.6443, 0.1211, 0.1016, 0.0618,\n",
       "           0.0997, 0.0355, 0.0472, 0.1412, 0.0292, 0.0535, 0.0992, 0.0576, 0.0549,\n",
       "           0.0453, 0.0322, 0.1114, 0.0231, 0.0152, 0.0651]),\n",
       "   tensor([0.0766, 0.0369, 0.2764, 0.1017, 0.0662, 0.0142, 0.1386, 0.0974, 0.0427,\n",
       "           0.0874, 0.0668, 0.0321, 0.0987, 0.0274, 0.7055, 0.1951, 0.0829, 0.0603,\n",
       "           0.1323, 0.0367, 0.0512, 0.1212, 0.0306, 0.0557, 0.1013, 0.0492, 0.0413,\n",
       "           0.0560, 0.0364, 0.2061, 0.0261, 0.0188, 0.1734]),\n",
       "   tensor([0.1126, 0.0468, 0.3562, 0.0800, 0.0828, 0.0142, 0.1907, 0.1525, 0.0349,\n",
       "           0.0954, 0.0736, 0.0458, 0.0647, 0.0358, 0.6809, 0.1508, 0.1023, 0.0733,\n",
       "           0.1044, 0.0353, 0.0527, 0.1193, 0.0277, 0.0495, 0.0818, 0.0642, 0.0388,\n",
       "           0.0501, 0.0252, 0.1292, 0.0215, 0.0144, 0.1131]),\n",
       "   tensor([0.1008, 0.0385, 0.3375, 0.0763, 0.0745, 0.0155, 0.1762, 0.1253, 0.0418,\n",
       "           0.1034, 0.0879, 0.0351, 0.0772, 0.0318, 0.6861, 0.1224, 0.1196, 0.0828,\n",
       "           0.1121, 0.0377, 0.0477, 0.1193, 0.0229, 0.0410, 0.0719, 0.0404, 0.0377,\n",
       "           0.0415, 0.0314, 0.1193, 0.0258, 0.0130, 0.1399]),\n",
       "   tensor([0.0719, 0.0336, 0.3636, 0.0777, 0.0690, 0.0175, 0.1721, 0.1334, 0.0375,\n",
       "           0.0891, 0.0832, 0.0432, 0.0776, 0.0391, 0.6512, 0.1475, 0.1141, 0.0753,\n",
       "           0.0973, 0.0337, 0.0430, 0.1150, 0.0210, 0.0444, 0.0822, 0.0407, 0.0364,\n",
       "           0.0401, 0.0297, 0.1289, 0.0249, 0.0170, 0.1624]),\n",
       "   tensor([0.0789, 0.0345, 0.3427, 0.0850, 0.0548, 0.0120, 0.1425, 0.1128, 0.0443,\n",
       "           0.0744, 0.0860, 0.0399, 0.0801, 0.0346, 0.6293, 0.1554, 0.1091, 0.0718,\n",
       "           0.1108, 0.0367, 0.0417, 0.1285, 0.0238, 0.0567, 0.1036, 0.0419, 0.0313,\n",
       "           0.0480, 0.0314, 0.1566, 0.0332, 0.0155, 0.1837]),\n",
       "   tensor([0.0772, 0.0340, 0.3742, 0.0806, 0.0662, 0.0148, 0.1429, 0.1329, 0.0458,\n",
       "           0.0998, 0.0962, 0.0389, 0.0830, 0.0288, 0.6077, 0.1456, 0.1202, 0.0751,\n",
       "           0.1300, 0.0518, 0.0350, 0.1374, 0.0208, 0.0488, 0.0980, 0.0478, 0.0390,\n",
       "           0.0513, 0.0334, 0.1613, 0.0295, 0.0133, 0.2307]),\n",
       "   tensor([0.0596, 0.0445, 0.3383, 0.0831, 0.0612, 0.0143, 0.1428, 0.1236, 0.0385,\n",
       "           0.0940, 0.0982, 0.0421, 0.0894, 0.0327, 0.6454, 0.1538, 0.1121, 0.0704,\n",
       "           0.1186, 0.0417, 0.0446, 0.1157, 0.0296, 0.0476, 0.0954, 0.0491, 0.0354,\n",
       "           0.0565, 0.0315, 0.1745, 0.0285, 0.0116, 0.1924]),\n",
       "   tensor([0.0723, 0.0388, 0.3364, 0.0906, 0.0742, 0.0201, 0.1704, 0.1322, 0.0379,\n",
       "           0.1007, 0.0787, 0.0343, 0.0845, 0.0353, 0.6439, 0.1305, 0.1176, 0.0673,\n",
       "           0.1300, 0.0396, 0.0428, 0.1227, 0.0217, 0.0423, 0.0767, 0.0541, 0.0481,\n",
       "           0.0385, 0.0301, 0.1406, 0.0315, 0.0147, 0.2174]),\n",
       "   tensor([0.0674, 0.0426, 0.3344, 0.0667, 0.0672, 0.0162, 0.1384, 0.1182, 0.0390,\n",
       "           0.1012, 0.1014, 0.0511, 0.0767, 0.0268, 0.6095, 0.1569, 0.1018, 0.0835,\n",
       "           0.1210, 0.0468, 0.0520, 0.1098, 0.0219, 0.0530, 0.0957, 0.0411, 0.0292,\n",
       "           0.0622, 0.0376, 0.1589, 0.0352, 0.0167, 0.2725]),\n",
       "   tensor([0.0796, 0.0392, 0.3455, 0.0820, 0.0601, 0.0132, 0.1790, 0.1165, 0.0538,\n",
       "           0.0968, 0.0727, 0.0349, 0.1011, 0.0299, 0.6927, 0.1650, 0.1333, 0.0795,\n",
       "           0.1259, 0.0618, 0.0428, 0.1295, 0.0236, 0.0423, 0.0800, 0.0495, 0.0430,\n",
       "           0.0560, 0.0326, 0.1526, 0.0261, 0.0143, 0.2061]),\n",
       "   tensor([0.0637, 0.0361, 0.3070, 0.0780, 0.0680, 0.0127, 0.1352, 0.1150, 0.0410,\n",
       "           0.0899, 0.0883, 0.0453, 0.0957, 0.0256, 0.5895, 0.1347, 0.0905, 0.0779,\n",
       "           0.1217, 0.0374, 0.0519, 0.1094, 0.0244, 0.0618, 0.1156, 0.0643, 0.0458,\n",
       "           0.0611, 0.0303, 0.2060, 0.0259, 0.0219, 0.2203]),\n",
       "   tensor([0.0646, 0.0592, 0.3618, 0.0899, 0.1100, 0.0154, 0.2529, 0.1559, 0.0544,\n",
       "           0.0791, 0.0928, 0.0520, 0.0835, 0.0209, 0.6785, 0.1482, 0.1421, 0.1060,\n",
       "           0.1230, 0.0444, 0.0886, 0.1487, 0.0307, 0.0399, 0.1120, 0.0888, 0.0476,\n",
       "           0.0595, 0.0451, 0.2582, 0.0313, 0.0169, 0.2632]),\n",
       "   tensor([0.0597, 0.0625, 0.4031, 0.0810, 0.0666, 0.0186, 0.2062, 0.1555, 0.0384,\n",
       "           0.0816, 0.1219, 0.0567, 0.0703, 0.0467, 0.5685, 0.1551, 0.1251, 0.0679,\n",
       "           0.1121, 0.0440, 0.0509, 0.1157, 0.0239, 0.0545, 0.0931, 0.0473, 0.0218,\n",
       "           0.0655, 0.0284, 0.1442, 0.0363, 0.0138, 0.2152]),\n",
       "   tensor([0.0771, 0.0655, 0.5046, 0.0817, 0.1138, 0.0201, 0.1950, 0.1742, 0.0646,\n",
       "           0.1016, 0.0637, 0.0468, 0.0880, 0.0371, 0.7281, 0.1831, 0.1575, 0.0840,\n",
       "           0.1090, 0.0450, 0.0501, 0.1291, 0.0195, 0.0366, 0.0887, 0.0776, 0.0481,\n",
       "           0.0639, 0.0557, 0.1279, 0.0262, 0.0213, 0.1924]),\n",
       "   tensor([0.0789, 0.0351, 0.2917, 0.1178, 0.0612, 0.0171, 0.1280, 0.1454, 0.0322,\n",
       "           0.1115, 0.0913, 0.0466, 0.0845, 0.0318, 0.5668, 0.1750, 0.1114, 0.0583,\n",
       "           0.1102, 0.0342, 0.0359, 0.1203, 0.0263, 0.0595, 0.1154, 0.0512, 0.0422,\n",
       "           0.0517, 0.0241, 0.2017, 0.0342, 0.0206, 0.2234]),\n",
       "   tensor([0.0821, 0.0444, 0.3803, 0.0880, 0.0831, 0.0187, 0.1289, 0.1444, 0.0565,\n",
       "           0.1199, 0.0806, 0.0421, 0.0954, 0.0322, 0.6606, 0.1610, 0.1132, 0.0707,\n",
       "           0.1594, 0.0378, 0.0688, 0.0791, 0.0286, 0.0631, 0.0897, 0.0519, 0.0316,\n",
       "           0.0483, 0.0492, 0.1689, 0.0304, 0.0217, 0.2227])],\n",
       "  [tensor([0.1280, 0.0296, 0.2757, 0.0984, 0.0702, 0.0163, 0.2441, 0.1338, 0.0358,\n",
       "           0.1031, 0.0779, 0.0329, 0.0738, 0.0385, 0.6454, 0.1250, 0.0998, 0.0609,\n",
       "           0.1004, 0.0342, 0.0464, 0.1406, 0.0288, 0.0536, 0.0971, 0.0539, 0.0528,\n",
       "           0.0428, 0.0315, 0.1107, 0.0239, 0.0160, 0.0661]),\n",
       "   tensor([0.0794, 0.0361, 0.2769, 0.1023, 0.0635, 0.0141, 0.1372, 0.0944, 0.0427,\n",
       "           0.0902, 0.0665, 0.0320, 0.0972, 0.0287, 0.7059, 0.1980, 0.0813, 0.0591,\n",
       "           0.1328, 0.0352, 0.0500, 0.1209, 0.0299, 0.0553, 0.0989, 0.0460, 0.0397,\n",
       "           0.0527, 0.0356, 0.2026, 0.0268, 0.0194, 0.1726]),\n",
       "   tensor([0.1173, 0.0457, 0.3571, 0.0809, 0.0794, 0.0141, 0.1879, 0.1481, 0.0350,\n",
       "           0.0994, 0.0732, 0.0457, 0.0639, 0.0374, 0.6825, 0.1547, 0.1002, 0.0720,\n",
       "           0.1046, 0.0339, 0.0513, 0.1195, 0.0271, 0.0493, 0.0800, 0.0597, 0.0373,\n",
       "           0.0473, 0.0246, 0.1273, 0.0222, 0.0150, 0.1132]),\n",
       "   tensor([0.1048, 0.0379, 0.3389, 0.0773, 0.0719, 0.0155, 0.1745, 0.1218, 0.0421,\n",
       "           0.1074, 0.0875, 0.0354, 0.0764, 0.0336, 0.6867, 0.1258, 0.1170, 0.0814,\n",
       "           0.1129, 0.0364, 0.0465, 0.1197, 0.0225, 0.0411, 0.0707, 0.0381, 0.0365,\n",
       "           0.0394, 0.0308, 0.1178, 0.0266, 0.0137, 0.1400]),\n",
       "   tensor([0.0747, 0.0330, 0.3638, 0.0787, 0.0666, 0.0175, 0.1701, 0.1298, 0.0377,\n",
       "           0.0925, 0.0827, 0.0431, 0.0771, 0.0410, 0.6522, 0.1514, 0.1119, 0.0740,\n",
       "           0.0981, 0.0325, 0.0422, 0.1151, 0.0206, 0.0444, 0.0807, 0.0384, 0.0352,\n",
       "           0.0381, 0.0292, 0.1276, 0.0256, 0.0177, 0.1622]),\n",
       "   tensor([0.0825, 0.0339, 0.3431, 0.0858, 0.0527, 0.0120, 0.1408, 0.1098, 0.0445,\n",
       "           0.0778, 0.0856, 0.0401, 0.0795, 0.0363, 0.6313, 0.1597, 0.1073, 0.0709,\n",
       "           0.1116, 0.0355, 0.0409, 0.1287, 0.0235, 0.0565, 0.1013, 0.0393, 0.0304,\n",
       "           0.0454, 0.0309, 0.1545, 0.0342, 0.0161, 0.1833]),\n",
       "   tensor([0.0804, 0.0333, 0.3748, 0.0812, 0.0637, 0.0146, 0.1410, 0.1291, 0.0458,\n",
       "           0.1033, 0.0952, 0.0388, 0.0819, 0.0302, 0.6102, 0.1488, 0.1174, 0.0737,\n",
       "           0.1303, 0.0497, 0.0342, 0.1371, 0.0204, 0.0486, 0.0957, 0.0448, 0.0375,\n",
       "           0.0484, 0.0326, 0.1588, 0.0302, 0.0138, 0.2288]),\n",
       "   tensor([0.0619, 0.0433, 0.3385, 0.0838, 0.0589, 0.0142, 0.1410, 0.1197, 0.0386,\n",
       "           0.0970, 0.0968, 0.0418, 0.0882, 0.0343, 0.6471, 0.1572, 0.1095, 0.0688,\n",
       "           0.1191, 0.0399, 0.0433, 0.1155, 0.0289, 0.0473, 0.0931, 0.0460, 0.0341,\n",
       "           0.0531, 0.0308, 0.1718, 0.0291, 0.0121, 0.1912]),\n",
       "   tensor([0.0754, 0.0378, 0.3375, 0.0910, 0.0711, 0.0199, 0.1677, 0.1283, 0.0381,\n",
       "           0.1044, 0.0782, 0.0343, 0.0834, 0.0370, 0.6454, 0.1337, 0.1147, 0.0660,\n",
       "           0.1302, 0.0380, 0.0415, 0.1227, 0.0212, 0.0423, 0.0752, 0.0505, 0.0463,\n",
       "           0.0364, 0.0294, 0.1382, 0.0322, 0.0153, 0.2156]),\n",
       "   tensor([0.0703, 0.0415, 0.3351, 0.0674, 0.0643, 0.0160, 0.1361, 0.1146, 0.0392,\n",
       "           0.1047, 0.1003, 0.0509, 0.0760, 0.0282, 0.6111, 0.1613, 0.0995, 0.0819,\n",
       "           0.1217, 0.0449, 0.0506, 0.1098, 0.0215, 0.0529, 0.0937, 0.0384, 0.0281,\n",
       "           0.0585, 0.0367, 0.1570, 0.0360, 0.0173, 0.2707]),\n",
       "   tensor([0.0828, 0.0382, 0.3457, 0.0825, 0.0577, 0.0131, 0.1760, 0.1125, 0.0541,\n",
       "           0.1000, 0.0720, 0.0349, 0.0998, 0.0315, 0.6942, 0.1690, 0.1298, 0.0781,\n",
       "           0.1267, 0.0591, 0.0416, 0.1294, 0.0232, 0.0422, 0.0783, 0.0462, 0.0414,\n",
       "           0.0525, 0.0319, 0.1502, 0.0269, 0.0149, 0.2050]),\n",
       "   tensor([0.0663, 0.0349, 0.3079, 0.0780, 0.0648, 0.0126, 0.1328, 0.1105, 0.0412,\n",
       "           0.0928, 0.0870, 0.0448, 0.0943, 0.0269, 0.5930, 0.1384, 0.0880, 0.0760,\n",
       "           0.1221, 0.0357, 0.0500, 0.1089, 0.0237, 0.0611, 0.1125, 0.0591, 0.0435,\n",
       "           0.0570, 0.0293, 0.2023, 0.0265, 0.0226, 0.2188]),\n",
       "   tensor([0.0671, 0.0561, 0.3610, 0.0896, 0.1034, 0.0149, 0.2473, 0.1480, 0.0541,\n",
       "           0.0811, 0.0902, 0.0507, 0.0813, 0.0218, 0.6813, 0.1513, 0.1371, 0.1021,\n",
       "           0.1224, 0.0415, 0.0837, 0.1478, 0.0291, 0.0390, 0.1079, 0.0802, 0.0445,\n",
       "           0.0544, 0.0434, 0.2511, 0.0316, 0.0173, 0.2583]),\n",
       "   tensor([0.0626, 0.0605, 0.4030, 0.0817, 0.0635, 0.0183, 0.2016, 0.1501, 0.0387,\n",
       "           0.0849, 0.1204, 0.0563, 0.0696, 0.0489, 0.5710, 0.1595, 0.1219, 0.0665,\n",
       "           0.1128, 0.0420, 0.0492, 0.1155, 0.0234, 0.0544, 0.0908, 0.0439, 0.0210,\n",
       "           0.0612, 0.0277, 0.1423, 0.0372, 0.0143, 0.2140]),\n",
       "   tensor([0.0800, 0.0624, 0.5033, 0.0811, 0.1073, 0.0197, 0.1915, 0.1658, 0.0642,\n",
       "           0.1043, 0.0623, 0.0459, 0.0861, 0.0389, 0.7306, 0.1864, 0.1515, 0.0814,\n",
       "           0.1087, 0.0423, 0.0477, 0.1284, 0.0186, 0.0359, 0.0858, 0.0705, 0.0455,\n",
       "           0.0586, 0.0535, 0.1239, 0.0267, 0.0219, 0.1893]),\n",
       "   tensor([0.0822, 0.0345, 0.2932, 0.1187, 0.0589, 0.0170, 0.1264, 0.1408, 0.0326,\n",
       "           0.1156, 0.0908, 0.0465, 0.0837, 0.0336, 0.5692, 0.1791, 0.1089, 0.0573,\n",
       "           0.1112, 0.0329, 0.0351, 0.1201, 0.0259, 0.0594, 0.1126, 0.0477, 0.0405,\n",
       "           0.0488, 0.0237, 0.1990, 0.0351, 0.0215, 0.2221]),\n",
       "   tensor([0.0856, 0.0432, 0.3798, 0.0891, 0.0793, 0.0186, 0.1274, 0.1387, 0.0565,\n",
       "           0.1241, 0.0796, 0.0418, 0.0941, 0.0342, 0.6625, 0.1653, 0.1103, 0.0689,\n",
       "           0.1602, 0.0360, 0.0665, 0.0794, 0.0279, 0.0625, 0.0875, 0.0481, 0.0303,\n",
       "           0.0450, 0.0478, 0.1662, 0.0313, 0.0226, 0.2205])]],\n",
       " 'phon_vecs': [[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "  [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0])]],\n",
       " 'phon_tokens': [[tensor([31]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([ 2, 14]),\n",
       "   tensor([14]),\n",
       "   tensor([14])],\n",
       "  [tensor([31]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([14]),\n",
       "   tensor([ 2, 14]),\n",
       "   tensor([14]),\n",
       "   tensor([14])]],\n",
       " 'global_encoding': tensor([[[-0.2313,  0.7228,  0.4216, -0.2759, -0.6799,  2.5179, -0.5481,\n",
       "           -1.7234, -0.5683, -1.7835,  0.7691, -1.3612, -0.6257, -0.0173,\n",
       "           -0.2946, -1.5995, -0.2475, -1.1370, -0.2397,  0.1449,  0.1369,\n",
       "            1.3791, -1.7256,  1.7593,  1.1316, -1.1334,  1.4173, -0.1635,\n",
       "            0.3895,  0.7134,  1.2306,  1.1893]],\n",
       " \n",
       "         [[-0.3477,  0.4001,  0.5504, -0.6418, -1.0981,  2.1670, -0.1438,\n",
       "           -1.1657, -0.7332, -1.7569,  0.7914, -1.5912, -0.6279,  0.2332,\n",
       "           -0.5414, -1.5302, -0.1053, -1.4906, -0.4463,  0.0644,  0.4286,\n",
       "            1.8827, -1.5557,  1.5377,  1.1638, -1.0928,  1.5193, -0.1205,\n",
       "            0.3251,  1.0666,  1.1875,  1.2534]]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = ds.character_tokenizer.encode([\"hero\", \"timer\"])\n",
    "model.generate(\"o2p\", datum['enc_input_ids'], datum['enc_pad_mask'], None, None, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2310,  0.1568,  1.0121,  0.0091, -0.9715,  1.6031, -0.3786, -1.5646,\n",
       "        -1.4997, -1.8750,  0.6249, -1.6910, -0.8417,  0.2227, -0.0109, -1.4188,\n",
       "        -0.1484, -0.9539,  0.4672, -0.2058,  0.3121,  1.7313, -1.3862,  1.4183,\n",
       "         1.0050, -1.6837,  1.0909,  0.7093,  0.6097,  0.9900,  1.5349,  0.9439])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['global_encoding'][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unprecedented',\n",
       " 'causal',\n",
       " 'walks',\n",
       " 'cupboard',\n",
       " 'greed',\n",
       " 'chartered',\n",
       " 'player',\n",
       " 'yards',\n",
       " 'outpost',\n",
       " 'mentally',\n",
       " 'intervening',\n",
       " 'paints',\n",
       " 'canada',\n",
       " 'professions',\n",
       " 'transported',\n",
       " 'unconscious',\n",
       " 'flew',\n",
       " 'nate',\n",
       " 'carpet',\n",
       " 'durable',\n",
       " 'tagged',\n",
       " 'tip',\n",
       " 'crow',\n",
       " 'happier',\n",
       " 'reside',\n",
       " 'cursing',\n",
       " 'mobs',\n",
       " 'accordance',\n",
       " 'forehead',\n",
       " 'abruptly',\n",
       " 'newer',\n",
       " 'washington',\n",
       " 'peanut',\n",
       " 'zebras',\n",
       " 'reproductive',\n",
       " 'finances',\n",
       " 'determines',\n",
       " 'ten',\n",
       " 'defective',\n",
       " 'beauty',\n",
       " 'minnie',\n",
       " 'battled',\n",
       " 'smokey',\n",
       " 'strains',\n",
       " 'bedding',\n",
       " 'thriving',\n",
       " 'occupational',\n",
       " 'governed',\n",
       " 'yugoslavia',\n",
       " 'steady',\n",
       " 'beating',\n",
       " 'matrix',\n",
       " 'vertebrae',\n",
       " 'dotted',\n",
       " 'boa',\n",
       " 'sutton',\n",
       " 'roberta',\n",
       " 'ineffective',\n",
       " 'quixote',\n",
       " 'fetus',\n",
       " 'deficit',\n",
       " 'fragments',\n",
       " 'eccentric',\n",
       " 'hordes']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
