{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model from checkpoints\n",
    "This notebook allows you to take checkpoints from a trained model and make predictions for a set of words for each checkpoint. The data are written to CSV, one for each checkpoint (where a checkpoint corresponds to an epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import tqdm\n",
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "from pathlib import Path\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Load in a dataset to Traindata class which is embedded inside the phonology tokenizer. Later implementations should consider breaking this process apart such that Traindata is initialized prior to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = type(\n",
    "    \"config\",\n",
    "    (object,),\n",
    "    {\"dataset_filename\": Path(\"data/kidwords_5000000_020724_.csv\")},\n",
    ")\n",
    "ds = ConnTextULDataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in checkpoints\n",
    "We nee the checkpoint file names in order to iterate through them, load and predict. Note that the data are written back in the same location from which the checkpoints are read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/modelresults59355/root_2024-04-24_15h38m59355ms_chkpt*.pth\"\n",
    "checkpoints = glob.glob(PATH)\n",
    "checkpoints.sort()\n",
    "print(f\"{checkpoints=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish batches\n",
    "Larger batches make for faster processing, but your machine may impose an upper limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_words=set()\n",
    "all_words.update(ds.words)\n",
    "batches = [list(all_words)[i:i+batch_size] for i in range(0, len(all_words), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions\n",
    "Iterate through and generate, write predictions for the words you've initialized in `config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for checkpoint in tqdm.tqdm(checkpoints):\n",
    "\n",
    "    outfile = checkpoint.replace(\".pth\", \".csv\")\n",
    "\n",
    "\n",
    "    chkpt = torch.load(checkpoint)\n",
    "    dfa = pd.DataFrame(columns=[\"phon_prediction\"])\n",
    "    model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "    model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dl = []\n",
    "\n",
    "    start_row = 0  # Initialize starting row for each new sheet\n",
    "\n",
    "    print(\"Checkpoint\", checkpoint, \"...started\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(batches)):\n",
    "            batch = batches[batch_idx]\n",
    "            new_row = {}\n",
    "            datum = ds.character_tokenizer.encode(batch)\n",
    "            pred = model.generate(\n",
    "                \"o2p\",\n",
    "                datum['enc_input_ids'],\n",
    "                datum['enc_pad_mask'],\n",
    "                None,\n",
    "                None,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            for idx, orth in enumerate(batch):\n",
    "                # Save the original input orthography\n",
    "                new_row[\"word_raw\"] = orth\n",
    "                # Save the target phonology for the above input orthography\n",
    "                phon = ds.cmudict[orth]\n",
    "                new_row[\"phon_target\"] = \":\".join(phon)\n",
    "                # Remove the start and end tokens from each phonological vector\n",
    "                # and convert them from tensors to lists\n",
    "                phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][idx][1:-1]]\n",
    "                # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "                phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "                    phon_pred_features, phonology=True, hot_nodes=True\n",
    "                )\n",
    "                # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "                # separated by colons\n",
    "                phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "                new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "                # Save a boolean indicating whether the prediction was correct\n",
    "                new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "                # Save the phonological features for the target phonology\n",
    "                phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "                if phon_target_features:\n",
    "                    phon_target_features = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(v.item()) for v in vector])\n",
    "                            for vector in phon_target_features[\"targets\"][0]\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    phon_target_features = \"None\"\n",
    "                new_row[\"phon_target_features\"] = phon_target_features\n",
    "                # Save the phonological features for the predicted phonology\n",
    "                phon_prediction_features = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(int(v.item())) for v in vector])\n",
    "                        for vector in pred[\"phon_vecs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "                # Save the phonological probabilities for the predicted phonology\n",
    "                phon_prediction_probabilities = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(v.item()) for v in vector])\n",
    "                        for vector in pred[\"phon_probs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "                # Save the global encoding vector for the predicted phonology\n",
    "                global_encoding = \";\".join(\n",
    "                    \n",
    "                        [\n",
    "                        \":\".join([str(v.item()) for v in vector]) \n",
    "                        for vector in pred[\"global_encoding\"][idx]\n",
    "                        ]\n",
    "                    \n",
    "                    )\n",
    "                new_row[\"global_encoding\"] = global_encoding\n",
    "\n",
    "                \n",
    "                dfa = pd.DataFrame([new_row])\n",
    "                dl.append(dfa)\n",
    "            print(\"Batch\", batch_idx, \"of\", len(batches), \"...done\")\n",
    "    pd.concat(dl).to_csv(outfile, index=False)\n",
    "    print(\"Checkpoint done:\", checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ds.phonology_tokenizer.encode([orth])['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_end = len(ds.phonology_tokenizer.traindata.phonreps[\"%\"])\n",
    "\n",
    "with open('models/modelresults59355/accuracy.csv', 'w') as f:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(batches)):\n",
    "            batch = batches[batch_idx]\n",
    "            new_row = {}\n",
    "            datum = ds.character_tokenizer.encode(batch)\n",
    "            pred = model.generate(\n",
    "                \"o2p\",\n",
    "                datum['enc_input_ids'],\n",
    "                datum['enc_pad_mask'],\n",
    "                None,\n",
    "                None,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            for idx, orth in enumerate(batch):\n",
    "\n",
    "                by_phoneme = []\n",
    "            \n",
    "                target = ds.phonology_tokenizer.encode([orth])['targets'][0].tolist()\n",
    "                prediction = [e.tolist() for e in pred['phon_vecs'][idx]]\n",
    "                \n",
    "                for i, e in enumerate(target):\n",
    "                    by_phoneme.append(e == prediction[i])    \n",
    "\n",
    "                phonemes_correct_total = sum(by_phoneme)\n",
    "                phonemes_correct_mean = sum(by_phoneme)/len(by_phoneme)\n",
    "\n",
    "                f.write(\"{}, {}, {}\\n\".format(orth, phonemes_correct_total, phonemes_correct_mean))\n",
    "\n",
    "            print(batch_idx, \"out of\", len(batches), \"...done\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
