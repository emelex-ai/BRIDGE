{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad for personal exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading cmudict: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "from src.model import Model\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/data_test50.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([48508, 10326, 14046, 25414, 24923, 39616, 20705, 14472, 30073,\\n            27181, 42380, 31641, 45987, 13295,  9703, 48658, 38992, 29223,\\n            31962,  6268],\\n           dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m [\u001b[39m20\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m250\u001b[39m, \u001b[39m500\u001b[39m, \u001b[39m1000\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     new_df \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mword_raw\u001b[39;49m\u001b[39m'\u001b[39;49m][torch\u001b[39m.\u001b[39;49mrandint(low\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m, size\u001b[39m=\u001b[39;49m(size,))\u001b[39m.\u001b[39;49mtolist()]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     new_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/data_test\u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/series.py:1008\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m   1006\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values(key)\n\u001b[0;32m-> 1008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_with(key)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/series.py:1043\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m key_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minteger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[key]\n\u001b[1;32m   1044\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:1074\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1071\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1073\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1074\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:1302\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1304\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:1240\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1239\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1242\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:1433\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1430\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1431\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1433\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1435\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py:6108\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6106\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6108\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6110\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6112\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py:6168\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6166\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6167\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6168\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6170\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([48508, 10326, 14046, 25414, 24923, 39616, 20705, 14472, 30073,\\n            27181, 42380, 31641, 45987, 13295,  9703, 48658, 38992, 29223,\\n            31962,  6268],\\n           dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "for size in [20, 50, 100, 250, 500, 1000]:\n",
    "    new_df = df['word_raw'][torch.randint(low=0, high=50000, size=(size,)).tolist()]\n",
    "    new_df.to_csv(f\"data/data_test{size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"data/data_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConnText..., nb_rows:  None\n",
      "ConnText..., which_dataset:  5\n",
      "CURRENT FOLDER:  /Users/erlebach/src/2023/ConnTextUL\n",
      "len(self.dataset):  (90544, 7)\n",
      "orth_vocab_size:  49\n",
      "phon_vocab_size:  34\n",
      "d_model:  64\n",
      "ds.dataset.shape:  (90544, 7)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ConnTextULDataset should have option to read less rows\n",
    "should test: nb_rows, test, which_dataset\n",
    "Test == True: \n",
    "  - Use which_data (nb_rows should be disabled)\n",
    "Test == False\n",
    "  - Randomly sample nb_rows. If nb_rows==None, read the entire dataset\n",
    "\n",
    "Suggestion: add a shuffle=Bool to CnnTextULDataset constructor when test=False\n",
    "\n",
    "Comment: vocabularies of orthography and phonology are independent of the input data size. Somehow,\n",
    "I don't think that should be the case. It could mess up testing. \n",
    "\"\"\"\n",
    "\n",
    "ds = ConnTextULDataset(nb_rows=None, test=False, which_dataset=5)\n",
    "num_layers = 2\n",
    "num_layers_dict={\n",
    "    'orth_enc': num_layers,\n",
    "    'orth_dec': num_layers,\n",
    "    'phon_enc': num_layers,\n",
    "    'phon_dec': num_layers,\n",
    "    'mixing_enc': num_layers\n",
    "}\n",
    "d_model = 64\n",
    "m = Model(orth_vocab_size=len(ds.character_tokenizer),\n",
    "          phon_vocab_size=len(ds.phonology_tokenizer),\n",
    "          d_model=d_model,\n",
    "          d_embedding=1,  # for mixing\n",
    "          max_orth_seq_len=2,\n",
    "          max_phon_seq_len=2,\n",
    "          nhead=4,\n",
    "          num_layers_dict=num_layers_dict\n",
    "          )\n",
    "m.eval();  # returns self (i.e., m)\n",
    "orth_vocab_size = len(ds.character_tokenizer)\n",
    "phon_vocab_size = len(ds.phonology_tokenizer)\n",
    "print(\"orth_vocab_size: \", orth_vocab_size)\n",
    "print(\"phon_vocab_size: \", phon_vocab_size)\n",
    "print(\"d_model: \", d_model)\n",
    "print(\"ds.dataset.shape: \", ds.dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'phonology_tokenizer', 'character_tokenizer', 'max_orth_seq_len', 'max_phon_seq_len', 'max_seq_len', 'words']\n",
      "(90544, 7)\n"
     ]
    }
   ],
   "source": [
    "print(list(ds.__dict__.keys()))\n",
    "print(ds.dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words =  ['as', 'she', 'and', 'you', 'was']\n",
      "orth['enc_input_ids'] =  torch.Size([5, 5]) tensor([[ 0, 11, 29,  1,  4],\n",
      "        [ 0, 29, 18, 15,  1],\n",
      "        [ 0, 11, 24, 14,  1],\n",
      "        [ 0, 35, 25, 31,  1],\n",
      "        [ 0, 33, 11, 29,  1]])\n",
      "orth['enc_pad_mask'] =  torch.Size([5, 5]) tensor([[False, False, False, False,  True],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "orth['dec_input_ids'] =  torch.Size([5, 4]) tensor([[ 0, 11, 29,  4],\n",
      "        [ 0, 29, 18, 15],\n",
      "        [ 0, 11, 24, 14],\n",
      "        [ 0, 35, 25, 31],\n",
      "        [ 0, 33, 11, 29]])\n",
      "orth['dec_pad_mask'] =  torch.Size([5, 4]) tensor([[False, False, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "==> phon['enc_input_ids'] =  5\n",
      "     [tensor([31]), tensor([14, 15, 17, 22, 29]), tensor([ 2,  7, 14]), tensor([32]), tensor([33])]\n",
      "     [tensor([31]), tensor([3, 7]), tensor([14, 15, 18, 24, 29]), tensor([32]), tensor([33])]\n",
      "     [tensor([31]), tensor([14, 16, 21]), tensor([ 2,  9, 14]), tensor([ 2,  6, 14]), tensor([32])]\n",
      "     [tensor([31]), tensor([ 3, 11, 14]), tensor([14, 17, 18, 24, 26, 29]), tensor([32]), tensor([33])]\n",
      "     [tensor([31]), tensor([ 0, 11]), tensor([14, 17, 23, 29]), tensor([ 2,  7, 14]), tensor([32])]\n",
      "phon['enc_pad_mask'] =  torch.Size([5, 5]) tensor([[False, False, False, False,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "==> phon['dec_input_ids'] =  5\n",
      "      [tensor([31]), tensor([14, 15, 17, 22, 29]), tensor([ 2,  7, 14]), tensor([33])]\n",
      "      [tensor([31]), tensor([3, 7]), tensor([14, 15, 18, 24, 29]), tensor([33])]\n",
      "      [tensor([31]), tensor([14, 16, 21]), tensor([ 2,  9, 14]), tensor([ 2,  6, 14])]\n",
      "      [tensor([31]), tensor([ 3, 11, 14]), tensor([14, 17, 18, 24, 26, 29]), tensor([33])]\n",
      "      [tensor([31]), tensor([ 0, 11]), tensor([14, 17, 23, 29]), tensor([ 2,  7, 14])]\n",
      "==> phon['dec_input_ids'] =  5\n",
      "phon['dec_pad_mask'] =  torch.Size([5, 4]) tensor([[False, False, False,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "phon['targets'] =  torch.Size([5, 4, 33]) tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "          0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "        [[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "          0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "          0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "          1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n"
     ]
    }
   ],
   "source": [
    "# self.vocab = ['[BOS]', '[EOS]', '[CLS]', '[UNK]', '[PAD]']\n",
    "# Ortho ids\n",
    "#      BOS: 0\n",
    "#      EOS: 1\n",
    "#      CLS: 2\n",
    "#      UNK: 3\n",
    "#      PAD: 4\n",
    "# Phono Ids\n",
    "#    BOS: 31  (0 0 0 0 ... 1 0 0)   (a 1 in 31st position)\n",
    "#    EOS: 32  (0 0 0 0 ... 0 1 0)\n",
    "#    PAD: 33  (0 0 0 0 ... 0 0 1)\n",
    "\n",
    "idx = 10040  # What is idx?phon['enc_input_ids'].shape, \n",
    "idx = 10031\n",
    "num_words = 5   # batch_size\n",
    "word = ds.words[idx:idx+num_words]\n",
    "print(\"words = \", word)\n",
    "batch = ds[idx:idx+num_words]\n",
    "#print(\"batch = \", batch)\n",
    "orth, phon = batch['orthography'].to('cpu'), batch['phonology'].to('cpu')\n",
    "# orth and phon are dicitonaries\n",
    "# ids represent either a character or a phoneme\n",
    "# Each word: 0,x,x,x,1,4  (padding is optional. 0 and 1 are mandatory)\n",
    "print(\"orth['enc_input_ids'] = \", orth['enc_input_ids'].shape, orth['enc_input_ids'])  # batch_size, max_orth_size\n",
    "print(\"orth['enc_pad_mask'] = \", orth['enc_pad_mask'].shape, orth['enc_pad_mask'])\n",
    "# Each word: 0,x,x,x,4  (padding is optional. 0 is  mandatory) No EoS on decoder input. Why? \n",
    "# Only input into the decoder a character if the next character will be predicted. \n",
    "# Since one does not predict the character folling EoS, it does not have to be in the decoder input\n",
    "print(\"orth['dec_input_ids'] = \", orth['dec_input_ids'].shape, orth['dec_input_ids'])\n",
    "print(\"orth['dec_pad_mask'] = \", orth['dec_pad_mask'].shape, orth['dec_pad_mask'])\n",
    "#print(\"phon['enc_input_ids'] = \", phon['enc_input_ids'].shape, phon['enc_input_ids'])\n",
    "# phon: no \"bOS and EOS\" tokens. Why not? \n",
    "print(\"==> phon['enc_input_ids'] = \", len(phon['enc_input_ids']))\n",
    "# Each phono is a list of integers: e.g. [3,5] corresponds to [0 0 0 1 0 1 0 0 0 0 ... 0 0 ]\n",
    "#                                   e.g. [0,3,5] corresponds to [1 0 0 1 0 1 0 0 0 0 ... 0 0 ]\n",
    "for  i in phon['enc_input_ids']:\n",
    "    print(\"    \", i)\n",
    "\n",
    "print(\"phon['enc_pad_mask'] = \", phon['enc_pad_mask'].shape, phon['enc_pad_mask'])\n",
    "#print(\"phon['dec_input_ids'] = \", phon['dec_input_ids'].shape, phon['dec_input_ids'])\n",
    "print(\"==> phon['dec_input_ids'] = \", len(phon['dec_input_ids']))\n",
    "for i in phon['dec_input_ids']:\n",
    "    print(\"     \", i)\n",
    "print(\"==> phon['dec_input_ids'] = \", len(phon['dec_input_ids']))\n",
    "print(\"phon['dec_pad_mask'] = \", phon['dec_pad_mask'].shape, phon['dec_pad_mask'])\n",
    "print(\"phon['targets'] = \", phon['targets'].shape, phon['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before mixed_encoding\n",
      "enter embed_tokens\n",
      "tokens shape:  torch.Size([5, 5])\n",
      "tokens:  tensor([[ 0, 11, 29,  1,  4],\n",
      "        [ 0, 29, 18, 15,  1],\n",
      "        [ 0, 11, 24, 14,  1],\n",
      "        [ 0, 35, 25, 31,  1],\n",
      "        [ 0, 33, 11, 29,  1]])\n",
      "=shape self.orthography_embedding(tokens):  torch.Size([5, 5, 64])\n",
      "=shape self.orth_position_embedding.weight[None, :tokens.shape[1]]:  torch.Size([1, 2, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m z \u001b[39m=\u001b[39m m(orth_enc_input\u001b[39m=\u001b[39;49morth[\u001b[39m'\u001b[39;49m\u001b[39menc_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       orth_enc_pad_mask\u001b[39m=\u001b[39;49morth[\u001b[39m'\u001b[39;49m\u001b[39menc_pad_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       orth_dec_input\u001b[39m=\u001b[39;49morth[\u001b[39m'\u001b[39;49m\u001b[39mdec_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m       orth_dec_pad_mask\u001b[39m=\u001b[39;49morth[\u001b[39m'\u001b[39;49m\u001b[39mdec_pad_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       phon_enc_input\u001b[39m=\u001b[39;49mphon[\u001b[39m'\u001b[39;49m\u001b[39menc_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m       phon_enc_pad_mask\u001b[39m=\u001b[39;49mphon[\u001b[39m'\u001b[39;49m\u001b[39menc_pad_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m       phon_dec_input\u001b[39m=\u001b[39;49mphon[\u001b[39m'\u001b[39;49m\u001b[39mdec_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m       phon_dec_pad_mask\u001b[39m=\u001b[39;49mphon[\u001b[39m'\u001b[39;49m\u001b[39mdec_pad_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m       )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/2023/ConnTextUL/src/model.py:332\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, orth_enc_input, orth_enc_pad_mask, orth_dec_input, orth_dec_pad_mask, phon_enc_input, phon_enc_pad_mask, phon_dec_input, phon_dec_pad_mask)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    322\u001b[0m     orth_enc_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     phon_dec_pad_mask,\n\u001b[1;32m    330\u001b[0m ):\n\u001b[1;32m    331\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore mixed_encoding\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m     mixed_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(\n\u001b[1;32m    333\u001b[0m         orth_enc_input, orth_enc_pad_mask, phon_enc_input, phon_enc_pad_mask\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter mixed_encoding\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    336\u001b[0m     \u001b[39m# print(mixed_encoding[0,0,:10])\u001b[39;00m\n",
      "File \u001b[0;32m~/src/2023/ConnTextUL/src/model.py:245\u001b[0m, in \u001b[0;36mModel.embed\u001b[0;34m(self, orthography, orthography_padding_mask, phonology, phonology_padding_mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m, orthography, orthography_padding_mask, phonology, phonology_padding_mask\n\u001b[1;32m    244\u001b[0m ):\n\u001b[0;32m--> 245\u001b[0m     orthography, phonology \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(orthography, \u001b[39m\"\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(\n\u001b[1;32m    246\u001b[0m         phonology, \u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m     orthography_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morthography_encoder(\n\u001b[1;32m    250\u001b[0m         orthography, src_key_padding_mask\u001b[39m=\u001b[39morthography_padding_mask\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m     \u001b[39m#print(f\"orthography_encoding.shape = {orthography_encoding.shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/src/2023/ConnTextUL/src/model.py:204\u001b[0m, in \u001b[0;36mModel.embed_tokens\u001b[0;34m(self, tokens, mode)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=shape self.orthography_embedding(tokens): \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morthography_embedding(tokens)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    201\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=shape self.orth_position_embedding.weight[None, :tokens.shape[1]]: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morth_position_embedding\u001b[39m.\u001b[39mweight[\u001b[39mNone\u001b[39;00m, :tokens\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    202\u001b[0m     \u001b[39mreturn\u001b[39;00m (   \u001b[39m# ERROR L197\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         \u001b[39m# ERROR: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morthography_embedding(tokens)   \u001b[39m# ERROR\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m         \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morth_position_embedding\u001b[39m.\u001b[39;49mweight[\u001b[39mNone\u001b[39;49;00m, : tokens\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]]\n\u001b[1;32m    206\u001b[0m     )  \u001b[39m# GE\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m# This is where we need to average the phonological embedding vectors\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "z = m(orth_enc_input=orth['enc_input_ids'],\n",
    "      orth_enc_pad_mask=orth['enc_pad_mask'],\n",
    "      orth_dec_input=orth['dec_input_ids'],\n",
    "      orth_dec_pad_mask=orth['dec_pad_mask'],\n",
    "      phon_enc_input=phon['enc_input_ids'],\n",
    "      phon_enc_pad_mask=phon['enc_pad_mask'],\n",
    "      phon_dec_input=phon['dec_input_ids'],\n",
    "      phon_dec_pad_mask=phon['dec_pad_mask']\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mz[\u001b[39m\u001b[39m'\u001b[39m\u001b[39morth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].shape\u001b[39m\u001b[39m\"\u001b[39m, z[\u001b[39m'\u001b[39m\u001b[39morth\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mz[\u001b[39m\u001b[39m'\u001b[39m\u001b[39morth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m] = \u001b[39m\u001b[39m\"\u001b[39m, z[\u001b[39m'\u001b[39m\u001b[39morth\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/scratchpad_GE.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39morth[\u001b[39m\u001b[39m'\u001b[39m\u001b[39menc_input_ids\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].shape = \u001b[39m\u001b[39m\"\u001b[39m, orth[\u001b[39m'\u001b[39m\u001b[39menc_input_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"z['orth'].shape\", z['orth'].shape)\n",
    "print(\"z['orth'] = \", z['orth'])\n",
    "print(\"orth['enc_input_ids'].shape = \", orth['enc_input_ids'].shape)\n",
    "labels = orth['enc_input_ids'][:,1:]\n",
    "print(\"orth['enc_input_ids'][:,1:] = \", labels)\n",
    "preds = torch.argmax(z['orth'], dim=1)\n",
    "print(\"torch.argmax(z['orth'], dim=1) = \", preds)\n",
    "accuracy = (preds == labels).sum() / len(labels)\n",
    "print(\"accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z['phon'].shape torch.Size([1, 2, 4, 33])\n",
      "z['phon'] =  tensor([[[[ 4.4194e-01,  1.2137e-02, -1.1275e+00, -1.3524e-01,  6.3349e-02,\n",
      "           -2.1050e-01, -9.0739e-02,  3.0333e-01,  5.6287e-01, -1.5716e-03,\n",
      "           -4.5574e-01, -8.8808e-01, -7.1200e-01, -3.6442e-01,  7.7940e-02,\n",
      "           -1.1434e-01,  3.8295e-01, -5.4414e-01,  4.1145e-01,  2.9466e-01,\n",
      "            3.3463e-01, -2.9968e-02, -7.9627e-01,  1.0003e+00, -1.3511e-01,\n",
      "            1.0474e+00, -7.4798e-02,  2.0223e-01, -2.5267e-01,  4.4629e-01,\n",
      "           -2.7202e-02, -4.7697e-01, -4.0632e-01],\n",
      "          [ 7.3698e-01,  8.1904e-02,  6.9783e-01, -1.5817e-01,  8.8913e-01,\n",
      "           -3.7205e-01, -1.2592e-01,  1.2707e-01,  1.7307e-01,  2.5315e-01,\n",
      "           -1.6076e-01,  3.8544e-01, -5.7465e-01, -7.1618e-01, -8.9231e-01,\n",
      "           -5.9913e-01, -3.8458e-01,  9.2893e-02,  1.3562e-01,  4.3413e-01,\n",
      "            1.0148e-01,  1.6369e-01, -1.5533e-01,  3.9790e-01, -1.9901e-02,\n",
      "            3.8274e-01,  9.4589e-02,  4.4247e-01, -7.1658e-01,  5.7099e-01,\n",
      "           -4.6511e-01,  1.8199e-01, -1.3451e-01],\n",
      "          [ 7.6317e-01,  6.1599e-01,  1.2847e-01,  3.0220e-01,  1.0287e+00,\n",
      "            6.5214e-01, -1.2834e-01,  6.6172e-01,  2.0579e-01,  4.4686e-01,\n",
      "           -8.1047e-01,  9.9459e-02, -9.2888e-01, -7.9728e-01, -4.1217e-01,\n",
      "           -3.5965e-02,  1.9654e-01, -8.0664e-01, -8.1127e-01,  5.4127e-01,\n",
      "           -2.9452e-01, -2.7105e-01, -7.7490e-01,  4.3423e-01,  1.4548e-01,\n",
      "           -4.0843e-01, -1.6619e-01,  7.3791e-01, -1.2662e+00, -6.7386e-01,\n",
      "            1.6885e-01, -4.7113e-02,  1.6506e-01],\n",
      "          [ 4.7252e-01,  1.0143e-01, -2.7045e-01, -8.1715e-02,  6.3961e-01,\n",
      "            5.6352e-01, -2.0884e-01,  9.6811e-01, -4.5370e-01,  5.5363e-01,\n",
      "           -6.0811e-01,  4.2300e-02, -1.0678e+00, -5.5507e-01,  6.9242e-01,\n",
      "            5.7591e-01, -6.6930e-03, -7.8298e-01, -8.0180e-01,  4.6426e-01,\n",
      "           -2.6852e-01, -6.3184e-01, -5.5849e-01,  4.4380e-02,  6.3126e-01,\n",
      "           -3.3354e-01, -1.3223e-01,  6.3155e-01, -4.2745e-01, -4.7519e-01,\n",
      "            7.7706e-01,  5.9871e-01,  3.1433e-01]],\n",
      "\n",
      "         [[ 9.8696e-01, -2.6378e-01,  2.3596e-01,  6.6254e-01, -1.9409e-01,\n",
      "           -7.8220e-01, -1.0239e+00,  6.3048e-01, -1.1721e-01,  1.4005e-01,\n",
      "           -4.4182e-01, -2.6614e-01, -8.9335e-02, -9.1511e-01, -2.7059e-01,\n",
      "           -1.1936e+00,  1.3401e+00, -2.1598e-01,  4.0915e-01, -3.1213e-01,\n",
      "            2.4303e-01,  2.0617e-01, -5.9141e-01,  4.6475e-02, -3.7313e-01,\n",
      "            2.0697e-03,  5.7910e-01, -1.0814e+00,  3.0084e-01, -8.2681e-01,\n",
      "            4.2340e-02, -3.2863e-01, -1.0736e+00],\n",
      "          [-5.8803e-01, -4.2684e-01, -3.5978e-01,  7.4431e-01, -8.1110e-01,\n",
      "           -2.1331e-01, -7.4900e-01, -1.2795e-01,  5.4836e-01,  6.2540e-01,\n",
      "           -4.0505e-01, -8.0897e-01, -5.3420e-01, -1.2529e-01, -2.1185e-01,\n",
      "            3.5994e-01,  1.0670e+00, -1.1461e-02, -2.2455e-01,  6.0371e-01,\n",
      "            4.5858e-01,  3.7534e-01, -1.0564e+00,  2.2625e-01, -9.8975e-01,\n",
      "           -1.6241e-01,  4.1808e-01, -2.5653e-01, -1.3695e-01, -1.4705e+00,\n",
      "            1.2783e-02,  2.7858e-01,  7.2379e-02],\n",
      "          [ 6.1295e-01, -6.0160e-02, -3.5900e-01,  3.5104e-01, -5.4039e-01,\n",
      "            8.6590e-01, -3.3836e-01,  3.7957e-01, -4.8124e-01,  6.6177e-01,\n",
      "           -1.2401e+00,  3.2924e-02, -6.0907e-01,  5.4467e-01,  1.5743e-01,\n",
      "           -8.4735e-01,  6.7150e-02,  1.4116e-01,  4.9234e-01,  1.2134e+00,\n",
      "            9.6406e-01,  1.3624e+00, -1.3971e+00, -2.8130e-03,  6.7727e-02,\n",
      "           -4.9513e-01, -9.6479e-01, -8.5311e-01,  2.2457e-01, -1.2246e+00,\n",
      "           -9.4762e-01,  3.3113e-01, -1.0932e-01],\n",
      "          [ 3.9322e-01, -2.1491e-01,  3.0202e-01, -5.3477e-02,  1.0120e-01,\n",
      "            7.1607e-01,  4.3224e-01, -2.1435e-01,  5.5517e-01,  6.3975e-01,\n",
      "           -2.2153e-02,  7.8546e-01,  2.4075e-01,  4.4637e-01,  3.0457e-03,\n",
      "            1.4509e-01,  8.2804e-01, -5.1331e-01,  7.6806e-01,  3.3436e-01,\n",
      "            1.2652e-02,  8.5741e-01, -1.6880e+00,  1.8133e-01,  8.6929e-03,\n",
      "           -6.2761e-01, -1.6737e-01, -1.0705e+00,  7.6095e-02, -2.7709e-01,\n",
      "           -9.9872e-01,  8.0371e-01,  3.0582e-01]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "          0, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "          0, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "         [0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "          1, 0, 0, 0, 0, 1, 1, 0, 1, 0]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"z['phon'].shape\", z['phon'].shape)\n",
    "print(\"z['phon'] = \", z['phon'])\n",
    "torch.argmax(z['phon'], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = m(orthography['enc_input_ids'], orthography['enc_pad_mask'],\n",
    "                orthography['dec_input_ids'], orthography['dec_pad_mask'],\n",
    "                phonology['enc_input_ids'], phonology['enc_pad_mask'],\n",
    "                phonology['dec_input_ids'], phonology['dec_pad_mask'])\n",
    "        \n",
    "orth_loss = pt.nn.CrossEntropyLoss(ignore_index=4)(logits['orth'], orthography['enc_input_ids'][:,1:]) \n",
    "phon_loss = pt.nn.CrossEntropyLoss(ignore_index=2)(logits['phon'], phonology['targets'])\n",
    "loss = orth_loss + phon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phon_output.shape =  torch.Size([1, 1, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 1, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 1, 33])\n",
      "phon_output.shape =  torch.Size([1, 2, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 2, 33])\n",
      "phon_output.shape =  torch.Size([1, 3, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 3, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 3, 33])\n",
      "phon_output.shape =  torch.Size([1, 4, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 4, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 4, 33])\n",
      "phon_output.shape =  torch.Size([1, 5, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 5, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 5, 33])\n",
      "phon_output.shape =  torch.Size([1, 6, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 6, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 6, 33])\n",
      "phon_output.shape =  torch.Size([1, 7, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 7, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 7, 33])\n",
      "phon_output.shape =  torch.Size([1, 8, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 8, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 8, 33])\n",
      "phon_output.shape =  torch.Size([1, 9, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 9, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 9, 33])\n",
      "phon_output.shape =  torch.Size([1, 10, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 10, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 10, 33])\n",
      "phon_output.shape =  torch.Size([1, 11, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 11, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 11, 33])\n",
      "phon_output.shape =  torch.Size([1, 12, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 12, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 12, 33])\n",
      "phon_output.shape =  torch.Size([1, 13, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 13, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 13, 33])\n",
      "phon_output.shape =  torch.Size([1, 14, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 14, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 14, 33])\n",
      "phon_output.shape =  torch.Size([1, 15, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 15, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 15, 33])\n",
      "phon_output.shape =  torch.Size([1, 16, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 16, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 16, 33])\n",
      "phon_output.shape =  torch.Size([1, 17, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 17, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 17, 33])\n",
      "phon_output.shape =  torch.Size([1, 18, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 18, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 18, 33])\n",
      "phon_output.shape =  torch.Size([1, 19, 64])\n",
      "phonology_token_logits.shape =  torch.Size([1, 19, 66])\n",
      "phonology_token_logits.shape =  torch.Size([1, 2, 19, 33])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[16, 40,  4,  0, 16, 15,  5, 45,  7, 31, 19, 16, 42, 40,  8, 15,  0, 16,\n",
       "         42]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_prime = m.generate(orth_enc_input=orth, \n",
    "                     orth_enc_pad_mask=torch.zeros_like(orth).bool(), \n",
    "                     phon_enc_input=phon, \n",
    "                     phon_enc_pad_mask=torch.zeros_like(phon).bool(), \n",
    "                     deterministic=True)['orth'][:, 1:]\n",
    "z_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6767, -0.1219, -0.5411,  0.1170, -0.3387, -0.1288, -0.5503,\n",
       "            0.6902,  0.1972, -0.2650,  0.5467,  0.9316,  0.4024, -0.4422,\n",
       "           -0.1477,  0.7241, -0.9001, -0.2532,  0.5851,  1.1965, -0.7901,\n",
       "            0.2979, -0.0242,  0.4706,  0.8686,  0.1165,  0.4917,  0.5010,\n",
       "           -0.1539, -0.0028,  0.1609, -0.2441,  0.3435]],\n",
       "\n",
       "         [[ 0.5110,  0.4595, -0.6616,  0.0228, -0.6862,  0.1313,  0.8328,\n",
       "            0.2021,  0.2570,  0.8814,  0.1321, -0.1988, -0.1504, -0.1617,\n",
       "            0.7846, -0.2460,  0.9513, -0.4330, -0.6739,  0.0562,  0.5938,\n",
       "            0.0119, -0.3092, -0.0674,  0.9380,  0.5049,  0.0826, -0.9075,\n",
       "            0.3517,  0.6457,  0.2784, -0.3076,  0.8851]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#assert z.item() == z_prime.item()\n",
    "\n",
    "################# Check Phonology ########################\n",
    "z = m(orth, \n",
    "      torch.zeros_like(orth).bool(), \n",
    "      orth, \n",
    "      torch.zeros_like(orth).bool(),\n",
    "      [[phon][0]], \n",
    "      torch.zeros_like(phon).bool(), \n",
    "      [[phon][0]], \n",
    "      torch.zeros_like(phon).bool()\n",
    ")['phon']; z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "z_prime = m.generate(orth, \n",
    "                     torch.zeros_like(orth).bool(), \n",
    "                     [[phon][0]], \n",
    "                     torch.zeros_like(phon).bool(), \n",
    "                     deterministic=True)['phon']\n",
    "z_prime\n",
    "\n",
    "#assert (torch.where(z[0].argmax(dim=0) == 1)[1] == z_prime[0][1]).all()\n",
    "print(len(z_prime[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,C,E = 2,3,4\n",
    "a = torch.rand((B,C,E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(a.transpose(1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4301, 0.9611, 0.7723, 0.9204, 0.1864, 0.3543, 0.0298, 0.9956,\n",
       "          0.0215, 0.8572, 0.2218, 0.4789, 0.0823, 0.7532, 0.1532, 0.0425,\n",
       "          0.4336, 0.8475, 0.8205, 0.8800, 0.2938, 0.7716, 0.4422, 0.3135,\n",
       "          0.1729, 0.0260, 0.9826, 0.9753, 0.2033, 0.6105, 0.4312, 0.0927,\n",
       "          0.9542],\n",
       "         [0.7559, 0.8657, 0.4542, 0.3938, 0.1533, 0.3608, 0.4802, 0.3396,\n",
       "          0.7139, 0.2800, 0.1930, 0.4848, 0.7758, 0.4037, 0.2637, 0.3022,\n",
       "          0.6368, 0.4707, 0.4163, 0.9229, 0.5459, 0.4463, 0.9723, 0.4306,\n",
       "          0.7044, 0.4951, 0.3131, 0.5019, 0.4948, 0.4797, 0.4198, 0.3487,\n",
       "          0.9346]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=torch.rand((1,2,33)); p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(p[:,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(p)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7559, 0.8657, 0.4542, 0.3938, 0.1533, 0.3608, 0.4802, 0.3396, 0.7139,\n",
       "         0.2800, 0.1930, 0.4848, 0.7758, 0.4037, 0.2637, 0.3022, 0.6368, 0.4707,\n",
       "         0.4163, 0.9229, 0.5459, 0.4463, 0.9723, 0.4306, 0.7044, 0.4951, 0.3131,\n",
       "         0.5019, 0.4948, 0.4797, 0.4198, 0.3487, 0.9346]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  8, 12, 16, 19, 20, 22, 24, 27, 32])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(p[:,1] > 0.5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load('models/model0_checkpoint99.pth', map_location=torch.device('cpu'))\n",
    "m = Model(len(ds.character_tokenizer), len(ds.phonology_tokenizer), d_model=chkpt['d_model'], nhead=chkpt['nhead'])\n",
    "m.load_state_dict(chkpt['model'])\n",
    "m.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word =  ['sam']\n"
     ]
    }
   ],
   "source": [
    "idx = 10040\n",
    "word = ds.words[idx:idx+1]\n",
    "print(\"word = \", word)\n",
    "batch = ds[idx:idx+1]\n",
    "#print(\"batch = \", batch)\n",
    "orth, phon = batch['orthography'].to('cpu'), batch['phonology'].to('cpu')\n",
    "orthography = orth['enc_input_ids']\n",
    "orthography_mask = orth['enc_pad_mask']\n",
    "phonology = phon['enc_input_ids']\n",
    "phonology_mask = phon['enc_pad_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = m.generate(orthography, orthography_mask, phonology, phonology_mask, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BOS]ci[EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS][EOS]']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.character_tokenizer.decode(generation['orth'][:,:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([31]),\n",
       " tensor([ 9, 14]),\n",
       " tensor([14, 15, 17, 26, 28, 29]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([32]),\n",
       " tensor([14, 18]),\n",
       " tensor([32]),\n",
       " tensor([32])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation['phon'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.phonology_tokenizer.decode(generation['phon'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', '[EOS]', '[CLS]', '[UNK]', '[PAD]', 'i', '', '\\x80', 'o', 'f', '', 'v', '', '.', ',', 'r', \"'\", '\\x9d', '\\x9c', 'g', 'k', 'c', 'n', 'm', 'z', '-', 'y', 'e', 'h', 'j', 'p', 'x', 's', '\"', 'q', '\\x94', '', '', 'w', '', 'l', 'a', '!', 'u', 'b', 'd', 't', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(ds.character_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_protocol',\n",
       " 'character_tokenizer',\n",
       " 'dataset',\n",
       " 'phonology_tokenizer',\n",
       " 'words']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
