{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model reload\n",
    "## Author: G. Erlebacher\n",
    "We will perform the following experiment in `--test` mode. \n",
    "0. Initialize the model and save the initial state, `model0`\n",
    "1. Run the `model0` for a single epoch and save it to `model1`\n",
    "2. Run `model0` for two epochs and save the results to `model2a`\n",
    "3. load `model1` and run `model1` for a single epoch. Save this to `model2b`\n",
    "4. Compare `model2a` and `model2b`. They should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up auto-reloading of modules\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Option\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# os.chdir('/path/to/root/folder')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwandb_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m WandbWrapper\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ConnTextULDataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlebach/src/2023/ConnTextUL/GE_notebooks/model_reload.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Option\n",
    "# import os\n",
    "# os.chdir('/path/to/root/folder')\n",
    "\n",
    "from src.wandb_wrapper import WandbWrapper\n",
    "from src.dataset import ConnTextULDataset\n",
    "from src.model import Model\n",
    "from src.main import hardcoded_args\n",
    "from src.train_impl import create_data_slices\n",
    "import src.train_impl as train_impl\n",
    "import torch\n",
    "from attrdict import AttrDict\n",
    "import tqdm\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "import pandas as pd\n",
    "\n",
    "wandb = WandbWrapper()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_context():\n",
    "    ds = ConnTextULDataset(None, test=True, which_dataset=100, nb_rows=-1)\n",
    "    return ds\n",
    "\n",
    "ds = reset_context()\n",
    "ds1 = reset_context()\n",
    "c = hardcoded_args()\n",
    "\n",
    "dd = ds[:2]\n",
    "print(\"dd: \", dd['phonology'])\n",
    "\n",
    "dd1 = ds1[:2]\n",
    "print(\"dd1: \", dd1['phonology'])\n",
    "\n",
    "# for d in range(len(ds)):\n",
    "    # print(ds[d])\n",
    "    # for k,v in d.items():\n",
    "    #     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "\n",
    "num_layers_dict = {\n",
    "    \"phon_dec\": num_layers,\n",
    "    \"phon_enc\": num_layers,\n",
    "    \"orth_dec\": num_layers,\n",
    "    \"orth_enc\": num_layers,\n",
    "    \"mixing_enc\": num_layers,\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    len(ds.character_tokenizer),\n",
    "    len(ds.phonology_tokenizer),\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    max_orth_seq_len=ds.max_orth_seq_len,\n",
    "    max_phon_seq_len=ds.max_phon_seq_len,\n",
    "    num_layers_dict=num_layers_dict,\n",
    "    d_embedding=1,\n",
    ")\n",
    "\n",
    "print(\"load, global embedding.requires_grad: \", model.state_dict()['global_embedding'].requires_grad    )\n",
    "# Why is it that reuqires_grad is false? That would explain why waits are not changing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = AttrDict({\"batch_size_train\": 1, \"batch_size_val\": 1, \"train_test_split\": 0.8})\n",
    "c.continue_training = False\n",
    "c.d_model = 16\n",
    "c.nhead = 1\n",
    "c.d_embedding = 1\n",
    "# c.pathway = 'o2p'\n",
    "c.pathway = 'op2op'\n",
    "c.learning_rate = 1.e-3\n",
    "\n",
    "num_train = int(len(ds) * c.train_test_split)\n",
    "# train_dataset_slices, val = create_data_slices(num_train, c, ds)\n",
    "# val_cutpoint = val[0].start\n",
    "# datum = ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models_test\"\n",
    "model, opt = train_impl.setup_model(MODEL_PATH, c, ds, num_layers_dict)\n",
    "print(\"opt: \", opt)\n",
    "print(\"opt.state_dict(): \", opt.state_dict())\n",
    "train_impl.print_weight_norms(model, \"norms of model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial model \n",
    "epoch = 0\n",
    "# epoch_num not currently used\n",
    "model_id = 3\n",
    "epoch_num = 0\n",
    "\n",
    "print(\"==> before train_impl.save\")\n",
    "train_impl.save(epoch, c, model, opt, MODEL_PATH, model_id, epoch_num=0)\n",
    "print(\"==> after train_impl.save\")\n",
    "\n",
    "for m in model.parameters():\n",
    "    print(f\"model: {m.requires_grad=}\")\n",
    "    break\n",
    "\n",
    "model_file_name = train_impl.get_model_file_name(model_id, epoch_num)\n",
    "print(\"==> before train_impl.load_model\")  \n",
    "model1, opt1, c1 = train_impl.load_model(MODEL_PATH, model_id, epoch_num)\n",
    "train_impl.print_weight_norms(model1, \"norms of model1\")\n",
    "# print(\"opt1: \", opt1)\n",
    "\n",
    "for m in model1.parameters():\n",
    "    print(f\"model1: {m.requires_grad=}\")  # it is True\n",
    "    break\n",
    "\n",
    "print(\"norms of weights are the same. Use in pytest\")\n",
    "\n",
    "assert c1 == c, \"config dictionaries are not the same\"\n",
    "assert opt.state_dict() == opt1.state_dict(), \"opt.state_dict are not the same\"\n",
    "\n",
    "# Assume model1 and model2 are instances of your model\n",
    "state_dict  = model.state_dict()\n",
    "state_dict1 = model1.state_dict()\n",
    "assert train_impl.compare_state_dicts(state_dict, state_dict1), \"Model State dicts are not equal.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataset_slices(ds, c):\n",
    "    \"\"\"\n",
    "    Reset Dataset Slices\n",
    "\n",
    "    Parameters:\n",
    "    - ds\n",
    "    - c\n",
    "    \"\"\"\n",
    "    ds = reset_context()  # SHOULD NOT BE REQUIRED, unless ds is modified in place\n",
    "    num_train = int(len(ds) * c.train_test_split)\n",
    "    train_dataset_slices, val_dataset_slices = train_impl.create_data_slices(\n",
    "            num_train, c, ds\n",
    "        )\n",
    "    c.n_steps_per_epoch = len(train_dataset_slices)\n",
    "    return train_dataset_slices, val_dataset_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.num_epochs = 0  # WHAT IS THIS?\n",
    "pbar = tqdm.tqdm(range(epoch_num, epoch_num + c.num_epochs), position=0)\n",
    "device = 'cpu'\n",
    "example_ct = [0]\n",
    "c.max_nb_steps = 4\n",
    "wandb.is_wandb_on = False\n",
    "generated_text_table = wandb.Table(columns=[\"Step\", \"Generated Output\"])\n",
    "\n",
    "# Closures to simplify function calls. I need to repeat them just before running \n",
    "# the models to ensure that train_dataset_slices is the same in both cases\n",
    "def setup_closures(model, opt):\n",
    "    \"\"\"\n",
    "    This closure function cannot be moved elsehwere, because it depends on `train_data_slices` and other\n",
    "    variables to be in the global contecxt\n",
    "    \"\"\"\n",
    "    example_ct = [0]\n",
    "    train_dataset_slices, _ = reset_dataset_slices(ds, c)\n",
    "    def single_step_fct(batch_slice, step, epoch, mode):\n",
    "      return train_impl.single_step(\n",
    "        c,\n",
    "        pbar,\n",
    "        model,\n",
    "        train_dataset_slices,\n",
    "        batch_slice,\n",
    "        ds,\n",
    "        device,\n",
    "        opt,\n",
    "        epoch,\n",
    "        step,\n",
    "        generated_text_table,\n",
    "        example_ct,\n",
    "        mode,\n",
    "    )\n",
    "\n",
    "    def train_single_epoch_fct(epoch):\n",
    "      return train_impl.train_single_epoch(\n",
    "        c,\n",
    "        model,\n",
    "        train_dataset_slices,\n",
    "        epoch,\n",
    "        single_step_fct,\n",
    "    )\n",
    "\n",
    "    return single_step_fct, train_single_epoch_fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and model1 on the same initial data. Compare metrics\n",
    "\n",
    "def run_train(model, opt, num_epochs):\n",
    "    c.seed = 100\n",
    "    torch.manual_seed(c.seed)\n",
    "    torch.cuda.manual_seed_all(c.seed)\n",
    "    single_step_fct, train_single_epoch_fct = setup_closures(model, opt)\n",
    "    metrics: List[Dict] = [{}]\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"************* epoch: \", epoch, \" *******************88\")\n",
    "        metrics[0] = train_single_epoch_fct(epoch)\n",
    "        # print(metrics[0])\n",
    "        return metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for m in model.parameters():\n",
    "    print(f\"model: {m.requires_grad=}\")  # it is True\n",
    "    break\n",
    "for m in model1.parameters():\n",
    "    print(f\"model1: {m.requires_grad=}\")  # it is True\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(model, opt, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(model1, opt1, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt1.state_dict())\n",
    "print(opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.state_dict())\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in opt.state_dict().items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in opt1.state_dict().items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in opt1.state_dict().items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt, opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
