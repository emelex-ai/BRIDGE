{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache folder: /workspaces/ConnTextUL/data/.cache already exists\n"
     ]
    }
   ],
   "source": [
    "# Load in a dataset to access Matt's Traindata class which\n",
    "# is embedded inside the phonology tokenizer (consider changing this\n",
    "from pathlib import Path\n",
    "\n",
    "config = type(\n",
    "    \"config\",\n",
    "    (object,),\n",
    "    {\"dataset_filename\": Path(\"/workspaces/ConnTextUL/data/data.csv\")},\n",
    ")\n",
    "ds = ConnTextULDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Woodcock Johnson III Form A dataset for assessment\n",
    "with open(\"data/wj_iii_form_a.json\", \"r\") as f:\n",
    "    wj3_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template dataframe to populate for each checkpoint\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"word_raw\",\n",
    "        \"phon_target\",\n",
    "        \"phon_prediction\",\n",
    "        \"correct\",\n",
    "        \"phon_target_features\",\n",
    "        \"phon_prediction_features\",\n",
    "        \"phon_prediction_probabilities\",\n",
    "        \"global_encoding\",\n",
    "        \"in_wj3\",\n",
    "        \"in_traindata\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints=['models/root_2024-03-04_14h08m31745ms_chkpt001.pth', 'models/root_2024-03-04_14h08m31745ms_chkpt002.pth', 'models/root_2024-03-04_16h02m25184ms_chkpt001.pth', 'models/root_2024-03-04_16h02m25184ms_chkpt002.pth', 'models/root_2024-03-04_16h07m30392ms_chkpt001.pth', 'models/root_2024-03-04_16h07m30392ms_chkpt002.pth', 'models/root_2024-03-04_16h08m15959ms_chkpt001.pth', 'models/root_2024-03-04_16h08m15959ms_chkpt002.pth', 'models/root_2024-03-04_17h17m55970ms_chkpt001.pth', 'models/root_2024-03-04_17h17m55970ms_chkpt002.pth']\n"
     ]
    }
   ],
   "source": [
    "# read in the checkpoint file names. We will load one at a time\n",
    "# and generate predictions of the wj3 assessments one at at ime\n",
    "checkpoints = glob.glob(\"models/root*.pth\")\n",
    "checkpoints.sort()\n",
    "print(f\"{checkpoints=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--In Generate--\n",
      "orth_enc_input:  torch.Size([3, 14])\n",
      "orth_enc_pad_mask:  torch.Size([3, 14])\n",
      "phon_enc_input:  torch.Size([3, 14])\n",
      "phon_enc_pad_mask:  torch.Size([3, 14])\n",
      "generated_phon_tokens=[[tensor([31])], [tensor([31])], [tensor([31])]]\n",
      "--In phonology_decoder_loop--\n",
      "generated_phon_embeddings.shape=torch.Size([3, 1, 64])\n",
      "prompt_encoding.shape=torch.Size([3, 1, 64])\n",
      "len(generated_phon_tokens)=3\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 1, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([1, 1])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.8323, 0.7345, 0.6179, 0.8690, 0.7797, 0.7236, 0.6461, 0.6367,\n",
      "          0.7406, 0.6195, 0.8935, 0.7721, 0.6925, 0.6723, 0.6889, 0.5953,\n",
      "          0.8798, 0.7288, 0.8581, 0.7074, 0.6787, 0.5445, 0.6221, 0.6496,\n",
      "          0.6130, 0.6879, 0.8145, 0.6857, 0.8033, 0.5670, 0.6682, 0.5927,\n",
      "          0.6022],\n",
      "         [0.1677, 0.2655, 0.3821, 0.1310, 0.2203, 0.2764, 0.3539, 0.3633,\n",
      "          0.2594, 0.3805, 0.1065, 0.2279, 0.3075, 0.3277, 0.3111, 0.4047,\n",
      "          0.1202, 0.2712, 0.1419, 0.2926, 0.3213, 0.4555, 0.3779, 0.3504,\n",
      "          0.3870, 0.3121, 0.1855, 0.3143, 0.1967, 0.4330, 0.3318, 0.4073,\n",
      "          0.3978]],\n",
      "\n",
      "        [[0.8243, 0.7277, 0.6542, 0.8686, 0.7847, 0.7413, 0.6710, 0.6252,\n",
      "          0.7372, 0.6141, 0.8736, 0.7717, 0.6987, 0.6445, 0.6790, 0.5448,\n",
      "          0.8606, 0.6962, 0.8758, 0.7111, 0.6682, 0.5159, 0.6580, 0.6293,\n",
      "          0.6086, 0.6573, 0.8268, 0.6804, 0.7804, 0.5582, 0.6666, 0.5945,\n",
      "          0.5938],\n",
      "         [0.1757, 0.2723, 0.3458, 0.1314, 0.2153, 0.2587, 0.3290, 0.3748,\n",
      "          0.2628, 0.3859, 0.1264, 0.2283, 0.3013, 0.3555, 0.3210, 0.4552,\n",
      "          0.1394, 0.3038, 0.1242, 0.2889, 0.3318, 0.4841, 0.3420, 0.3707,\n",
      "          0.3914, 0.3427, 0.1732, 0.3196, 0.2196, 0.4418, 0.3334, 0.4055,\n",
      "          0.4062]],\n",
      "\n",
      "        [[0.8180, 0.7019, 0.6226, 0.8510, 0.7953, 0.7579, 0.6502, 0.6343,\n",
      "          0.7281, 0.6024, 0.8939, 0.7821, 0.6870, 0.6643, 0.7182, 0.5806,\n",
      "          0.8894, 0.7469, 0.8722, 0.6981, 0.6566, 0.5272, 0.6522, 0.6502,\n",
      "          0.5925, 0.6724, 0.8354, 0.6651, 0.8127, 0.5889, 0.6681, 0.5954,\n",
      "          0.6254],\n",
      "         [0.1820, 0.2981, 0.3774, 0.1490, 0.2047, 0.2421, 0.3498, 0.3657,\n",
      "          0.2719, 0.3976, 0.1061, 0.2179, 0.3130, 0.3357, 0.2818, 0.4194,\n",
      "          0.1106, 0.2531, 0.1278, 0.3019, 0.3434, 0.4728, 0.3478, 0.3498,\n",
      "          0.4075, 0.3276, 0.1646, 0.3349, 0.1873, 0.4111, 0.3319, 0.4046,\n",
      "          0.3746]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([], dtype=torch.int64), tensor([], dtype=torch.int64))\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=tensor([33])\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=tensor([33])\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=tensor([33])\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33])], [tensor([31]), tensor([33])], [tensor([31]), tensor([33])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 2, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([2, 2])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.8292, 0.5617, 0.6029, 0.8265, 0.8653, 0.6266, 0.8632, 0.6451,\n",
      "          0.6987, 0.6487, 0.8305, 0.7820, 0.5503, 0.6957, 0.5912, 0.4230,\n",
      "          0.8113, 0.6329, 0.7786, 0.8644, 0.6991, 0.5899, 0.6734, 0.5938,\n",
      "          0.7093, 0.8029, 0.4897, 0.7512, 0.7744, 0.5412, 0.8702, 0.5548,\n",
      "          0.6144],\n",
      "         [0.1708, 0.4383, 0.3971, 0.1735, 0.1347, 0.3734, 0.1368, 0.3549,\n",
      "          0.3013, 0.3513, 0.1695, 0.2180, 0.4497, 0.3043, 0.4088, 0.5770,\n",
      "          0.1887, 0.3671, 0.2214, 0.1356, 0.3009, 0.4101, 0.3266, 0.4062,\n",
      "          0.2907, 0.1971, 0.5103, 0.2488, 0.2256, 0.4588, 0.1298, 0.4452,\n",
      "          0.3856]],\n",
      "\n",
      "        [[0.8098, 0.5444, 0.6466, 0.8142, 0.8650, 0.6413, 0.8778, 0.6380,\n",
      "          0.6807, 0.6335, 0.7862, 0.7773, 0.5514, 0.6633, 0.5761, 0.3639,\n",
      "          0.7732, 0.5888, 0.7943, 0.8626, 0.6794, 0.5676, 0.7023, 0.5678,\n",
      "          0.6997, 0.7823, 0.4942, 0.7487, 0.7414, 0.5321, 0.8667, 0.5527,\n",
      "          0.6085],\n",
      "         [0.1902, 0.4556, 0.3534, 0.1858, 0.1350, 0.3587, 0.1222, 0.3620,\n",
      "          0.3193, 0.3665, 0.2138, 0.2227, 0.4486, 0.3367, 0.4239, 0.6361,\n",
      "          0.2268, 0.4112, 0.2057, 0.1374, 0.3206, 0.4324, 0.2977, 0.4322,\n",
      "          0.3003, 0.2177, 0.5058, 0.2513, 0.2586, 0.4679, 0.1333, 0.4473,\n",
      "          0.3915]],\n",
      "\n",
      "        [[0.8120, 0.5181, 0.6073, 0.8013, 0.8778, 0.6725, 0.8664, 0.6451,\n",
      "          0.6818, 0.6301, 0.8318, 0.7937, 0.5420, 0.6861, 0.6276, 0.4053,\n",
      "          0.8256, 0.6577, 0.8008, 0.8608, 0.6751, 0.5724, 0.7050, 0.5951,\n",
      "          0.6883, 0.7918, 0.5242, 0.7306, 0.7844, 0.5641, 0.8709, 0.5547,\n",
      "          0.6405],\n",
      "         [0.1880, 0.4819, 0.3927, 0.1987, 0.1222, 0.3275, 0.1336, 0.3549,\n",
      "          0.3182, 0.3699, 0.1682, 0.2063, 0.4580, 0.3139, 0.3724, 0.5947,\n",
      "          0.1744, 0.3423, 0.1992, 0.1392, 0.3249, 0.4276, 0.2950, 0.4049,\n",
      "          0.3117, 0.2082, 0.4758, 0.2694, 0.2156, 0.4359, 0.1291, 0.4453,\n",
      "          0.3595]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 1, 1, 2]), tensor([15, 26, 15, 26, 15]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33])]\n",
      "new_phon_tokes=[15, 26]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33])]\n",
      "new_phon_tokes=[15, 26]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33])]\n",
      "new_phon_tokes=[15]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), [15, 26]], [tensor([31]), tensor([33]), [15, 26]], [tensor([31]), tensor([33]), [15]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(chkpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m datum \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mcharacter_tokenizer\u001b[38;5;241m.\u001b[39mencode([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjingleheimer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo2p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_pad_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_pad_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/src/model.py:684\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, pathway, orth_enc_input, orth_enc_pad_mask, phon_enc_input, phon_enc_pad_mask, deterministic)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_phon_tokens\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    683\u001b[0m generated_phon_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_phon_tokens(generated_phon_tokens)\n\u001b[0;32m--> 684\u001b[0m generated_phon_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphonology_decoder_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated_phon_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated_phon_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m generated_phon_probs \u001b[38;5;241m=\u001b[39m generated_phon_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphon_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_phon_probs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/src/model.py:619\u001b[0m, in \u001b[0;36mModel.phonology_decoder_loop\u001b[0;34m(self, mask, generated_phon_embeddings, generated_phon_tokens, prompt_encoding, deterministic)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_phon_tokens\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# generated_phon_tokens[0].append(new_phonology_tokens)\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m         generated_phon_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_phon_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgenerated_phon_tokens\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphon_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m: phon_probs,\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphon_vecs\u001b[39m\u001b[38;5;124m\"\u001b[39m: phon_vecs,\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphon_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: generated_phon_tokens[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    627\u001b[0m }\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/src/model.py:227\u001b[0m, in \u001b[0;36mModel.embed_phon_tokens\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_num, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m indx, tokes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Here tokens should be a pytorch tensor of integers.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# It extracts the indicated rows from self.phonology_embedding\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m         avg_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphonology_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m# Insert the resulting averaged embedding vector into the\u001b[39;00m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;66;03m# output_embedding tensor as a new row\u001b[39;00m\n\u001b[1;32m    230\u001b[0m         output_embedding[batch_num, indx, :] \u001b[38;5;241m=\u001b[39m avg_embedding\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "\n",
    "chkpt = pt.load(checkpoints[-1])\n",
    "model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "\n",
    "datum = ds.character_tokenizer.encode([\"hello\", \"a\", \"jingleheimer\"])\n",
    "pred = model.generate(\n",
    "    \"o2p\",\n",
    "    datum[\"enc_input_ids\"],\n",
    "    datum[\"enc_pad_mask\"],\n",
    "    datum[\"enc_input_ids\"],\n",
    "    datum[\"enc_pad_mask\"],\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"{pred=}\")\n",
    "for k, v in pred.items():\n",
    "    print(f\"{k=}, {v=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4743, 0.6130, 0.4738, 0.3406],\n",
       "         [0.5257, 0.3870, 0.5262, 0.6594]],\n",
       "\n",
       "        [[0.3976, 0.7930, 0.6806, 0.3008],\n",
       "         [0.6024, 0.2070, 0.3194, 0.6992]],\n",
       "\n",
       "        [[0.1859, 0.0626, 0.2934, 0.6691],\n",
       "         [0.8141, 0.9374, 0.7066, 0.3309]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pt.softmax(pt.randn((3,2,4)),1); t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([31])], [tensor([31])]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[pt.tensor([31])]]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5257, 0.3870, 0.5262, 0.6594])\n",
      "tensor([0.6024, 0.2070, 0.3194, 0.6992])\n",
      "tensor([0.8141, 0.9374, 0.7066, 0.3309])\n"
     ]
    }
   ],
   "source": [
    "for t_i in t:\n",
    "    print(t_i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5280, 0.0858, 0.4528, 0.4649],\n",
       "        [0.3310, 0.6017, 0.5014, 0.5309],\n",
       "        [0.4542, 0.5266, 0.6082, 0.2605]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 1],\n",
       "        [1, 0, 0, 1],\n",
       "        [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = (t[:,1,:] > 0.5).long(); fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = pt.bernoulli(t[:,1,:]).long(); fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 1, 2, 2, 2]), tensor([0, 3, 0, 3, 0, 1, 2]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = pt.where(fp); indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 3\n",
      "1 0\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(*indices):\n",
    "    print(x.item(), y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred[\"phon_probs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for op, p in zip(old_pred, pred):\n",
    "    if op == p:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pred = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take all orthographic words from both WJ3 and the\n",
    "# original dataset and remove duplicates.\n",
    "all_words = set(wj3_json.keys())\n",
    "all_words.update(ds.words)  # Take the union of both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"wj3_predictions.xlsx\", engine=\"openpyxl\")\n",
    "for checkpoint in tqdm.tqdm(checkpoints):\n",
    "    # Empty the dataframe to prepare for the next checkpoint (aka excel page)\n",
    "    df.drop(df.index, inplace=True)\n",
    "    chkpt = pt.load(checkpoint)\n",
    "    model = chkpt[\"model\"]\n",
    "    # In this loop, orth is the orthographic form of the target word and\n",
    "    # phon is the phonological form of the target word (not phonetic vectors)\n",
    "    for orth in all_words:\n",
    "        new_row = {}\n",
    "        # Since all_words is a union of both WJ3 and the original dataset,\n",
    "        # the phonological form of any orth in all_words is in either WJ3 or\n",
    "        # the original cmudict dataset. We check both below\n",
    "        in_wj3 = orth in wj3_json.keys()\n",
    "        new_row[\"in_wj3\"] = in_wj3\n",
    "        in_traindata = orth in ds.words\n",
    "        new_row[\"in_traindata\"] = in_traindata\n",
    "        if in_traindata:\n",
    "            phon = ds.cmudict[orth]\n",
    "        else:\n",
    "            phon = wj3_json[orth]\n",
    "\n",
    "        datum = ds.character_tokenizer.encode(orth)\n",
    "        pred = chkpt[\"model\"].generate(\n",
    "            \"o2p\",\n",
    "            datum[\"enc_input_ids\"],\n",
    "            datum[\"enc_pad_mask\"],\n",
    "            datum[\"enc_input_ids\"],\n",
    "            datum[\"enc_pad_mask\"],\n",
    "            deterministic=True,\n",
    "        )\n",
    "        # Save the original input orthography\n",
    "        new_row[\"word_raw\"] = orth\n",
    "        # Save the target phonology for the above input orthography\n",
    "        new_row[\"phon_target\"] = \":\".join(phon)\n",
    "        # Remove the start and end tokens from each phonological vector\n",
    "        # and convert them from tensors to lists\n",
    "        phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][1:-1]]\n",
    "        # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "        phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "            phon_pred_features, phonology=True, hot_nodes=True\n",
    "        )\n",
    "        # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "        # separated by colons\n",
    "        phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "        new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "        # Save a boolean indicating whether the prediction was correct\n",
    "        new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "        # Save the phonological features for the target phonology\n",
    "        phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "        if phon_target_features:\n",
    "            phon_target_features = \";\".join(\n",
    "                [\n",
    "                    \":\".join([str(v.item()) for v in vector])\n",
    "                    for vector in phon_target_features[\"targets\"][0]\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            phon_target_features = \"None\"\n",
    "        new_row[\"phon_target_features\"] = phon_target_features\n",
    "        # Save the phonological features for the predicted phonology\n",
    "        phon_prediction_features = \";\".join(\n",
    "            [\n",
    "                \":\".join([str(int(v.item())) for v in vector])\n",
    "                for vector in pred[\"phon_vecs\"][1:-1]\n",
    "            ]\n",
    "        )\n",
    "        new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "        # Save the phonological probabilities for the predicted phonology\n",
    "        phon_prediction_probabilities = \";\".join(\n",
    "            [\n",
    "                \":\".join([str(v.item()) for v in vector])\n",
    "                for vector in pred[\"phon_probs\"][1:-1]\n",
    "            ]\n",
    "        )\n",
    "        new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "        # Save the global encoding vector for the predicted phonology\n",
    "        global_encoding = \":\".join(\n",
    "            [str(v.item()) for v in pred[\"global_encoding\"].squeeze()]\n",
    "        )\n",
    "        new_row[\"global_encoding\"] = global_encoding\n",
    "        new_row = pd.Series(new_row)\n",
    "        new_row = new_row.to_frame().transpose()\n",
    "        df = pd.concat([df, new_row])\n",
    "    df.to_excel(writer, sheet_name=\"epoch \" + checkpoint[-7:-4])\n",
    "writer.book.save(\"wj3_predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = {\"a\": 1, \"b\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 2}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = {\"a\": 3, \"b\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 3, 'b': 4}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "\n",
    "model = pt.load(\"models/root_2024-02-17_21h49m56470ms_chkpt001.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_path', 'config', 'model', 'optimizer', 'model_id'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'project': 'nathan',\n",
       " 'num_epochs': 2,\n",
       " 'batch_size_train': 8,\n",
       " 'batch_size_val': 8,\n",
       " 'num_layers': 2,\n",
       " 'learning_rate': 0.001,\n",
       " 'continue_training': False,\n",
       " 'model_chkpt': '',\n",
       " 'd_model': 16,\n",
       " 'nhead': 2,\n",
       " 'wandb': False,\n",
       " 'test': True,\n",
       " 'max_nb_steps': -1,\n",
       " 'train_test_split': 0.9,\n",
       " 'which_dataset': 100,\n",
       " 'sweep': '',\n",
       " 'd_embedding': 2,\n",
       " 'seed': 1337,\n",
       " 'nb_samples': 1000,\n",
       " 'model_path': './models',\n",
       " 'pathway': 'op2op',\n",
       " 'save_every': 2,\n",
       " 'model_id': 'root_2024-02-17_21h49m56470ms',\n",
       " 'chkpt_file_exists': False,\n",
       " 'epochs_completed': 0,\n",
       " 'model_file_name': 'root_2024-02-17_21h49m56470ms_chkpt000.pth',\n",
       " 'n_steps_per_epoch': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([\"a\", \"b\"]) == set([\"b\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     wj3_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m/workspaces/ConnTextUL/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "filename = None\n",
    "with open(filename, \"r\") as f:\n",
    "    wj3_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"/workspaces/ConnTextUL/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/ConnTextUL')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[] for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [], []]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
