{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/nathan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache folder: /Users/nathan/Dropbox/code/research/fsu_haskins/ConnTextUL/data/.cache already exists\n",
      "shark's removed from pool because it is missing in cmudict\n",
      "honeyed removed from pool because it is missing in cmudict\n",
      "zookeeper removed from pool because it is missing in cmudict\n",
      "statue's removed from pool because it is missing in cmudict\n",
      "tia's removed from pool because it is missing in cmudict\n",
      "we’re removed from pool because it is missing in cmudict\n",
      "wiggled removed from pool because it is missing in cmudict\n",
      "yipped removed from pool because it is missing in cmudict\n",
      "u.s.a removed from pool because it is missing in cmudict\n",
      "moosling's removed from pool because it is missing in cmudict\n",
      "gumbles removed from pool because it is missing in cmudict\n",
      "snuffling removed from pool because it is missing in cmudict\n",
      "oink removed from pool because it is missing in cmudict\n",
      "quacked removed from pool because it is missing in cmudict\n",
      "quacking removed from pool because it is missing in cmudict\n",
      "“we removed from pool because it is missing in cmudict\n",
      "dilbert's removed from pool because it is missing in cmudict\n",
      "baa removed from pool because it is missing in cmudict\n",
      "“toss removed from pool because it is missing in cmudict\n",
      "grown-up removed from pool because it is missing in cmudict\n",
      "o.k removed from pool because it is missing in cmudict\n",
      "hic removed from pool because it is missing in cmudict\n",
      "she’s removed from pool because it is missing in cmudict\n",
      "yaar removed from pool because it is missing in cmudict\n",
      "pecked removed from pool because it is missing in cmudict\n",
      "oarlocks removed from pool because it is missing in cmudict\n",
      "uncurls removed from pool because it is missing in cmudict\n",
      "singer,” removed from pool because it is missing in cmudict\n",
      "โcold removed from pool because it is missing in cmudict\n",
      "i’m removed from pool because it is missing in cmudict\n",
      "folktales removed from pool because it is missing in cmudict\n",
      "birkin removed from pool because it is missing in cmudict\n",
      "rath's removed from pool because it is missing in cmudict\n",
      "“he removed from pool because it is missing in cmudict\n",
      "ok,” removed from pool because it is missing in cmudict\n",
      "roxy's removed from pool because it is missing in cmudict\n",
      "thing.” removed from pool because it is missing in cmudict\n",
      "top-grade removed from pool because it is missing in cmudict\n",
      "spit' removed from pool because it is missing in cmudict\n",
      "sim’s removed from pool because it is missing in cmudict\n",
      "deena removed from pool because it is missing in cmudict\n",
      "knapsacks removed from pool because it is missing in cmudict\n",
      "flowerbed removed from pool because it is missing in cmudict\n",
      "laundromutt removed from pool because it is missing in cmudict\n",
      "hippo's removed from pool because it is missing in cmudict\n",
      "pups' removed from pool because it is missing in cmudict\n",
      "head-on removed from pool because it is missing in cmudict\n",
      "a.m removed from pool because it is missing in cmudict\n",
      "diz removed from pool because it is missing in cmudict\n",
      "throbbed removed from pool because it is missing in cmudict\n",
      "mick's removed from pool because it is missing in cmudict\n",
      "“would removed from pool because it is missing in cmudict\n",
      "pum removed from pool because it is missing in cmudict\n",
      "papa's removed from pool because it is missing in cmudict\n",
      "cal's removed from pool because it is missing in cmudict\n",
      "squished removed from pool because it is missing in cmudict\n",
      "sonoran removed from pool because it is missing in cmudict\n",
      "gerbil removed from pool because it is missing in cmudict\n",
      "crabgrass removed from pool because it is missing in cmudict\n",
      "beanbags removed from pool because it is missing in cmudict\n",
      "cactuses removed from pool because it is missing in cmudict\n",
      "lizzy's removed from pool because it is missing in cmudict\n",
      "rabbit's removed from pool because it is missing in cmudict\n",
      "aleck removed from pool because it is missing in cmudict\n",
      "troves removed from pool because it is missing in cmudict\n",
      "starfish's removed from pool because it is missing in cmudict\n",
      "sunburns removed from pool because it is missing in cmudict\n",
      "kaylin removed from pool because it is missing in cmudict\n",
      "sunblock removed from pool because it is missing in cmudict\n",
      "said,\"mom removed from pool because it is missing in cmudict\n",
      "willow's removed from pool because it is missing in cmudict\n",
      "“i removed from pool because it is missing in cmudict\n",
      "lamp,” removed from pool because it is missing in cmudict\n",
      "naple removed from pool because it is missing in cmudict\n",
      "giraffe's removed from pool because it is missing in cmudict\n",
      "bartholdi removed from pool because it is missing in cmudict\n",
      "stained-glass removed from pool because it is missing in cmudict\n",
      "maisy's removed from pool because it is missing in cmudict\n",
      "fγç¥ removed from pool because it is missing in cmudict\n",
      "pickax removed from pool because it is missing in cmudict\n",
      "“buddy removed from pool because it is missing in cmudict\n",
      "chimp's removed from pool because it is missing in cmudict\n",
      "granddad's removed from pool because it is missing in cmudict\n",
      "maisy removed from pool because it is missing in cmudict\n",
      "wish.” removed from pool because it is missing in cmudict\n",
      "orca's removed from pool because it is missing in cmudict\n",
      "until...the removed from pool because it is missing in cmudict\n",
      "“well removed from pool because it is missing in cmudict\n",
      "meli's removed from pool because it is missing in cmudict\n",
      "flicked removed from pool because it is missing in cmudict\n",
      "full-grown removed from pool because it is missing in cmudict\n",
      "teakettle removed from pool because it is missing in cmudict\n",
      "wets removed from pool because it is missing in cmudict\n",
      "hatchling removed from pool because it is missing in cmudict\n",
      "clothesline removed from pool because it is missing in cmudict\n",
      "hatchlings' removed from pool because it is missing in cmudict\n",
      "dog—a removed from pool because it is missing in cmudict\n",
      "ty's removed from pool because it is missing in cmudict\n",
      "granddad removed from pool because it is missing in cmudict\n",
      "here!” removed from pool because it is missing in cmudict\n",
      "exclaimed.\"it removed from pool because it is missing in cmudict\n",
      "vald removed from pool because it is missing in cmudict\n",
      "discoverers removed from pool because it is missing in cmudict\n",
      "gumble removed from pool because it is missing in cmudict\n",
      "paler removed from pool because it is missing in cmudict\n",
      "keb removed from pool because it is missing in cmudict\n",
      "acoma removed from pool because it is missing in cmudict\n",
      "tutankhamen removed from pool because it is missing in cmudict\n",
      "titanic's removed from pool because it is missing in cmudict\n",
      "flowerbeds removed from pool because it is missing in cmudict\n",
      "loves's removed from pool because it is missing in cmudict\n",
      "tiptoes removed from pool because it is missing in cmudict\n",
      "eek removed from pool because it is missing in cmudict\n",
      "beth's removed from pool because it is missing in cmudict\n",
      "whuff removed from pool because it is missing in cmudict\n",
      "dolphin's removed from pool because it is missing in cmudict\n",
      "whippet removed from pool because it is missing in cmudict\n",
      "minos removed from pool because it is missing in cmudict\n",
      "keee removed from pool because it is missing in cmudict\n",
      "lola's removed from pool because it is missing in cmudict\n",
      "moe's removed from pool because it is missing in cmudict\n",
      "carnarvon removed from pool because it is missing in cmudict\n",
      "hopeless,” removed from pool because it is missing in cmudict\n",
      "wriggled removed from pool because it is missing in cmudict\n",
      "henny removed from pool because it is missing in cmudict\n",
      "midas's removed from pool because it is missing in cmudict\n",
      "hermit's removed from pool because it is missing in cmudict\n",
      "daedalus's removed from pool because it is missing in cmudict\n",
      "hawk's removed from pool because it is missing in cmudict\n",
      "moosling removed from pool because it is missing in cmudict\n",
      "sharks' removed from pool because it is missing in cmudict\n",
      "it’s removed from pool because it is missing in cmudict\n",
      "quin's removed from pool because it is missing in cmudict\n",
      "pet-watcher removed from pool because it is missing in cmudict\n",
      "kiddo removed from pool because it is missing in cmudict\n",
      "splish removed from pool because it is missing in cmudict\n",
      "dil removed from pool because it is missing in cmudict\n",
      "vet's removed from pool because it is missing in cmudict\n",
      "orson's removed from pool because it is missing in cmudict\n",
      "mooed removed from pool because it is missing in cmudict\n",
      "silenus removed from pool because it is missing in cmudict\n",
      "krysta removed from pool because it is missing in cmudict\n",
      "clasps removed from pool because it is missing in cmudict\n",
      "forelegs removed from pool because it is missing in cmudict\n",
      "waddled removed from pool because it is missing in cmudict\n",
      "we’ve removed from pool because it is missing in cmudict\n",
      "periwinkles removed from pool because it is missing in cmudict\n",
      "cud removed from pool because it is missing in cmudict\n",
      "owl's removed from pool because it is missing in cmudict\n",
      "ned!γç¥ removed from pool because it is missing in cmudict\n",
      "podcast removed from pool because it is missing in cmudict\n",
      "coe's removed from pool because it is missing in cmudict\n",
      "clare's removed from pool because it is missing in cmudict\n",
      "blub removed from pool because it is missing in cmudict\n",
      "loxy removed from pool because it is missing in cmudict\n",
      "cub's removed from pool because it is missing in cmudict\n",
      "shhh removed from pool because it is missing in cmudict\n",
      "pablo's removed from pool because it is missing in cmudict\n",
      "handler's removed from pool because it is missing in cmudict\n",
      "krysta's removed from pool because it is missing in cmudict\n",
      "don’t removed from pool because it is missing in cmudict\n",
      "bottlenose removed from pool because it is missing in cmudict\n",
      "ducky removed from pool because it is missing in cmudict\n",
      "lunchbox removed from pool because it is missing in cmudict\n",
      "sim's removed from pool because it is missing in cmudict\n",
      "kit's removed from pool because it is missing in cmudict\n",
      "' removed from pool because it is missing in cmudict\n",
      "mmm removed from pool because it is missing in cmudict\n",
      "gramps removed from pool because it is missing in cmudict\n",
      "farmer’s removed from pool because it is missing in cmudict\n",
      "laboulaye removed from pool because it is missing in cmudict\n",
      "โhotโ removed from pool because it is missing in cmudict\n",
      "'snake removed from pool because it is missing in cmudict\n",
      "minotaur removed from pool because it is missing in cmudict\n",
      "kep removed from pool because it is missing in cmudict\n",
      "laboulaye's removed from pool because it is missing in cmudict\n",
      "boxlike removed from pool because it is missing in cmudict\n",
      "pactolus removed from pool because it is missing in cmudict\n",
      "footstools removed from pool because it is missing in cmudict\n",
      "theseus removed from pool because it is missing in cmudict\n",
      "fireboat removed from pool because it is missing in cmudict\n",
      "that’s removed from pool because it is missing in cmudict\n",
      "mora's removed from pool because it is missing in cmudict\n",
      "beaks removed from pool because it is missing in cmudict\n",
      "kaylin's removed from pool because it is missing in cmudict\n",
      "dormice removed from pool because it is missing in cmudict\n",
      "trolls removed from pool because it is missing in cmudict\n",
      "goblets removed from pool because it is missing in cmudict\n",
      "mousing removed from pool because it is missing in cmudict\n",
      "prout's removed from pool because it is missing in cmudict\n",
      "katlin removed from pool because it is missing in cmudict\n",
      "grandmas' removed from pool because it is missing in cmudict\n",
      "podcasts removed from pool because it is missing in cmudict\n",
      "squish removed from pool because it is missing in cmudict\n",
      "dog’s removed from pool because it is missing in cmudict\n",
      "ferret's removed from pool because it is missing in cmudict\n",
      "lumbered removed from pool because it is missing in cmudict\n",
      "beaver's removed from pool because it is missing in cmudict\n",
      "i’ll removed from pool because it is missing in cmudict\n",
      "gobbler's removed from pool because it is missing in cmudict\n",
      "e-mail removed from pool because it is missing in cmudict\n",
      "tt removed from pool because it is missing in cmudict\n",
      "plonk removed from pool because it is missing in cmudict\n",
      "e\"s removed from pool because it is missing in cmudict\n",
      "zaf removed from pool because it is missing in cmudict\n",
      "mavbe removed from pool because it is missing in cmudict\n",
      "pet’s removed from pool because it is missing in cmudict\n",
      "gumbles' removed from pool because it is missing in cmudict\n",
      "nell's removed from pool because it is missing in cmudict\n",
      "fun,γç¥ removed from pool because it is missing in cmudict\n",
      "lin's removed from pool because it is missing in cmudict\n",
      "iliamna removed from pool because it is missing in cmudict\n",
      "scatters removed from pool because it is missing in cmudict\n",
      "chipmunk removed from pool because it is missing in cmudict\n",
      "shallows removed from pool because it is missing in cmudict\n",
      "magnet.' removed from pool because it is missing in cmudict\n",
      "dil's removed from pool because it is missing in cmudict\n",
      "nelly's removed from pool because it is missing in cmudict\n",
      "glub removed from pool because it is missing in cmudict\n",
      "hairlike removed from pool because it is missing in cmudict\n",
      "wishes,” removed from pool because it is missing in cmudict\n",
      "kenny's removed from pool because it is missing in cmudict\n",
      "let’s removed from pool because it is missing in cmudict\n",
      "deserts,โ removed from pool because it is missing in cmudict\n",
      "bandits' removed from pool because it is missing in cmudict\n",
      "daedulus removed from pool because it is missing in cmudict\n",
      "doeskin removed from pool because it is missing in cmudict\n",
      "itched removed from pool because it is missing in cmudict\n",
      "โ removed from pool because it is missing in cmudict\n",
      "beeswax removed from pool because it is missing in cmudict\n",
      "neigh removed from pool because it is missing in cmudict\n",
      "lorises removed from pool because it is missing in cmudict\n",
      "humpback's removed from pool because it is missing in cmudict\n",
      "waaaaa removed from pool because it is missing in cmudict\n",
      "froggy's removed from pool because it is missing in cmudict\n",
      "peach's removed from pool because it is missing in cmudict\n",
      "hatchlings removed from pool because it is missing in cmudict\n",
      "nat's removed from pool because it is missing in cmudict\n",
      "mabel's removed from pool because it is missing in cmudict\n",
      "bartholdi's removed from pool because it is missing in cmudict\n",
      "cod,\"said removed from pool because it is missing in cmudict\n",
      "meli’s removed from pool because it is missing in cmudict\n",
      "dionysus removed from pool because it is missing in cmudict\n",
      "froggy removed from pool because it is missing in cmudict\n",
      "place,γç¥ removed from pool because it is missing in cmudict\n",
      "loosey removed from pool because it is missing in cmudict\n",
      "toasty removed from pool because it is missing in cmudict\n",
      "kittens' removed from pool because it is missing in cmudict\n",
      "— removed from pool because it is missing in cmudict\n",
      "puckered removed from pool because it is missing in cmudict\n",
      "tooted removed from pool because it is missing in cmudict\n",
      "fang's removed from pool because it is missing in cmudict\n",
      "kat's removed from pool because it is missing in cmudict\n",
      "won't removed because punctuation is present\n",
      "jessica's removed because punctuation is present\n",
      "wasn't removed because punctuation is present\n",
      "it's removed because punctuation is present\n",
      "player's removed because punctuation is present\n",
      "u.s removed because punctuation is present\n",
      "principal's removed because punctuation is present\n",
      "we've removed because punctuation is present\n",
      "liberty's removed because punctuation is present\n",
      "they'll removed because punctuation is present\n",
      "blake's removed because punctuation is present\n",
      "sun's removed because punctuation is present\n",
      "ray's removed because punctuation is present\n",
      "winner's removed because punctuation is present\n",
      "years' removed because punctuation is present\n",
      "haven't removed because punctuation is present\n",
      "i'm removed because punctuation is present\n",
      "graham's removed because punctuation is present\n",
      "snake's removed because punctuation is present\n",
      "moody's removed because punctuation is present\n",
      "you'll removed because punctuation is present\n",
      "jesse's removed because punctuation is present\n",
      "potter's removed because punctuation is present\n",
      "coach's removed because punctuation is present\n",
      "otter's removed because punctuation is present\n",
      "sam's removed because punctuation is present\n",
      "butterfly's removed because punctuation is present\n",
      "dan's removed because punctuation is present\n",
      "doesn't removed because punctuation is present\n",
      "kate's removed because punctuation is present\n",
      "chad's removed because punctuation is present\n",
      "bell's removed because punctuation is present\n",
      "o'clock removed because punctuation is present\n",
      "mother's removed because punctuation is present\n",
      "twain's removed because punctuation is present\n",
      "there's removed because punctuation is present\n",
      "we'll removed because punctuation is present\n",
      "penny's removed because punctuation is present\n",
      "that's removed because punctuation is present\n",
      "hector's removed because punctuation is present\n",
      "jim's removed because punctuation is present\n",
      "country's removed because punctuation is present\n",
      "jupiter's removed because punctuation is present\n",
      "hank's removed because punctuation is present\n",
      "wouldn't removed because punctuation is present\n",
      "paul's removed because punctuation is present\n",
      "jon's removed because punctuation is present\n",
      "america's removed because punctuation is present\n",
      "sister's removed because punctuation is present\n",
      "ben's removed because punctuation is present\n",
      "he'd removed because punctuation is present\n",
      "child's removed because punctuation is present\n",
      "chuck's removed because punctuation is present\n",
      "didn't removed because punctuation is present\n",
      "aren't removed because punctuation is present\n",
      "she's removed because punctuation is present\n",
      "father's removed because punctuation is present\n",
      "howe's removed because punctuation is present\n",
      "man's removed because punctuation is present\n",
      "nick's removed because punctuation is present\n",
      "hunter's removed because punctuation is present\n",
      "couldn't removed because punctuation is present\n",
      "amanda's removed because punctuation is present\n",
      "mom's removed because punctuation is present\n",
      "master's removed because punctuation is present\n",
      "jane's removed because punctuation is present\n",
      "ned's removed because punctuation is present\n",
      "tom's removed because punctuation is present\n",
      "washington's removed because punctuation is present\n",
      "boris's removed because punctuation is present\n",
      "turtle's removed because punctuation is present\n",
      "river's removed because punctuation is present\n",
      "tut's removed because punctuation is present\n",
      "patty's removed because punctuation is present\n",
      "they're removed because punctuation is present\n",
      "fox's removed because punctuation is present\n",
      "bear's removed because punctuation is present\n",
      "ron's removed because punctuation is present\n",
      "worker's removed because punctuation is present\n",
      "i'd removed because punctuation is present\n",
      "dog's removed because punctuation is present\n",
      "alice's removed because punctuation is present\n",
      "person's removed because punctuation is present\n",
      "plum's removed because punctuation is present\n",
      "catcher's removed because punctuation is present\n",
      "let's removed because punctuation is present\n",
      "birds' removed because punctuation is present\n",
      "she'll removed because punctuation is present\n",
      "peck's removed because punctuation is present\n",
      "badger's removed because punctuation is present\n",
      "rob's removed because punctuation is present\n",
      "you've removed because punctuation is present\n",
      "lion's removed because punctuation is present\n",
      "you'd removed because punctuation is present\n",
      "rosa's removed because punctuation is present\n",
      "we're removed because punctuation is present\n",
      "robbie's removed because punctuation is present\n",
      "bee's removed because punctuation is present\n",
      "ship's removed because punctuation is present\n",
      "here's removed because punctuation is present\n",
      "wolf's removed because punctuation is present\n",
      "you're removed because punctuation is present\n",
      "someone's removed because punctuation is present\n",
      "annie's removed because punctuation is present\n",
      "pam's removed because punctuation is present\n",
      "goat's removed because punctuation is present\n",
      "people's removed because punctuation is present\n",
      "grandfather's removed because punctuation is present\n",
      "it'll removed because punctuation is present\n",
      "sheep's removed because punctuation is present\n",
      "jan's removed because punctuation is present\n",
      "crane's removed because punctuation is present\n",
      "luke's removed because punctuation is present\n",
      "whale's removed because punctuation is present\n",
      "brad's removed because punctuation is present\n",
      "i've removed because punctuation is present\n",
      "another's removed because punctuation is present\n",
      "dolphins' removed because punctuation is present\n",
      "dana's removed because punctuation is present\n",
      "jake's removed because punctuation is present\n",
      "world's removed because punctuation is present\n",
      "farmer's removed because punctuation is present\n",
      "gene's removed because punctuation is present\n",
      "carter's removed because punctuation is present\n",
      "duck's removed because punctuation is present\n",
      "brother's removed because punctuation is present\n",
      "other's removed because punctuation is present\n",
      "dean's removed because punctuation is present\n",
      "isn't removed because punctuation is present\n",
      "rose's removed because punctuation is present\n",
      "steel's removed because punctuation is present\n",
      "elephant's removed because punctuation is present\n",
      "can't removed because punctuation is present\n",
      "rudy's removed because punctuation is present\n",
      "what's removed because punctuation is present\n",
      "today's removed because punctuation is present\n",
      "king's removed because punctuation is present\n",
      "morris's removed because punctuation is present\n",
      "don't removed because punctuation is present\n",
      "i'll removed because punctuation is present\n",
      "hill's removed because punctuation is present\n",
      "grandma's removed because punctuation is present\n",
      "fred's removed because punctuation is present\n",
      "hadn't removed because punctuation is present\n",
      "good-bye removed because punctuation is present\n",
      "bird's removed because punctuation is present\n",
      "morgan's removed because punctuation is present\n",
      "sloan's removed because punctuation is present\n",
      "plant's removed because punctuation is present\n",
      "he's removed because punctuation is present\n",
      "hal's removed because punctuation is present\n",
      "they'd removed because punctuation is present\n",
      "fish's removed because punctuation is present\n",
      "queen's removed because punctuation is present\n",
      "x-ray removed because punctuation is present\n",
      "brian's removed because punctuation is present\n",
      "dad's removed because punctuation is present\n",
      "rick's removed because punctuation is present\n",
      "orthpad changed to 0 because onehot encodings were selected for orthography\n",
      "Representations initialized. Done.\n"
     ]
    }
   ],
   "source": [
    "# Load in a dataset to access Matt's Traindata class which\n",
    "# is embedded inside the phonology tokenizer (consider changing this\n",
    "from pathlib import Path\n",
    "\n",
    "config = type(\n",
    "    \"config\",\n",
    "    (object,),\n",
    "    {\"dataset_filename\": Path(\"data/data.csv\")},\n",
    ")\n",
    "ds = ConnTextULDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Woodcock Johnson III Form A dataset for assessment\n",
    "with open(\"data/wj_iii_form_a.json\", \"r\") as f:\n",
    "    wj3_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template dataframe to populate for each checkpoint\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"word_raw\",\n",
    "        \"phon_target\",\n",
    "        \"phon_prediction\",\n",
    "        \"correct\",\n",
    "        \"phon_target_features\",\n",
    "        \"phon_prediction_features\",\n",
    "        \"phon_prediction_probabilities\",\n",
    "        \"global_encoding\",\n",
    "        \"in_wj3\",\n",
    "        \"in_traindata\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints=['models/nathan_2023-10-12_09h52m23512ms_chkpt020.pth', 'models/root_2024-03-04_17h17m55970ms_chkpt002.pth']\n"
     ]
    }
   ],
   "source": [
    "# read in the checkpoint file names. We will load one at a time\n",
    "# and generate predictions of the wj3 assessments one at at ime\n",
    "checkpoints = glob.glob(\"models/*.pth\")\n",
    "checkpoints.sort()\n",
    "print(f\"{checkpoints=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datum['enc_input_ids']=tensor([[ 0, 11, 29, 26, 16, 14, 20, 18, 27, 33, 28, 26, 19, 32,  1],\n",
      "        [ 0, 18, 19, 13, 13, 31, 26,  1,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 0, 18,  1,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4]])\n",
      "--In Generate--\n",
      "orth_enc_input:  torch.Size([3, 15])\n",
      "orth_enc_pad_mask:  torch.Size([3, 15])\n",
      "generated_phon_tokens=[[tensor([31])], [tensor([31])], [tensor([31])]]\n",
      "--In phonology_decoder_loop--\n",
      "generated_phon_embeddings.shape=torch.Size([3, 1, 64])\n",
      "prompt_encoding.shape=torch.Size([3, 1, 64])\n",
      "len(generated_phon_tokens)=3\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 1, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([1, 1])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.8230, 0.6945, 0.6237, 0.8580, 0.7908, 0.7536, 0.6512, 0.6216,\n",
      "          0.7205, 0.5940, 0.8924, 0.7799, 0.7046, 0.6498, 0.7362, 0.5607,\n",
      "          0.8805, 0.7564, 0.8643, 0.7366, 0.6603, 0.5095, 0.6486, 0.6562,\n",
      "          0.6083, 0.6695, 0.8318, 0.7024, 0.8087, 0.5704, 0.6647, 0.6212,\n",
      "          0.6385],\n",
      "         [0.1770, 0.3055, 0.3763, 0.1420, 0.2092, 0.2464, 0.3488, 0.3784,\n",
      "          0.2795, 0.4060, 0.1076, 0.2201, 0.2954, 0.3502, 0.2638, 0.4393,\n",
      "          0.1195, 0.2436, 0.1357, 0.2634, 0.3397, 0.4905, 0.3514, 0.3438,\n",
      "          0.3917, 0.3305, 0.1682, 0.2976, 0.1913, 0.4296, 0.3353, 0.3788,\n",
      "          0.3615]],\n",
      "\n",
      "        [[0.8182, 0.7235, 0.6216, 0.8641, 0.7890, 0.7454, 0.6376, 0.6177,\n",
      "          0.7339, 0.6113, 0.8952, 0.7715, 0.6926, 0.6712, 0.7169, 0.5924,\n",
      "          0.8767, 0.7237, 0.8661, 0.7102, 0.6669, 0.5365, 0.6408, 0.6437,\n",
      "          0.5917, 0.6823, 0.8288, 0.6638, 0.8178, 0.5652, 0.6652, 0.5953,\n",
      "          0.6088],\n",
      "         [0.1818, 0.2765, 0.3784, 0.1359, 0.2110, 0.2546, 0.3624, 0.3823,\n",
      "          0.2661, 0.3887, 0.1048, 0.2285, 0.3074, 0.3288, 0.2831, 0.4076,\n",
      "          0.1233, 0.2763, 0.1339, 0.2898, 0.3331, 0.4635, 0.3592, 0.3563,\n",
      "          0.4083, 0.3177, 0.1712, 0.3362, 0.1822, 0.4348, 0.3348, 0.4047,\n",
      "          0.3912]],\n",
      "\n",
      "        [[0.8073, 0.7312, 0.6077, 0.8605, 0.7733, 0.7297, 0.6732, 0.6228,\n",
      "          0.7184, 0.6084, 0.8836, 0.7694, 0.7078, 0.6366, 0.6549, 0.5396,\n",
      "          0.8693, 0.6923, 0.8618, 0.6965, 0.6739, 0.4915, 0.6546, 0.6515,\n",
      "          0.6038, 0.6894, 0.8300, 0.6620, 0.7857, 0.5528, 0.6486, 0.5914,\n",
      "          0.5965],\n",
      "         [0.1927, 0.2688, 0.3923, 0.1395, 0.2267, 0.2703, 0.3268, 0.3772,\n",
      "          0.2816, 0.3916, 0.1164, 0.2306, 0.2922, 0.3634, 0.3451, 0.4604,\n",
      "          0.1307, 0.3077, 0.1382, 0.3035, 0.3261, 0.5085, 0.3454, 0.3485,\n",
      "          0.3962, 0.3106, 0.1700, 0.3380, 0.2143, 0.4472, 0.3514, 0.4086,\n",
      "          0.4035]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([2]), tensor([21]))\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=tensor([33])\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=tensor([33])\n",
      "gen_phon_tokes=[tensor([31])]\n",
      "new_phon_tokes=[21]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33])], [tensor([31]), tensor([33])], [tensor([31]), tensor([21])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 2, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([2, 2])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.8127, 0.5010, 0.6145, 0.8061, 0.8711, 0.6648, 0.8679, 0.6355,\n",
      "          0.6736, 0.6200, 0.8244, 0.7920, 0.5644, 0.6713, 0.6461, 0.3856,\n",
      "          0.8108, 0.6623, 0.7837, 0.8810, 0.6814, 0.5534, 0.7015, 0.5978,\n",
      "          0.7045, 0.7883, 0.5207, 0.7638, 0.7811, 0.5521, 0.8683, 0.5842,\n",
      "          0.6483],\n",
      "         [0.1873, 0.4990, 0.3855, 0.1939, 0.1289, 0.3352, 0.1321, 0.3645,\n",
      "          0.3264, 0.3800, 0.1756, 0.2080, 0.4356, 0.3287, 0.3539, 0.6144,\n",
      "          0.1892, 0.3377, 0.2163, 0.1190, 0.3186, 0.4466, 0.2985, 0.4022,\n",
      "          0.2955, 0.2117, 0.4793, 0.2362, 0.2189, 0.4479, 0.1317, 0.4158,\n",
      "          0.3517]],\n",
      "\n",
      "        [[0.8126, 0.5402, 0.6127, 0.8177, 0.8732, 0.6552, 0.8626, 0.6294,\n",
      "          0.6892, 0.6372, 0.8339, 0.7863, 0.5491, 0.6959, 0.6263, 0.4140,\n",
      "          0.8044, 0.6214, 0.7915, 0.8689, 0.6889, 0.5815, 0.6949, 0.5849,\n",
      "          0.6920, 0.8020, 0.5090, 0.7320, 0.7927, 0.5369, 0.8708, 0.5554,\n",
      "          0.6193],\n",
      "         [0.1874, 0.4598, 0.3873, 0.1823, 0.1268, 0.3448, 0.1374, 0.3706,\n",
      "          0.3108, 0.3628, 0.1661, 0.2137, 0.4509, 0.3041, 0.3737, 0.5860,\n",
      "          0.1956, 0.3786, 0.2085, 0.1311, 0.3111, 0.4185, 0.3051, 0.4151,\n",
      "          0.3080, 0.1980, 0.4910, 0.2680, 0.2073, 0.4631, 0.1292, 0.4446,\n",
      "          0.3807]],\n",
      "\n",
      "        [[0.7885, 0.6847, 0.5689, 0.8750, 0.8125, 0.7116, 0.8229, 0.5257,\n",
      "          0.5263, 0.7118, 0.8438, 0.7705, 0.5393, 0.5356, 0.6794, 0.4713,\n",
      "          0.8127, 0.7777, 0.7791, 0.8465, 0.6401, 0.6881, 0.5799, 0.5591,\n",
      "          0.5988, 0.8951, 0.5143, 0.7174, 0.7350, 0.6512, 0.7906, 0.6185,\n",
      "          0.7355],\n",
      "         [0.2115, 0.3153, 0.4311, 0.1250, 0.1875, 0.2884, 0.1771, 0.4743,\n",
      "          0.4737, 0.2882, 0.1562, 0.2295, 0.4607, 0.4644, 0.3206, 0.5287,\n",
      "          0.1873, 0.2223, 0.2209, 0.1535, 0.3599, 0.3119, 0.4201, 0.4409,\n",
      "          0.4012, 0.1049, 0.4857, 0.2826, 0.2650, 0.3488, 0.2094, 0.3815,\n",
      "          0.2645]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 1, 2]), tensor([15, 15, 15]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33])]\n",
      "new_phon_tokes=[15]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33])]\n",
      "new_phon_tokes=[15]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21])]\n",
      "new_phon_tokes=[15]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15])], [tensor([31]), tensor([33]), tensor([15])], [tensor([31]), tensor([21]), tensor([15])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 3, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([3, 3])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7704, 0.6075, 0.6365, 0.7976, 0.5560, 0.5641, 0.8295, 0.7313,\n",
      "          0.8734, 0.8124, 0.7937, 0.8914, 0.6120, 0.7025, 0.7755, 0.6563,\n",
      "          0.8832, 0.7509, 0.8624, 0.7410, 0.7288, 0.8187, 0.5263, 0.8387,\n",
      "          0.7485, 0.4848, 0.8025, 0.6939, 0.5480, 0.8413, 0.7734, 0.8286,\n",
      "          0.6841],\n",
      "         [0.2296, 0.3925, 0.3635, 0.2024, 0.4440, 0.4359, 0.1705, 0.2687,\n",
      "          0.1266, 0.1876, 0.2063, 0.1086, 0.3880, 0.2975, 0.2245, 0.3437,\n",
      "          0.1168, 0.2491, 0.1376, 0.2590, 0.2712, 0.1813, 0.4737, 0.1613,\n",
      "          0.2515, 0.5152, 0.1975, 0.3061, 0.4520, 0.1587, 0.2266, 0.1714,\n",
      "          0.3159]],\n",
      "\n",
      "        [[0.7655, 0.6506, 0.6336, 0.8054, 0.5526, 0.5516, 0.8232, 0.7237,\n",
      "          0.8818, 0.8215, 0.7996, 0.8858, 0.6014, 0.7238, 0.7614, 0.6865,\n",
      "          0.8782, 0.7163, 0.8660, 0.7191, 0.7352, 0.8368, 0.5218, 0.8315,\n",
      "          0.7370, 0.4971, 0.7953, 0.6547, 0.5616, 0.8349, 0.7726, 0.8139,\n",
      "          0.6580],\n",
      "         [0.2345, 0.3494, 0.3664, 0.1946, 0.4474, 0.4484, 0.1768, 0.2763,\n",
      "          0.1182, 0.1785, 0.2004, 0.1142, 0.3986, 0.2762, 0.2386, 0.3135,\n",
      "          0.1218, 0.2837, 0.1340, 0.2809, 0.2648, 0.1632, 0.4782, 0.1685,\n",
      "          0.2630, 0.5029, 0.2047, 0.3453, 0.4384, 0.1651, 0.2274, 0.1861,\n",
      "          0.3420]],\n",
      "\n",
      "        [[0.7458, 0.6548, 0.6168, 0.8005, 0.5331, 0.5239, 0.8494, 0.7277,\n",
      "          0.8743, 0.8242, 0.7848, 0.8851, 0.6149, 0.6918, 0.7011, 0.6377,\n",
      "          0.8713, 0.6836, 0.8630, 0.7064, 0.7430, 0.8116, 0.5342, 0.8355,\n",
      "          0.7490, 0.5031, 0.7951, 0.6496, 0.5212, 0.8340, 0.7615, 0.8123,\n",
      "          0.6481],\n",
      "         [0.2542, 0.3452, 0.3832, 0.1995, 0.4669, 0.4761, 0.1506, 0.2723,\n",
      "          0.1257, 0.1758, 0.2152, 0.1149, 0.3851, 0.3082, 0.2989, 0.3623,\n",
      "          0.1287, 0.3164, 0.1370, 0.2936, 0.2570, 0.1884, 0.4658, 0.1645,\n",
      "          0.2510, 0.4969, 0.2049, 0.3504, 0.4788, 0.1660, 0.2385, 0.1877,\n",
      "          0.3519]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 1]), tensor([25, 25]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15])]\n",
      "new_phon_tokes=[25]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15])]\n",
      "new_phon_tokes=[25]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15])]\n",
      "new_phon_tokes=tensor([33])\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25])], [tensor([31]), tensor([33]), tensor([15]), tensor([25])], [tensor([31]), tensor([21]), tensor([15]), tensor([33])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 4, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([4, 4])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7249, 0.4593, 0.7462, 0.7572, 0.7354, 0.6418, 0.6466, 0.6744,\n",
      "          0.8853, 0.5899, 0.7913, 0.8059, 0.7437, 0.5176, 0.7183, 0.7438,\n",
      "          0.7934, 0.6547, 0.8337, 0.5027, 0.6382, 0.6907, 0.6350, 0.8962,\n",
      "          0.7227, 0.6137, 0.7502, 0.5703, 0.7344, 0.5250, 0.6308, 0.6422,\n",
      "          0.4616],\n",
      "         [0.2751, 0.5407, 0.2538, 0.2428, 0.2646, 0.3582, 0.3534, 0.3256,\n",
      "          0.1147, 0.4101, 0.2087, 0.1941, 0.2563, 0.4824, 0.2817, 0.2562,\n",
      "          0.2066, 0.3453, 0.1663, 0.4973, 0.3618, 0.3093, 0.3650, 0.1038,\n",
      "          0.2773, 0.3863, 0.2498, 0.4297, 0.2656, 0.4750, 0.3692, 0.3578,\n",
      "          0.5384]],\n",
      "\n",
      "        [[0.7189, 0.5064, 0.7482, 0.7669, 0.7287, 0.6326, 0.6306, 0.6675,\n",
      "          0.8926, 0.6069, 0.7938, 0.7946, 0.7337, 0.5459, 0.6976, 0.7715,\n",
      "          0.7809, 0.6046, 0.8365, 0.4671, 0.6411, 0.7204, 0.6257, 0.8921,\n",
      "          0.7047, 0.6285, 0.7405, 0.5205, 0.7429, 0.5104, 0.6295, 0.6157,\n",
      "          0.4250],\n",
      "         [0.2811, 0.4936, 0.2518, 0.2331, 0.2713, 0.3674, 0.3694, 0.3325,\n",
      "          0.1074, 0.3931, 0.2062, 0.2054, 0.2663, 0.4541, 0.3024, 0.2285,\n",
      "          0.2191, 0.3954, 0.1635, 0.5329, 0.3589, 0.2796, 0.3743, 0.1079,\n",
      "          0.2953, 0.3715, 0.2595, 0.4795, 0.2571, 0.4896, 0.3705, 0.3843,\n",
      "          0.5750]],\n",
      "\n",
      "        [[0.7298, 0.5828, 0.5769, 0.7245, 0.7741, 0.5608, 0.7617, 0.6598,\n",
      "          0.9110, 0.4817, 0.7267, 0.7491, 0.7708, 0.6013, 0.6806, 0.5932,\n",
      "          0.8557, 0.6892, 0.8857, 0.8153, 0.6706, 0.5224, 0.7319, 0.7985,\n",
      "          0.7780, 0.5899, 0.7281, 0.8838, 0.5675, 0.5714, 0.7544, 0.4750,\n",
      "          0.6125],\n",
      "         [0.2702, 0.4172, 0.4231, 0.2755, 0.2259, 0.4392, 0.2383, 0.3402,\n",
      "          0.0890, 0.5183, 0.2733, 0.2509, 0.2292, 0.3987, 0.3194, 0.4068,\n",
      "          0.1443, 0.3108, 0.1143, 0.1847, 0.3294, 0.4776, 0.2681, 0.2015,\n",
      "          0.2220, 0.4101, 0.2719, 0.1162, 0.4325, 0.4286, 0.2456, 0.5250,\n",
      "          0.3875]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 1, 1, 2, 2]), tensor([ 1, 32, 19, 32,  9, 31]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25])]\n",
      "new_phon_tokes=[1, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25])]\n",
      "new_phon_tokes=[19, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33])]\n",
      "new_phon_tokes=[9, 31]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 5, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([5, 5])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.5707, 0.3736, 0.7680, 0.6383, 0.6825, 0.6012, 0.6301, 0.7702,\n",
      "          0.7040, 0.4972, 0.7570, 0.7524, 0.8130, 0.5409, 0.7548, 0.6060,\n",
      "          0.9048, 0.8195, 0.6158, 0.5276, 0.5627, 0.3836, 0.7945, 0.7346,\n",
      "          0.6794, 0.4615, 0.9444, 0.7801, 0.6059, 0.7440, 0.8616, 0.7449,\n",
      "          0.7232],\n",
      "         [0.4293, 0.6264, 0.2320, 0.3617, 0.3175, 0.3988, 0.3699, 0.2298,\n",
      "          0.2960, 0.5028, 0.2430, 0.2476, 0.1870, 0.4591, 0.2452, 0.3940,\n",
      "          0.0952, 0.1805, 0.3842, 0.4724, 0.4373, 0.6164, 0.2055, 0.2654,\n",
      "          0.3206, 0.5385, 0.0556, 0.2199, 0.3941, 0.2560, 0.1384, 0.2551,\n",
      "          0.2768]],\n",
      "\n",
      "        [[0.5332, 0.5027, 0.8205, 0.6023, 0.6069, 0.5610, 0.5748, 0.8361,\n",
      "          0.7191, 0.5480, 0.7033, 0.7470, 0.7413, 0.4293, 0.7402, 0.7623,\n",
      "          0.8822, 0.7765, 0.6586, 0.5629, 0.6443, 0.4013, 0.8125, 0.8726,\n",
      "          0.5956, 0.6256, 0.9106, 0.6003, 0.4007, 0.7640, 0.8805, 0.6940,\n",
      "          0.6275],\n",
      "         [0.4668, 0.4973, 0.1795, 0.3977, 0.3931, 0.4390, 0.4252, 0.1639,\n",
      "          0.2809, 0.4520, 0.2967, 0.2530, 0.2587, 0.5707, 0.2598, 0.2377,\n",
      "          0.1178, 0.2235, 0.3414, 0.4371, 0.3557, 0.5987, 0.1875, 0.1274,\n",
      "          0.4044, 0.3744, 0.0894, 0.3997, 0.5993, 0.2360, 0.1195, 0.3060,\n",
      "          0.3725]],\n",
      "\n",
      "        [[0.7406, 0.5870, 0.7376, 0.6627, 0.6726, 0.5701, 0.5981, 0.6952,\n",
      "          0.6929, 0.4613, 0.8104, 0.8138, 0.7885, 0.5000, 0.7000, 0.6075,\n",
      "          0.8783, 0.8673, 0.8999, 0.6437, 0.5072, 0.5201, 0.7770, 0.7649,\n",
      "          0.7621, 0.5056, 0.9137, 0.6736, 0.3716, 0.7334, 0.7616, 0.7104,\n",
      "          0.6300],\n",
      "         [0.2594, 0.4130, 0.2624, 0.3373, 0.3274, 0.4299, 0.4019, 0.3048,\n",
      "          0.3071, 0.5387, 0.1896, 0.1862, 0.2115, 0.5000, 0.3000, 0.3925,\n",
      "          0.1217, 0.1327, 0.1001, 0.3563, 0.4928, 0.4799, 0.2230, 0.2351,\n",
      "          0.2379, 0.4944, 0.0863, 0.3264, 0.6284, 0.2666, 0.2384, 0.2896,\n",
      "          0.3700]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2]), tensor([ 1,  9, 21, 25, 13, 21, 28,  9, 13, 28]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32])]\n",
      "new_phon_tokes=[1, 9, 21, 25]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32])]\n",
      "new_phon_tokes=[13, 21, 28]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31])]\n",
      "new_phon_tokes=[9, 13, 28]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 6, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([6, 6])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.6758, 0.4588, 0.4441, 0.7753, 0.5970, 0.7653, 0.6258, 0.6114,\n",
      "          0.5809, 0.7097, 0.8746, 0.8228, 0.4704, 0.5965, 0.6324, 0.5057,\n",
      "          0.8677, 0.6834, 0.6132, 0.7436, 0.7504, 0.6641, 0.7694, 0.5293,\n",
      "          0.7963, 0.8061, 0.8578, 0.5963, 0.8352, 0.7402, 0.6777, 0.5907,\n",
      "          0.4242],\n",
      "         [0.3242, 0.5412, 0.5559, 0.2247, 0.4030, 0.2347, 0.3742, 0.3886,\n",
      "          0.4191, 0.2903, 0.1254, 0.1772, 0.5296, 0.4035, 0.3676, 0.4943,\n",
      "          0.1323, 0.3166, 0.3868, 0.2564, 0.2496, 0.3359, 0.2306, 0.4707,\n",
      "          0.2037, 0.1939, 0.1422, 0.4037, 0.1648, 0.2598, 0.3223, 0.4093,\n",
      "          0.5758]],\n",
      "\n",
      "        [[0.7053, 0.5702, 0.3125, 0.8226, 0.7245, 0.7351, 0.6091, 0.5401,\n",
      "          0.5627, 0.6328, 0.8467, 0.7490, 0.4910, 0.6280, 0.5768, 0.5881,\n",
      "          0.9069, 0.7212, 0.4798, 0.8211, 0.8268, 0.6150, 0.7794, 0.4868,\n",
      "          0.7987, 0.7185, 0.8030, 0.6684, 0.7862, 0.7436, 0.7770, 0.5467,\n",
      "          0.5191],\n",
      "         [0.2947, 0.4298, 0.6875, 0.1774, 0.2755, 0.2649, 0.3909, 0.4599,\n",
      "          0.4373, 0.3672, 0.1533, 0.2510, 0.5090, 0.3720, 0.4232, 0.4119,\n",
      "          0.0931, 0.2788, 0.5202, 0.1789, 0.1732, 0.3850, 0.2206, 0.5132,\n",
      "          0.2013, 0.2815, 0.1970, 0.3316, 0.2138, 0.2564, 0.2230, 0.4533,\n",
      "          0.4809]],\n",
      "\n",
      "        [[0.6989, 0.6224, 0.3245, 0.7407, 0.6572, 0.7244, 0.6481, 0.6031,\n",
      "          0.6263, 0.6392, 0.8453, 0.8267, 0.5357, 0.5977, 0.5821, 0.5582,\n",
      "          0.8875, 0.6411, 0.5545, 0.7790, 0.7953, 0.7067, 0.7986, 0.5117,\n",
      "          0.8462, 0.6575, 0.8364, 0.6048, 0.7120, 0.7309, 0.6934, 0.6212,\n",
      "          0.5099],\n",
      "         [0.3011, 0.3776, 0.6755, 0.2593, 0.3428, 0.2756, 0.3519, 0.3969,\n",
      "          0.3737, 0.3608, 0.1547, 0.1733, 0.4643, 0.4023, 0.4179, 0.4418,\n",
      "          0.1125, 0.3589, 0.4455, 0.2210, 0.2047, 0.2933, 0.2014, 0.4883,\n",
      "          0.1538, 0.3425, 0.1636, 0.3952, 0.2880, 0.2691, 0.3066, 0.3788,\n",
      "          0.4901]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2]), tensor([ 1,  2, 12, 32,  2, 12, 18, 23,  2]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25])]\n",
      "new_phon_tokes=[1, 2, 12, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28])]\n",
      "new_phon_tokes=[2, 12, 18, 23]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28])]\n",
      "new_phon_tokes=[2]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 7, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([7, 7])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7723, 0.5460, 0.7423, 0.6853, 0.6024, 0.6841, 0.7782, 0.9317,\n",
      "          0.6558, 0.6309, 0.6769, 0.7617, 0.2974, 0.6795, 0.9520, 0.6175,\n",
      "          0.8502, 0.8037, 0.7995, 0.7714, 0.7395, 0.7493, 0.5785, 0.4473,\n",
      "          0.4066, 0.6873, 0.7299, 0.8294, 0.7841, 0.6951, 0.7120, 0.7116,\n",
      "          0.7118],\n",
      "         [0.2277, 0.4540, 0.2577, 0.3147, 0.3976, 0.3159, 0.2218, 0.0683,\n",
      "          0.3442, 0.3691, 0.3231, 0.2383, 0.7026, 0.3205, 0.0480, 0.3825,\n",
      "          0.1498, 0.1963, 0.2005, 0.2286, 0.2605, 0.2507, 0.4215, 0.5527,\n",
      "          0.5934, 0.3127, 0.2701, 0.1706, 0.2159, 0.3049, 0.2880, 0.2884,\n",
      "          0.2882]],\n",
      "\n",
      "        [[0.8285, 0.5711, 0.7654, 0.6786, 0.6512, 0.5893, 0.7336, 0.9474,\n",
      "          0.6955, 0.5818, 0.6290, 0.7392, 0.2608, 0.7032, 0.9444, 0.7495,\n",
      "          0.8531, 0.7933, 0.8381, 0.7226, 0.7851, 0.7708, 0.4592, 0.5588,\n",
      "          0.3470, 0.6491, 0.6106, 0.7900, 0.7295, 0.6618, 0.7085, 0.6916,\n",
      "          0.6603],\n",
      "         [0.1715, 0.4289, 0.2346, 0.3214, 0.3488, 0.4107, 0.2664, 0.0526,\n",
      "          0.3045, 0.4182, 0.3710, 0.2608, 0.7392, 0.2968, 0.0556, 0.2505,\n",
      "          0.1469, 0.2067, 0.1619, 0.2774, 0.2149, 0.2292, 0.5408, 0.4412,\n",
      "          0.6530, 0.3509, 0.3894, 0.2100, 0.2705, 0.3382, 0.2915, 0.3084,\n",
      "          0.3397]],\n",
      "\n",
      "        [[0.6788, 0.6861, 0.7242, 0.7402, 0.5938, 0.5256, 0.7444, 0.8589,\n",
      "          0.7242, 0.6386, 0.7805, 0.7109, 0.4054, 0.8084, 0.9392, 0.6460,\n",
      "          0.7895, 0.7258, 0.8094, 0.5045, 0.7854, 0.7850, 0.5545, 0.6254,\n",
      "          0.4249, 0.8314, 0.7125, 0.7788, 0.7020, 0.5861, 0.6837, 0.6499,\n",
      "          0.5461],\n",
      "         [0.3212, 0.3139, 0.2758, 0.2598, 0.4062, 0.4744, 0.2556, 0.1411,\n",
      "          0.2758, 0.3614, 0.2195, 0.2891, 0.5946, 0.1916, 0.0608, 0.3540,\n",
      "          0.2105, 0.2742, 0.1906, 0.4955, 0.2146, 0.2150, 0.4455, 0.3746,\n",
      "          0.5751, 0.1686, 0.2875, 0.2212, 0.2980, 0.4139, 0.3163, 0.3501,\n",
      "          0.4539]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 1, 1, 1, 2, 2]), tensor([12, 23, 24, 12, 22, 24, 12, 24]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32])]\n",
      "new_phon_tokes=[12, 23, 24]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23])]\n",
      "new_phon_tokes=[12, 22, 24]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2])]\n",
      "new_phon_tokes=[12, 24]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 8, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([8, 8])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.8706, 0.6243, 0.7284, 0.7780, 0.7114, 0.5991, 0.7513, 0.8938,\n",
      "          0.7647, 0.6380, 0.8155, 0.6382, 0.6656, 0.6972, 0.7945, 0.6847,\n",
      "          0.9029, 0.8040, 0.8776, 0.5033, 0.7175, 0.6531, 0.4281, 0.7913,\n",
      "          0.4296, 0.4362, 0.6885, 0.7194, 0.4176, 0.6694, 0.8472, 0.7678,\n",
      "          0.7281],\n",
      "         [0.1294, 0.3757, 0.2716, 0.2220, 0.2886, 0.4009, 0.2487, 0.1062,\n",
      "          0.2353, 0.3620, 0.1845, 0.3618, 0.3344, 0.3028, 0.2055, 0.3153,\n",
      "          0.0971, 0.1960, 0.1224, 0.4967, 0.2825, 0.3469, 0.5719, 0.2087,\n",
      "          0.5704, 0.5638, 0.3115, 0.2806, 0.5824, 0.3306, 0.1528, 0.2322,\n",
      "          0.2719]],\n",
      "\n",
      "        [[0.7578, 0.6876, 0.6635, 0.8183, 0.5645, 0.6190, 0.7770, 0.8277,\n",
      "          0.7789, 0.7302, 0.8587, 0.5554, 0.7312, 0.7026, 0.7260, 0.6707,\n",
      "          0.8507, 0.7918, 0.8120, 0.4691, 0.5956, 0.7781, 0.3764, 0.6674,\n",
      "          0.4437, 0.5221, 0.7907, 0.6641, 0.5440, 0.6846, 0.8315, 0.7724,\n",
      "          0.6694],\n",
      "         [0.2422, 0.3124, 0.3365, 0.1817, 0.4355, 0.3810, 0.2230, 0.1723,\n",
      "          0.2211, 0.2698, 0.1413, 0.4446, 0.2688, 0.2974, 0.2740, 0.3293,\n",
      "          0.1493, 0.2082, 0.1880, 0.5309, 0.4044, 0.2219, 0.6236, 0.3326,\n",
      "          0.5563, 0.4779, 0.2093, 0.3359, 0.4560, 0.3154, 0.1685, 0.2276,\n",
      "          0.3306]],\n",
      "\n",
      "        [[0.8319, 0.7418, 0.6456, 0.7673, 0.6644, 0.5668, 0.8036, 0.8305,\n",
      "          0.6975, 0.6213, 0.8349, 0.6157, 0.6754, 0.7373, 0.7403, 0.5941,\n",
      "          0.8950, 0.7713, 0.8639, 0.4770, 0.6909, 0.6606, 0.4388, 0.7080,\n",
      "          0.4838, 0.5666, 0.7800, 0.6352, 0.5004, 0.6224, 0.7821, 0.7403,\n",
      "          0.7756],\n",
      "         [0.1681, 0.2582, 0.3544, 0.2327, 0.3356, 0.4332, 0.1964, 0.1695,\n",
      "          0.3025, 0.3787, 0.1651, 0.3843, 0.3246, 0.2627, 0.2597, 0.4059,\n",
      "          0.1050, 0.2287, 0.1361, 0.5230, 0.3091, 0.3394, 0.5612, 0.2920,\n",
      "          0.5162, 0.4334, 0.2200, 0.3648, 0.4996, 0.3776, 0.2179, 0.2597,\n",
      "          0.2244]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2]), tensor([22, 24, 25, 28, 19, 22, 24, 19, 22, 24]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24])]\n",
      "new_phon_tokes=[22, 24, 25, 28]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24])]\n",
      "new_phon_tokes=[19, 22, 24]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24])]\n",
      "new_phon_tokes=[19, 22, 24]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 9, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([9, 9])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.6647, 0.3621, 0.5413, 0.5702, 0.7438, 0.7114, 0.8412, 0.7877,\n",
      "          0.6055, 0.6270, 0.8747, 0.8051, 0.7365, 0.5313, 0.5555, 0.4258,\n",
      "          0.8576, 0.7987, 0.7401, 0.7245, 0.6273, 0.6014, 0.6866, 0.8314,\n",
      "          0.8198, 0.7878, 0.6893, 0.4225, 0.6130, 0.6594, 0.7683, 0.8254,\n",
      "          0.7616],\n",
      "         [0.3353, 0.6379, 0.4587, 0.4298, 0.2562, 0.2886, 0.1588, 0.2123,\n",
      "          0.3945, 0.3730, 0.1253, 0.1949, 0.2635, 0.4687, 0.4445, 0.5742,\n",
      "          0.1424, 0.2013, 0.2599, 0.2755, 0.3727, 0.3986, 0.3134, 0.1686,\n",
      "          0.1802, 0.2122, 0.3107, 0.5775, 0.3870, 0.3406, 0.2317, 0.1746,\n",
      "          0.2384]],\n",
      "\n",
      "        [[0.6494, 0.4691, 0.5812, 0.5599, 0.5679, 0.6517, 0.8660, 0.7922,\n",
      "          0.5900, 0.6679, 0.8579, 0.8080, 0.7016, 0.5042, 0.6004, 0.4815,\n",
      "          0.8873, 0.7731, 0.7711, 0.7243, 0.6300, 0.5600, 0.7242, 0.8510,\n",
      "          0.7326, 0.8505, 0.7466, 0.3855, 0.5740, 0.6883, 0.7740, 0.7911,\n",
      "          0.7384],\n",
      "         [0.3506, 0.5309, 0.4188, 0.4401, 0.4321, 0.3483, 0.1340, 0.2078,\n",
      "          0.4100, 0.3321, 0.1421, 0.1920, 0.2984, 0.4958, 0.3996, 0.5185,\n",
      "          0.1127, 0.2269, 0.2289, 0.2757, 0.3700, 0.4400, 0.2758, 0.1490,\n",
      "          0.2674, 0.1495, 0.2534, 0.6145, 0.4260, 0.3117, 0.2260, 0.2089,\n",
      "          0.2616]],\n",
      "\n",
      "        [[0.6550, 0.5317, 0.5791, 0.5456, 0.5552, 0.6154, 0.8808, 0.7905,\n",
      "          0.5603, 0.6551, 0.8417, 0.8132, 0.7174, 0.4738, 0.5273, 0.4317,\n",
      "          0.8803, 0.7371, 0.7794, 0.7156, 0.6346, 0.5332, 0.7235, 0.8530,\n",
      "          0.7513, 0.8578, 0.7429, 0.3951, 0.5013, 0.6515, 0.7522, 0.7885,\n",
      "          0.7392],\n",
      "         [0.3450, 0.4683, 0.4209, 0.4544, 0.4448, 0.3846, 0.1192, 0.2095,\n",
      "          0.4397, 0.3449, 0.1583, 0.1868, 0.2826, 0.5262, 0.4727, 0.5683,\n",
      "          0.1197, 0.2629, 0.2206, 0.2844, 0.3654, 0.4668, 0.2765, 0.1470,\n",
      "          0.2487, 0.1422, 0.2571, 0.6049, 0.4987, 0.3485, 0.2478, 0.2115,\n",
      "          0.2608]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 1, 1, 1, 2, 2, 2]), tensor([ 1, 15, 27,  1, 15, 27, 13, 15, 27]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28])]\n",
      "new_phon_tokes=[1, 15, 27]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24])]\n",
      "new_phon_tokes=[1, 15, 27]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24])]\n",
      "new_phon_tokes=[13, 15, 27]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 10, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([10, 10])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.6049, 0.5730, 0.5557, 0.8177, 0.7672, 0.6812, 0.7011, 0.7674,\n",
      "          0.6037, 0.6773, 0.8222, 0.8888, 0.6836, 0.6313, 0.7728, 0.5471,\n",
      "          0.8577, 0.6507, 0.5788, 0.4382, 0.7928, 0.7129, 0.7475, 0.6806,\n",
      "          0.5468, 0.7069, 0.8948, 0.6336, 0.5122, 0.8024, 0.8625, 0.5979,\n",
      "          0.8417],\n",
      "         [0.3951, 0.4270, 0.4443, 0.1823, 0.2328, 0.3188, 0.2989, 0.2326,\n",
      "          0.3963, 0.3227, 0.1778, 0.1112, 0.3164, 0.3687, 0.2272, 0.4529,\n",
      "          0.1423, 0.3493, 0.4212, 0.5618, 0.2072, 0.2871, 0.2525, 0.3194,\n",
      "          0.4532, 0.2931, 0.1052, 0.3664, 0.4878, 0.1976, 0.1375, 0.4021,\n",
      "          0.1583]],\n",
      "\n",
      "        [[0.6031, 0.6215, 0.5509, 0.8314, 0.7599, 0.6672, 0.6834, 0.7521,\n",
      "          0.6139, 0.6969, 0.8290, 0.8889, 0.6690, 0.6602, 0.7611, 0.5718,\n",
      "          0.8575, 0.6051, 0.5898, 0.4026, 0.7892, 0.7474, 0.7397, 0.6730,\n",
      "          0.5332, 0.7135, 0.8930, 0.5987, 0.5251, 0.8021, 0.8587, 0.5604,\n",
      "          0.8278],\n",
      "         [0.3969, 0.3785, 0.4491, 0.1686, 0.2401, 0.3328, 0.3166, 0.2479,\n",
      "          0.3861, 0.3031, 0.1710, 0.1111, 0.3310, 0.3398, 0.2389, 0.4282,\n",
      "          0.1425, 0.3949, 0.4102, 0.5974, 0.2108, 0.2526, 0.2603, 0.3270,\n",
      "          0.4668, 0.2865, 0.1070, 0.4013, 0.4749, 0.1979, 0.1413, 0.4396,\n",
      "          0.1722]],\n",
      "\n",
      "        [[0.6556, 0.7398, 0.5287, 0.8159, 0.7546, 0.6033, 0.7183, 0.7430,\n",
      "          0.5848, 0.6399, 0.8015, 0.8756, 0.7179, 0.6409, 0.7084, 0.5833,\n",
      "          0.8269, 0.5377, 0.5836, 0.4603, 0.8508, 0.7347, 0.7647, 0.7152,\n",
      "          0.6133, 0.6734, 0.8362, 0.5685, 0.4393, 0.7620, 0.8464, 0.5713,\n",
      "          0.8560],\n",
      "         [0.3444, 0.2602, 0.4713, 0.1841, 0.2454, 0.3967, 0.2817, 0.2570,\n",
      "          0.4152, 0.3601, 0.1985, 0.1244, 0.2821, 0.3591, 0.2916, 0.4167,\n",
      "          0.1731, 0.4623, 0.4164, 0.5397, 0.1492, 0.2653, 0.2353, 0.2848,\n",
      "          0.3867, 0.3266, 0.1638, 0.4315, 0.5607, 0.2380, 0.1536, 0.4287,\n",
      "          0.1440]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 1, 2, 2]), tensor([19, 19, 19, 28]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27])]\n",
      "new_phon_tokes=[19]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27])]\n",
      "new_phon_tokes=[19]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27])]\n",
      "new_phon_tokes=[19, 28]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 11, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([11, 11])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7674, 0.3516, 0.8392, 0.6459, 0.3942, 0.5681, 0.5619, 0.8909,\n",
      "          0.7410, 0.6376, 0.7080, 0.8285, 0.4759, 0.3745, 0.7089, 0.8336,\n",
      "          0.9375, 0.8335, 0.8084, 0.7300, 0.6977, 0.3979, 0.7756, 0.8957,\n",
      "          0.5406, 0.6671, 0.7823, 0.5342, 0.4390, 0.7640, 0.7540, 0.7363,\n",
      "          0.4319],\n",
      "         [0.2326, 0.6484, 0.1608, 0.3541, 0.6058, 0.4319, 0.4381, 0.1091,\n",
      "          0.2590, 0.3624, 0.2920, 0.1715, 0.5241, 0.6255, 0.2911, 0.1664,\n",
      "          0.0625, 0.1665, 0.1916, 0.2700, 0.3023, 0.6021, 0.2244, 0.1043,\n",
      "          0.4594, 0.3329, 0.2177, 0.4658, 0.5610, 0.2360, 0.2460, 0.2637,\n",
      "          0.5681]],\n",
      "\n",
      "        [[0.7616, 0.3769, 0.8408, 0.6602, 0.3944, 0.5551, 0.5403, 0.8887,\n",
      "          0.7520, 0.6510, 0.7143, 0.8222, 0.4566, 0.3946, 0.6860, 0.8543,\n",
      "          0.9358, 0.8112, 0.8103, 0.7067, 0.7020, 0.4230, 0.7702, 0.8936,\n",
      "          0.5229, 0.6744, 0.7775, 0.4891, 0.4526, 0.7563, 0.7547, 0.7130,\n",
      "          0.3967],\n",
      "         [0.2384, 0.6231, 0.1592, 0.3398, 0.6056, 0.4449, 0.4597, 0.1113,\n",
      "          0.2480, 0.3490, 0.2857, 0.1778, 0.5434, 0.6054, 0.3140, 0.1457,\n",
      "          0.0642, 0.1888, 0.1897, 0.2933, 0.2980, 0.5770, 0.2298, 0.1064,\n",
      "          0.4771, 0.3256, 0.2225, 0.5109, 0.5474, 0.2437, 0.2453, 0.2870,\n",
      "          0.6033]],\n",
      "\n",
      "        [[0.7618, 0.2968, 0.7533, 0.6952, 0.5954, 0.5743, 0.5198, 0.8898,\n",
      "          0.7503, 0.6148, 0.6899, 0.8156, 0.4835, 0.3636, 0.6102, 0.8239,\n",
      "          0.9497, 0.8548, 0.7016, 0.7133, 0.6858, 0.3642, 0.7801, 0.8781,\n",
      "          0.6466, 0.5606, 0.7420, 0.6127, 0.3814, 0.7639, 0.7618, 0.7587,\n",
      "          0.4291],\n",
      "         [0.2382, 0.7032, 0.2467, 0.3048, 0.4046, 0.4257, 0.4802, 0.1102,\n",
      "          0.2497, 0.3852, 0.3101, 0.1844, 0.5165, 0.6364, 0.3898, 0.1761,\n",
      "          0.0503, 0.1452, 0.2984, 0.2867, 0.3142, 0.6358, 0.2199, 0.1219,\n",
      "          0.3534, 0.4394, 0.2580, 0.3873, 0.6186, 0.2361, 0.2382, 0.2413,\n",
      "          0.5709]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 1]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]), tensor([ 1,  4, 12, 13, 21, 28, 32,  1,  4, 12, 13, 21, 27, 28, 32,  1, 12, 13,\n",
      "        21, 28, 32]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19])]\n",
      "new_phon_tokes=[1, 4, 12, 13, 21, 28, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19])]\n",
      "new_phon_tokes=[1, 4, 12, 13, 21, 27, 28, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28])]\n",
      "new_phon_tokes=[1, 12, 13, 21, 28, 32]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 12, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([12, 12])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7222, 0.5180, 0.4592, 0.8218, 0.7377, 0.6507, 0.5017, 0.5822,\n",
      "          0.7179, 0.6986, 0.9156, 0.8695, 0.7644, 0.5906, 0.7302, 0.5570,\n",
      "          0.7320, 0.7361, 0.6259, 0.6766, 0.6678, 0.6422, 0.7165, 0.4920,\n",
      "          0.8700, 0.5080, 0.8923, 0.7216, 0.7732, 0.6887, 0.8008, 0.4210,\n",
      "          0.3831],\n",
      "         [0.2778, 0.4820, 0.5408, 0.1782, 0.2623, 0.3493, 0.4983, 0.4178,\n",
      "          0.2821, 0.3014, 0.0844, 0.1305, 0.2356, 0.4094, 0.2698, 0.4430,\n",
      "          0.2680, 0.2639, 0.3741, 0.3234, 0.3322, 0.3578, 0.2835, 0.5080,\n",
      "          0.1300, 0.4920, 0.1077, 0.2784, 0.2268, 0.3113, 0.1992, 0.5790,\n",
      "          0.6169]],\n",
      "\n",
      "        [[0.7134, 0.5746, 0.4503, 0.8260, 0.7391, 0.6340, 0.4626, 0.5646,\n",
      "          0.7060, 0.7154, 0.9225, 0.8781, 0.7608, 0.6308, 0.6964, 0.5778,\n",
      "          0.7218, 0.6631, 0.6177, 0.6331, 0.6939, 0.6546, 0.7330, 0.4864,\n",
      "          0.8691, 0.5297, 0.8842, 0.6636, 0.7794, 0.6739, 0.7858, 0.3789,\n",
      "          0.3501],\n",
      "         [0.2866, 0.4254, 0.5497, 0.1740, 0.2609, 0.3660, 0.5374, 0.4354,\n",
      "          0.2940, 0.2846, 0.0775, 0.1219, 0.2392, 0.3692, 0.3036, 0.4222,\n",
      "          0.2782, 0.3369, 0.3823, 0.3669, 0.3061, 0.3454, 0.2670, 0.5136,\n",
      "          0.1309, 0.4703, 0.1158, 0.3364, 0.2206, 0.3261, 0.2142, 0.6211,\n",
      "          0.6499]],\n",
      "\n",
      "        [[0.7190, 0.5833, 0.4434, 0.8352, 0.7281, 0.6053, 0.5225, 0.5848,\n",
      "          0.7069, 0.6828, 0.9141, 0.8731, 0.7647, 0.5569, 0.6967, 0.5433,\n",
      "          0.7270, 0.6745, 0.6176, 0.6537, 0.6912, 0.6137, 0.7785, 0.4828,\n",
      "          0.8774, 0.5202, 0.8853, 0.6989, 0.7712, 0.6836, 0.7798, 0.3871,\n",
      "          0.3669],\n",
      "         [0.2810, 0.4167, 0.5566, 0.1648, 0.2719, 0.3947, 0.4775, 0.4152,\n",
      "          0.2931, 0.3172, 0.0859, 0.1269, 0.2353, 0.4431, 0.3033, 0.4567,\n",
      "          0.2730, 0.3255, 0.3824, 0.3463, 0.3088, 0.3863, 0.2215, 0.5172,\n",
      "          0.1226, 0.4798, 0.1147, 0.3011, 0.2288, 0.3164, 0.2202, 0.6129,\n",
      "          0.6331]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "tokens_tmp=(tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2]), tensor([ 2, 23, 31, 32,  2,  6, 23, 31, 32,  2, 23, 31, 32]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32])]\n",
      "new_phon_tokes=[2, 23, 31, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32])]\n",
      "new_phon_tokes=[2, 6, 23, 31, 32]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32])]\n",
      "new_phon_tokes=[2, 23, 31, 32]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32]), tensor([ 2,  6, 23, 31, 32])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32])]]\n",
      "\tgenerated_phon_embeddings:  torch.Size([3, 13, 64])\n",
      "\tprompt_encoding:  torch.Size([3, 1, 64])\n",
      "\tstep_mask:  torch.Size([13, 13])\n",
      "--In phono_sample--\n",
      "last_token_probs.shape=torch.Size([3, 2, 33])\n",
      "last_token_probs=tensor([[[0.7930, 0.6915, 0.8190, 0.6953, 0.7318, 0.6317, 0.7835, 0.8732,\n",
      "          0.7091, 0.6803, 0.7958, 0.8033, 0.5421, 0.4805, 0.8736, 0.6178,\n",
      "          0.8879, 0.7801, 0.9044, 0.6799, 0.7148, 0.8424, 0.6352, 0.7612,\n",
      "          0.4720, 0.7377, 0.5347, 0.8495, 0.6680, 0.5638, 0.6546, 0.7138,\n",
      "          0.7219],\n",
      "         [0.2070, 0.3085, 0.1810, 0.3047, 0.2682, 0.3683, 0.2165, 0.1268,\n",
      "          0.2909, 0.3197, 0.2042, 0.1967, 0.4579, 0.5195, 0.1264, 0.3822,\n",
      "          0.1121, 0.2199, 0.0956, 0.3201, 0.2852, 0.1576, 0.3648, 0.2388,\n",
      "          0.5280, 0.2623, 0.4653, 0.1505, 0.3320, 0.4362, 0.3454, 0.2862,\n",
      "          0.2781]],\n",
      "\n",
      "        [[0.8061, 0.7195, 0.8092, 0.6714, 0.7026, 0.6534, 0.7853, 0.8867,\n",
      "          0.7186, 0.6941, 0.7966, 0.7972, 0.5228, 0.4944, 0.8753, 0.6420,\n",
      "          0.8776, 0.7829, 0.9175, 0.6615, 0.7208, 0.8751, 0.5986, 0.7490,\n",
      "          0.4262, 0.7544, 0.5418, 0.8127, 0.6798, 0.5389, 0.6284, 0.7029,\n",
      "          0.7200],\n",
      "         [0.1939, 0.2805, 0.1908, 0.3286, 0.2974, 0.3466, 0.2147, 0.1133,\n",
      "          0.2814, 0.3059, 0.2034, 0.2028, 0.4772, 0.5056, 0.1247, 0.3580,\n",
      "          0.1224, 0.2171, 0.0825, 0.3385, 0.2792, 0.1249, 0.4014, 0.2510,\n",
      "          0.5738, 0.2456, 0.4582, 0.1873, 0.3202, 0.4611, 0.3716, 0.2971,\n",
      "          0.2800]],\n",
      "\n",
      "        [[0.7966, 0.7503, 0.8128, 0.7057, 0.7204, 0.5880, 0.8021, 0.8635,\n",
      "          0.6951, 0.6880, 0.7981, 0.8047, 0.5477, 0.4567, 0.8398, 0.5801,\n",
      "          0.8730, 0.7330, 0.9183, 0.6537, 0.7192, 0.8570, 0.6342, 0.7699,\n",
      "          0.4983, 0.7661, 0.5132, 0.8318, 0.6292, 0.5486, 0.6145, 0.6903,\n",
      "          0.7093],\n",
      "         [0.2034, 0.2497, 0.1872, 0.2943, 0.2796, 0.4120, 0.1979, 0.1365,\n",
      "          0.3049, 0.3120, 0.2019, 0.1953, 0.4523, 0.5433, 0.1602, 0.4199,\n",
      "          0.1270, 0.2670, 0.0817, 0.3463, 0.2808, 0.1430, 0.3658, 0.2301,\n",
      "          0.5017, 0.2339, 0.4868, 0.1682, 0.3708, 0.4514, 0.3855, 0.3097,\n",
      "          0.2907]]])\n",
      "feature_presence.shape=torch.Size([3, 33])\n",
      "feature_presence=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tokens_tmp=(tensor([0, 0, 1, 1, 2, 2]), tensor([13, 24, 13, 24, 13, 24]))\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32])]\n",
      "new_phon_tokes=[13, 24]\n",
      "gen_phon_tokes=[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32]), tensor([ 2,  6, 23, 31, 32])]\n",
      "new_phon_tokes=[13, 24]\n",
      "gen_phon_tokes=[tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32])]\n",
      "new_phon_tokes=[13, 24]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32]), tensor([ 2,  6, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])]]\n",
      "generated_phon_probs=[[tensor([0.1770, 0.3055, 0.3763, 0.1420, 0.2092, 0.2464, 0.3488, 0.3784, 0.2795,\n",
      "        0.4060, 0.1076, 0.2201, 0.2954, 0.3502, 0.2638, 0.4393, 0.1195, 0.2436,\n",
      "        0.1357, 0.2634, 0.3397, 0.4905, 0.3514, 0.3438, 0.3917, 0.3305, 0.1682,\n",
      "        0.2976, 0.1913, 0.4296, 0.3353, 0.3788, 0.3615]), tensor([0.1873, 0.4990, 0.3855, 0.1939, 0.1289, 0.3352, 0.1321, 0.3645, 0.3264,\n",
      "        0.3800, 0.1756, 0.2080, 0.4356, 0.3287, 0.3539, 0.6144, 0.1892, 0.3377,\n",
      "        0.2163, 0.1190, 0.3186, 0.4466, 0.2985, 0.4022, 0.2955, 0.2117, 0.4793,\n",
      "        0.2362, 0.2189, 0.4479, 0.1317, 0.4158, 0.3517]), tensor([0.2296, 0.3925, 0.3635, 0.2024, 0.4440, 0.4359, 0.1705, 0.2687, 0.1266,\n",
      "        0.1876, 0.2063, 0.1086, 0.3880, 0.2975, 0.2245, 0.3437, 0.1168, 0.2491,\n",
      "        0.1376, 0.2590, 0.2712, 0.1813, 0.4737, 0.1613, 0.2515, 0.5152, 0.1975,\n",
      "        0.3061, 0.4520, 0.1587, 0.2266, 0.1714, 0.3159]), tensor([0.2751, 0.5407, 0.2538, 0.2428, 0.2646, 0.3582, 0.3534, 0.3256, 0.1147,\n",
      "        0.4101, 0.2087, 0.1941, 0.2563, 0.4824, 0.2817, 0.2562, 0.2066, 0.3453,\n",
      "        0.1663, 0.4973, 0.3618, 0.3093, 0.3650, 0.1038, 0.2773, 0.3863, 0.2498,\n",
      "        0.4297, 0.2656, 0.4750, 0.3692, 0.3578, 0.5384]), tensor([0.4293, 0.6264, 0.2320, 0.3617, 0.3175, 0.3988, 0.3699, 0.2298, 0.2960,\n",
      "        0.5028, 0.2430, 0.2476, 0.1870, 0.4591, 0.2452, 0.3940, 0.0952, 0.1805,\n",
      "        0.3842, 0.4724, 0.4373, 0.6164, 0.2055, 0.2654, 0.3206, 0.5385, 0.0556,\n",
      "        0.2199, 0.3941, 0.2560, 0.1384, 0.2551, 0.2768]), tensor([0.3242, 0.5412, 0.5559, 0.2247, 0.4030, 0.2347, 0.3742, 0.3886, 0.4191,\n",
      "        0.2903, 0.1254, 0.1772, 0.5296, 0.4035, 0.3676, 0.4943, 0.1323, 0.3166,\n",
      "        0.3868, 0.2564, 0.2496, 0.3359, 0.2306, 0.4707, 0.2037, 0.1939, 0.1422,\n",
      "        0.4037, 0.1648, 0.2598, 0.3223, 0.4093, 0.5758]), tensor([0.2277, 0.4540, 0.2577, 0.3147, 0.3976, 0.3159, 0.2218, 0.0683, 0.3442,\n",
      "        0.3691, 0.3231, 0.2383, 0.7026, 0.3205, 0.0480, 0.3825, 0.1498, 0.1963,\n",
      "        0.2005, 0.2286, 0.2605, 0.2507, 0.4215, 0.5527, 0.5934, 0.3127, 0.2701,\n",
      "        0.1706, 0.2159, 0.3049, 0.2880, 0.2884, 0.2882]), tensor([0.1294, 0.3757, 0.2716, 0.2220, 0.2886, 0.4009, 0.2487, 0.1062, 0.2353,\n",
      "        0.3620, 0.1845, 0.3618, 0.3344, 0.3028, 0.2055, 0.3153, 0.0971, 0.1960,\n",
      "        0.1224, 0.4967, 0.2825, 0.3469, 0.5719, 0.2087, 0.5704, 0.5638, 0.3115,\n",
      "        0.2806, 0.5824, 0.3306, 0.1528, 0.2322, 0.2719]), tensor([0.3353, 0.6379, 0.4587, 0.4298, 0.2562, 0.2886, 0.1588, 0.2123, 0.3945,\n",
      "        0.3730, 0.1253, 0.1949, 0.2635, 0.4687, 0.4445, 0.5742, 0.1424, 0.2013,\n",
      "        0.2599, 0.2755, 0.3727, 0.3986, 0.3134, 0.1686, 0.1802, 0.2122, 0.3107,\n",
      "        0.5775, 0.3870, 0.3406, 0.2317, 0.1746, 0.2384]), tensor([0.3951, 0.4270, 0.4443, 0.1823, 0.2328, 0.3188, 0.2989, 0.2326, 0.3963,\n",
      "        0.3227, 0.1778, 0.1112, 0.3164, 0.3687, 0.2272, 0.4529, 0.1423, 0.3493,\n",
      "        0.4212, 0.5618, 0.2072, 0.2871, 0.2525, 0.3194, 0.4532, 0.2931, 0.1052,\n",
      "        0.3664, 0.4878, 0.1976, 0.1375, 0.4021, 0.1583]), tensor([0.2326, 0.6484, 0.1608, 0.3541, 0.6058, 0.4319, 0.4381, 0.1091, 0.2590,\n",
      "        0.3624, 0.2920, 0.1715, 0.5241, 0.6255, 0.2911, 0.1664, 0.0625, 0.1665,\n",
      "        0.1916, 0.2700, 0.3023, 0.6021, 0.2244, 0.1043, 0.4594, 0.3329, 0.2177,\n",
      "        0.4658, 0.5610, 0.2360, 0.2460, 0.2637, 0.5681]), tensor([0.2778, 0.4820, 0.5408, 0.1782, 0.2623, 0.3493, 0.4983, 0.4178, 0.2821,\n",
      "        0.3014, 0.0844, 0.1305, 0.2356, 0.4094, 0.2698, 0.4430, 0.2680, 0.2639,\n",
      "        0.3741, 0.3234, 0.3322, 0.3578, 0.2835, 0.5080, 0.1300, 0.4920, 0.1077,\n",
      "        0.2784, 0.2268, 0.3113, 0.1992, 0.5790, 0.6169]), tensor([0.2070, 0.3085, 0.1810, 0.3047, 0.2682, 0.3683, 0.2165, 0.1268, 0.2909,\n",
      "        0.3197, 0.2042, 0.1967, 0.4579, 0.5195, 0.1264, 0.3822, 0.1121, 0.2199,\n",
      "        0.0956, 0.3201, 0.2852, 0.1576, 0.3648, 0.2388, 0.5280, 0.2623, 0.4653,\n",
      "        0.1505, 0.3320, 0.4362, 0.3454, 0.2862, 0.2781])], [tensor([0.1818, 0.2765, 0.3784, 0.1359, 0.2110, 0.2546, 0.3624, 0.3823, 0.2661,\n",
      "        0.3887, 0.1048, 0.2285, 0.3074, 0.3288, 0.2831, 0.4076, 0.1233, 0.2763,\n",
      "        0.1339, 0.2898, 0.3331, 0.4635, 0.3592, 0.3563, 0.4083, 0.3177, 0.1712,\n",
      "        0.3362, 0.1822, 0.4348, 0.3348, 0.4047, 0.3912]), tensor([0.1874, 0.4598, 0.3873, 0.1823, 0.1268, 0.3448, 0.1374, 0.3706, 0.3108,\n",
      "        0.3628, 0.1661, 0.2137, 0.4509, 0.3041, 0.3737, 0.5860, 0.1956, 0.3786,\n",
      "        0.2085, 0.1311, 0.3111, 0.4185, 0.3051, 0.4151, 0.3080, 0.1980, 0.4910,\n",
      "        0.2680, 0.2073, 0.4631, 0.1292, 0.4446, 0.3807]), tensor([0.2345, 0.3494, 0.3664, 0.1946, 0.4474, 0.4484, 0.1768, 0.2763, 0.1182,\n",
      "        0.1785, 0.2004, 0.1142, 0.3986, 0.2762, 0.2386, 0.3135, 0.1218, 0.2837,\n",
      "        0.1340, 0.2809, 0.2648, 0.1632, 0.4782, 0.1685, 0.2630, 0.5029, 0.2047,\n",
      "        0.3453, 0.4384, 0.1651, 0.2274, 0.1861, 0.3420]), tensor([0.2811, 0.4936, 0.2518, 0.2331, 0.2713, 0.3674, 0.3694, 0.3325, 0.1074,\n",
      "        0.3931, 0.2062, 0.2054, 0.2663, 0.4541, 0.3024, 0.2285, 0.2191, 0.3954,\n",
      "        0.1635, 0.5329, 0.3589, 0.2796, 0.3743, 0.1079, 0.2953, 0.3715, 0.2595,\n",
      "        0.4795, 0.2571, 0.4896, 0.3705, 0.3843, 0.5750]), tensor([0.4668, 0.4973, 0.1795, 0.3977, 0.3931, 0.4390, 0.4252, 0.1639, 0.2809,\n",
      "        0.4520, 0.2967, 0.2530, 0.2587, 0.5707, 0.2598, 0.2377, 0.1178, 0.2235,\n",
      "        0.3414, 0.4371, 0.3557, 0.5987, 0.1875, 0.1274, 0.4044, 0.3744, 0.0894,\n",
      "        0.3997, 0.5993, 0.2360, 0.1195, 0.3060, 0.3725]), tensor([0.2947, 0.4298, 0.6875, 0.1774, 0.2755, 0.2649, 0.3909, 0.4599, 0.4373,\n",
      "        0.3672, 0.1533, 0.2510, 0.5090, 0.3720, 0.4232, 0.4119, 0.0931, 0.2788,\n",
      "        0.5202, 0.1789, 0.1732, 0.3850, 0.2206, 0.5132, 0.2013, 0.2815, 0.1970,\n",
      "        0.3316, 0.2138, 0.2564, 0.2230, 0.4533, 0.4809]), tensor([0.1715, 0.4289, 0.2346, 0.3214, 0.3488, 0.4107, 0.2664, 0.0526, 0.3045,\n",
      "        0.4182, 0.3710, 0.2608, 0.7392, 0.2968, 0.0556, 0.2505, 0.1469, 0.2067,\n",
      "        0.1619, 0.2774, 0.2149, 0.2292, 0.5408, 0.4412, 0.6530, 0.3509, 0.3894,\n",
      "        0.2100, 0.2705, 0.3382, 0.2915, 0.3084, 0.3397]), tensor([0.2422, 0.3124, 0.3365, 0.1817, 0.4355, 0.3810, 0.2230, 0.1723, 0.2211,\n",
      "        0.2698, 0.1413, 0.4446, 0.2688, 0.2974, 0.2740, 0.3293, 0.1493, 0.2082,\n",
      "        0.1880, 0.5309, 0.4044, 0.2219, 0.6236, 0.3326, 0.5563, 0.4779, 0.2093,\n",
      "        0.3359, 0.4560, 0.3154, 0.1685, 0.2276, 0.3306]), tensor([0.3506, 0.5309, 0.4188, 0.4401, 0.4321, 0.3483, 0.1340, 0.2078, 0.4100,\n",
      "        0.3321, 0.1421, 0.1920, 0.2984, 0.4958, 0.3996, 0.5185, 0.1127, 0.2269,\n",
      "        0.2289, 0.2757, 0.3700, 0.4400, 0.2758, 0.1490, 0.2674, 0.1495, 0.2534,\n",
      "        0.6145, 0.4260, 0.3117, 0.2260, 0.2089, 0.2616]), tensor([0.3969, 0.3785, 0.4491, 0.1686, 0.2401, 0.3328, 0.3166, 0.2479, 0.3861,\n",
      "        0.3031, 0.1710, 0.1111, 0.3310, 0.3398, 0.2389, 0.4282, 0.1425, 0.3949,\n",
      "        0.4102, 0.5974, 0.2108, 0.2526, 0.2603, 0.3270, 0.4668, 0.2865, 0.1070,\n",
      "        0.4013, 0.4749, 0.1979, 0.1413, 0.4396, 0.1722]), tensor([0.2384, 0.6231, 0.1592, 0.3398, 0.6056, 0.4449, 0.4597, 0.1113, 0.2480,\n",
      "        0.3490, 0.2857, 0.1778, 0.5434, 0.6054, 0.3140, 0.1457, 0.0642, 0.1888,\n",
      "        0.1897, 0.2933, 0.2980, 0.5770, 0.2298, 0.1064, 0.4771, 0.3256, 0.2225,\n",
      "        0.5109, 0.5474, 0.2437, 0.2453, 0.2870, 0.6033]), tensor([0.2866, 0.4254, 0.5497, 0.1740, 0.2609, 0.3660, 0.5374, 0.4354, 0.2940,\n",
      "        0.2846, 0.0775, 0.1219, 0.2392, 0.3692, 0.3036, 0.4222, 0.2782, 0.3369,\n",
      "        0.3823, 0.3669, 0.3061, 0.3454, 0.2670, 0.5136, 0.1309, 0.4703, 0.1158,\n",
      "        0.3364, 0.2206, 0.3261, 0.2142, 0.6211, 0.6499]), tensor([0.1939, 0.2805, 0.1908, 0.3286, 0.2974, 0.3466, 0.2147, 0.1133, 0.2814,\n",
      "        0.3059, 0.2034, 0.2028, 0.4772, 0.5056, 0.1247, 0.3580, 0.1224, 0.2171,\n",
      "        0.0825, 0.3385, 0.2792, 0.1249, 0.4014, 0.2510, 0.5738, 0.2456, 0.4582,\n",
      "        0.1873, 0.3202, 0.4611, 0.3716, 0.2971, 0.2800])], [tensor([0.1927, 0.2688, 0.3923, 0.1395, 0.2267, 0.2703, 0.3268, 0.3772, 0.2816,\n",
      "        0.3916, 0.1164, 0.2306, 0.2922, 0.3634, 0.3451, 0.4604, 0.1307, 0.3077,\n",
      "        0.1382, 0.3035, 0.3261, 0.5085, 0.3454, 0.3485, 0.3962, 0.3106, 0.1700,\n",
      "        0.3380, 0.2143, 0.4472, 0.3514, 0.4086, 0.4035]), tensor([0.2115, 0.3153, 0.4311, 0.1250, 0.1875, 0.2884, 0.1771, 0.4743, 0.4737,\n",
      "        0.2882, 0.1562, 0.2295, 0.4607, 0.4644, 0.3206, 0.5287, 0.1873, 0.2223,\n",
      "        0.2209, 0.1535, 0.3599, 0.3119, 0.4201, 0.4409, 0.4012, 0.1049, 0.4857,\n",
      "        0.2826, 0.2650, 0.3488, 0.2094, 0.3815, 0.2645]), tensor([0.2542, 0.3452, 0.3832, 0.1995, 0.4669, 0.4761, 0.1506, 0.2723, 0.1257,\n",
      "        0.1758, 0.2152, 0.1149, 0.3851, 0.3082, 0.2989, 0.3623, 0.1287, 0.3164,\n",
      "        0.1370, 0.2936, 0.2570, 0.1884, 0.4658, 0.1645, 0.2510, 0.4969, 0.2049,\n",
      "        0.3504, 0.4788, 0.1660, 0.2385, 0.1877, 0.3519]), tensor([0.2702, 0.4172, 0.4231, 0.2755, 0.2259, 0.4392, 0.2383, 0.3402, 0.0890,\n",
      "        0.5183, 0.2733, 0.2509, 0.2292, 0.3987, 0.3194, 0.4068, 0.1443, 0.3108,\n",
      "        0.1143, 0.1847, 0.3294, 0.4776, 0.2681, 0.2015, 0.2220, 0.4101, 0.2719,\n",
      "        0.1162, 0.4325, 0.4286, 0.2456, 0.5250, 0.3875]), tensor([0.2594, 0.4130, 0.2624, 0.3373, 0.3274, 0.4299, 0.4019, 0.3048, 0.3071,\n",
      "        0.5387, 0.1896, 0.1862, 0.2115, 0.5000, 0.3000, 0.3925, 0.1217, 0.1327,\n",
      "        0.1001, 0.3563, 0.4928, 0.4799, 0.2230, 0.2351, 0.2379, 0.4944, 0.0863,\n",
      "        0.3264, 0.6284, 0.2666, 0.2384, 0.2896, 0.3700]), tensor([0.3011, 0.3776, 0.6755, 0.2593, 0.3428, 0.2756, 0.3519, 0.3969, 0.3737,\n",
      "        0.3608, 0.1547, 0.1733, 0.4643, 0.4023, 0.4179, 0.4418, 0.1125, 0.3589,\n",
      "        0.4455, 0.2210, 0.2047, 0.2933, 0.2014, 0.4883, 0.1538, 0.3425, 0.1636,\n",
      "        0.3952, 0.2880, 0.2691, 0.3066, 0.3788, 0.4901]), tensor([0.3212, 0.3139, 0.2758, 0.2598, 0.4062, 0.4744, 0.2556, 0.1411, 0.2758,\n",
      "        0.3614, 0.2195, 0.2891, 0.5946, 0.1916, 0.0608, 0.3540, 0.2105, 0.2742,\n",
      "        0.1906, 0.4955, 0.2146, 0.2150, 0.4455, 0.3746, 0.5751, 0.1686, 0.2875,\n",
      "        0.2212, 0.2980, 0.4139, 0.3163, 0.3501, 0.4539]), tensor([0.1681, 0.2582, 0.3544, 0.2327, 0.3356, 0.4332, 0.1964, 0.1695, 0.3025,\n",
      "        0.3787, 0.1651, 0.3843, 0.3246, 0.2627, 0.2597, 0.4059, 0.1050, 0.2287,\n",
      "        0.1361, 0.5230, 0.3091, 0.3394, 0.5612, 0.2920, 0.5162, 0.4334, 0.2200,\n",
      "        0.3648, 0.4996, 0.3776, 0.2179, 0.2597, 0.2244]), tensor([0.3450, 0.4683, 0.4209, 0.4544, 0.4448, 0.3846, 0.1192, 0.2095, 0.4397,\n",
      "        0.3449, 0.1583, 0.1868, 0.2826, 0.5262, 0.4727, 0.5683, 0.1197, 0.2629,\n",
      "        0.2206, 0.2844, 0.3654, 0.4668, 0.2765, 0.1470, 0.2487, 0.1422, 0.2571,\n",
      "        0.6049, 0.4987, 0.3485, 0.2478, 0.2115, 0.2608]), tensor([0.3444, 0.2602, 0.4713, 0.1841, 0.2454, 0.3967, 0.2817, 0.2570, 0.4152,\n",
      "        0.3601, 0.1985, 0.1244, 0.2821, 0.3591, 0.2916, 0.4167, 0.1731, 0.4623,\n",
      "        0.4164, 0.5397, 0.1492, 0.2653, 0.2353, 0.2848, 0.3867, 0.3266, 0.1638,\n",
      "        0.4315, 0.5607, 0.2380, 0.1536, 0.4287, 0.1440]), tensor([0.2382, 0.7032, 0.2467, 0.3048, 0.4046, 0.4257, 0.4802, 0.1102, 0.2497,\n",
      "        0.3852, 0.3101, 0.1844, 0.5165, 0.6364, 0.3898, 0.1761, 0.0503, 0.1452,\n",
      "        0.2984, 0.2867, 0.3142, 0.6358, 0.2199, 0.1219, 0.3534, 0.4394, 0.2580,\n",
      "        0.3873, 0.6186, 0.2361, 0.2382, 0.2413, 0.5709]), tensor([0.2810, 0.4167, 0.5566, 0.1648, 0.2719, 0.3947, 0.4775, 0.4152, 0.2931,\n",
      "        0.3172, 0.0859, 0.1269, 0.2353, 0.4431, 0.3033, 0.4567, 0.2730, 0.3255,\n",
      "        0.3824, 0.3463, 0.3088, 0.3863, 0.2215, 0.5172, 0.1226, 0.4798, 0.1147,\n",
      "        0.3011, 0.2288, 0.3164, 0.2202, 0.6129, 0.6331]), tensor([0.2034, 0.2497, 0.1872, 0.2943, 0.2796, 0.4120, 0.1979, 0.1365, 0.3049,\n",
      "        0.3120, 0.2019, 0.1953, 0.4523, 0.5433, 0.1602, 0.4199, 0.1270, 0.2670,\n",
      "        0.0817, 0.3463, 0.2808, 0.1430, 0.3658, 0.2301, 0.5017, 0.2339, 0.4868,\n",
      "        0.1682, 0.3708, 0.4514, 0.3855, 0.3097, 0.2907])]]\n",
      "generated_phon_vecs=[[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])]]\n",
      "generated_phon_tokens=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32]), tensor([ 2,  6, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])]]\n",
      "--- DONE WITH GENERATE PHONOLOGY ---😆\n",
      "k='orth_probs', v=None\n",
      "k='orth_tokens', v=None\n",
      "k='phon_probs', v=[[tensor([0.1770, 0.3055, 0.3763, 0.1420, 0.2092, 0.2464, 0.3488, 0.3784, 0.2795,\n",
      "        0.4060, 0.1076, 0.2201, 0.2954, 0.3502, 0.2638, 0.4393, 0.1195, 0.2436,\n",
      "        0.1357, 0.2634, 0.3397, 0.4905, 0.3514, 0.3438, 0.3917, 0.3305, 0.1682,\n",
      "        0.2976, 0.1913, 0.4296, 0.3353, 0.3788, 0.3615]), tensor([0.1873, 0.4990, 0.3855, 0.1939, 0.1289, 0.3352, 0.1321, 0.3645, 0.3264,\n",
      "        0.3800, 0.1756, 0.2080, 0.4356, 0.3287, 0.3539, 0.6144, 0.1892, 0.3377,\n",
      "        0.2163, 0.1190, 0.3186, 0.4466, 0.2985, 0.4022, 0.2955, 0.2117, 0.4793,\n",
      "        0.2362, 0.2189, 0.4479, 0.1317, 0.4158, 0.3517]), tensor([0.2296, 0.3925, 0.3635, 0.2024, 0.4440, 0.4359, 0.1705, 0.2687, 0.1266,\n",
      "        0.1876, 0.2063, 0.1086, 0.3880, 0.2975, 0.2245, 0.3437, 0.1168, 0.2491,\n",
      "        0.1376, 0.2590, 0.2712, 0.1813, 0.4737, 0.1613, 0.2515, 0.5152, 0.1975,\n",
      "        0.3061, 0.4520, 0.1587, 0.2266, 0.1714, 0.3159]), tensor([0.2751, 0.5407, 0.2538, 0.2428, 0.2646, 0.3582, 0.3534, 0.3256, 0.1147,\n",
      "        0.4101, 0.2087, 0.1941, 0.2563, 0.4824, 0.2817, 0.2562, 0.2066, 0.3453,\n",
      "        0.1663, 0.4973, 0.3618, 0.3093, 0.3650, 0.1038, 0.2773, 0.3863, 0.2498,\n",
      "        0.4297, 0.2656, 0.4750, 0.3692, 0.3578, 0.5384]), tensor([0.4293, 0.6264, 0.2320, 0.3617, 0.3175, 0.3988, 0.3699, 0.2298, 0.2960,\n",
      "        0.5028, 0.2430, 0.2476, 0.1870, 0.4591, 0.2452, 0.3940, 0.0952, 0.1805,\n",
      "        0.3842, 0.4724, 0.4373, 0.6164, 0.2055, 0.2654, 0.3206, 0.5385, 0.0556,\n",
      "        0.2199, 0.3941, 0.2560, 0.1384, 0.2551, 0.2768]), tensor([0.3242, 0.5412, 0.5559, 0.2247, 0.4030, 0.2347, 0.3742, 0.3886, 0.4191,\n",
      "        0.2903, 0.1254, 0.1772, 0.5296, 0.4035, 0.3676, 0.4943, 0.1323, 0.3166,\n",
      "        0.3868, 0.2564, 0.2496, 0.3359, 0.2306, 0.4707, 0.2037, 0.1939, 0.1422,\n",
      "        0.4037, 0.1648, 0.2598, 0.3223, 0.4093, 0.5758]), tensor([0.2277, 0.4540, 0.2577, 0.3147, 0.3976, 0.3159, 0.2218, 0.0683, 0.3442,\n",
      "        0.3691, 0.3231, 0.2383, 0.7026, 0.3205, 0.0480, 0.3825, 0.1498, 0.1963,\n",
      "        0.2005, 0.2286, 0.2605, 0.2507, 0.4215, 0.5527, 0.5934, 0.3127, 0.2701,\n",
      "        0.1706, 0.2159, 0.3049, 0.2880, 0.2884, 0.2882]), tensor([0.1294, 0.3757, 0.2716, 0.2220, 0.2886, 0.4009, 0.2487, 0.1062, 0.2353,\n",
      "        0.3620, 0.1845, 0.3618, 0.3344, 0.3028, 0.2055, 0.3153, 0.0971, 0.1960,\n",
      "        0.1224, 0.4967, 0.2825, 0.3469, 0.5719, 0.2087, 0.5704, 0.5638, 0.3115,\n",
      "        0.2806, 0.5824, 0.3306, 0.1528, 0.2322, 0.2719]), tensor([0.3353, 0.6379, 0.4587, 0.4298, 0.2562, 0.2886, 0.1588, 0.2123, 0.3945,\n",
      "        0.3730, 0.1253, 0.1949, 0.2635, 0.4687, 0.4445, 0.5742, 0.1424, 0.2013,\n",
      "        0.2599, 0.2755, 0.3727, 0.3986, 0.3134, 0.1686, 0.1802, 0.2122, 0.3107,\n",
      "        0.5775, 0.3870, 0.3406, 0.2317, 0.1746, 0.2384]), tensor([0.3951, 0.4270, 0.4443, 0.1823, 0.2328, 0.3188, 0.2989, 0.2326, 0.3963,\n",
      "        0.3227, 0.1778, 0.1112, 0.3164, 0.3687, 0.2272, 0.4529, 0.1423, 0.3493,\n",
      "        0.4212, 0.5618, 0.2072, 0.2871, 0.2525, 0.3194, 0.4532, 0.2931, 0.1052,\n",
      "        0.3664, 0.4878, 0.1976, 0.1375, 0.4021, 0.1583]), tensor([0.2326, 0.6484, 0.1608, 0.3541, 0.6058, 0.4319, 0.4381, 0.1091, 0.2590,\n",
      "        0.3624, 0.2920, 0.1715, 0.5241, 0.6255, 0.2911, 0.1664, 0.0625, 0.1665,\n",
      "        0.1916, 0.2700, 0.3023, 0.6021, 0.2244, 0.1043, 0.4594, 0.3329, 0.2177,\n",
      "        0.4658, 0.5610, 0.2360, 0.2460, 0.2637, 0.5681]), tensor([0.2778, 0.4820, 0.5408, 0.1782, 0.2623, 0.3493, 0.4983, 0.4178, 0.2821,\n",
      "        0.3014, 0.0844, 0.1305, 0.2356, 0.4094, 0.2698, 0.4430, 0.2680, 0.2639,\n",
      "        0.3741, 0.3234, 0.3322, 0.3578, 0.2835, 0.5080, 0.1300, 0.4920, 0.1077,\n",
      "        0.2784, 0.2268, 0.3113, 0.1992, 0.5790, 0.6169]), tensor([0.2070, 0.3085, 0.1810, 0.3047, 0.2682, 0.3683, 0.2165, 0.1268, 0.2909,\n",
      "        0.3197, 0.2042, 0.1967, 0.4579, 0.5195, 0.1264, 0.3822, 0.1121, 0.2199,\n",
      "        0.0956, 0.3201, 0.2852, 0.1576, 0.3648, 0.2388, 0.5280, 0.2623, 0.4653,\n",
      "        0.1505, 0.3320, 0.4362, 0.3454, 0.2862, 0.2781])], [tensor([0.1818, 0.2765, 0.3784, 0.1359, 0.2110, 0.2546, 0.3624, 0.3823, 0.2661,\n",
      "        0.3887, 0.1048, 0.2285, 0.3074, 0.3288, 0.2831, 0.4076, 0.1233, 0.2763,\n",
      "        0.1339, 0.2898, 0.3331, 0.4635, 0.3592, 0.3563, 0.4083, 0.3177, 0.1712,\n",
      "        0.3362, 0.1822, 0.4348, 0.3348, 0.4047, 0.3912]), tensor([0.1874, 0.4598, 0.3873, 0.1823, 0.1268, 0.3448, 0.1374, 0.3706, 0.3108,\n",
      "        0.3628, 0.1661, 0.2137, 0.4509, 0.3041, 0.3737, 0.5860, 0.1956, 0.3786,\n",
      "        0.2085, 0.1311, 0.3111, 0.4185, 0.3051, 0.4151, 0.3080, 0.1980, 0.4910,\n",
      "        0.2680, 0.2073, 0.4631, 0.1292, 0.4446, 0.3807]), tensor([0.2345, 0.3494, 0.3664, 0.1946, 0.4474, 0.4484, 0.1768, 0.2763, 0.1182,\n",
      "        0.1785, 0.2004, 0.1142, 0.3986, 0.2762, 0.2386, 0.3135, 0.1218, 0.2837,\n",
      "        0.1340, 0.2809, 0.2648, 0.1632, 0.4782, 0.1685, 0.2630, 0.5029, 0.2047,\n",
      "        0.3453, 0.4384, 0.1651, 0.2274, 0.1861, 0.3420]), tensor([0.2811, 0.4936, 0.2518, 0.2331, 0.2713, 0.3674, 0.3694, 0.3325, 0.1074,\n",
      "        0.3931, 0.2062, 0.2054, 0.2663, 0.4541, 0.3024, 0.2285, 0.2191, 0.3954,\n",
      "        0.1635, 0.5329, 0.3589, 0.2796, 0.3743, 0.1079, 0.2953, 0.3715, 0.2595,\n",
      "        0.4795, 0.2571, 0.4896, 0.3705, 0.3843, 0.5750]), tensor([0.4668, 0.4973, 0.1795, 0.3977, 0.3931, 0.4390, 0.4252, 0.1639, 0.2809,\n",
      "        0.4520, 0.2967, 0.2530, 0.2587, 0.5707, 0.2598, 0.2377, 0.1178, 0.2235,\n",
      "        0.3414, 0.4371, 0.3557, 0.5987, 0.1875, 0.1274, 0.4044, 0.3744, 0.0894,\n",
      "        0.3997, 0.5993, 0.2360, 0.1195, 0.3060, 0.3725]), tensor([0.2947, 0.4298, 0.6875, 0.1774, 0.2755, 0.2649, 0.3909, 0.4599, 0.4373,\n",
      "        0.3672, 0.1533, 0.2510, 0.5090, 0.3720, 0.4232, 0.4119, 0.0931, 0.2788,\n",
      "        0.5202, 0.1789, 0.1732, 0.3850, 0.2206, 0.5132, 0.2013, 0.2815, 0.1970,\n",
      "        0.3316, 0.2138, 0.2564, 0.2230, 0.4533, 0.4809]), tensor([0.1715, 0.4289, 0.2346, 0.3214, 0.3488, 0.4107, 0.2664, 0.0526, 0.3045,\n",
      "        0.4182, 0.3710, 0.2608, 0.7392, 0.2968, 0.0556, 0.2505, 0.1469, 0.2067,\n",
      "        0.1619, 0.2774, 0.2149, 0.2292, 0.5408, 0.4412, 0.6530, 0.3509, 0.3894,\n",
      "        0.2100, 0.2705, 0.3382, 0.2915, 0.3084, 0.3397]), tensor([0.2422, 0.3124, 0.3365, 0.1817, 0.4355, 0.3810, 0.2230, 0.1723, 0.2211,\n",
      "        0.2698, 0.1413, 0.4446, 0.2688, 0.2974, 0.2740, 0.3293, 0.1493, 0.2082,\n",
      "        0.1880, 0.5309, 0.4044, 0.2219, 0.6236, 0.3326, 0.5563, 0.4779, 0.2093,\n",
      "        0.3359, 0.4560, 0.3154, 0.1685, 0.2276, 0.3306]), tensor([0.3506, 0.5309, 0.4188, 0.4401, 0.4321, 0.3483, 0.1340, 0.2078, 0.4100,\n",
      "        0.3321, 0.1421, 0.1920, 0.2984, 0.4958, 0.3996, 0.5185, 0.1127, 0.2269,\n",
      "        0.2289, 0.2757, 0.3700, 0.4400, 0.2758, 0.1490, 0.2674, 0.1495, 0.2534,\n",
      "        0.6145, 0.4260, 0.3117, 0.2260, 0.2089, 0.2616]), tensor([0.3969, 0.3785, 0.4491, 0.1686, 0.2401, 0.3328, 0.3166, 0.2479, 0.3861,\n",
      "        0.3031, 0.1710, 0.1111, 0.3310, 0.3398, 0.2389, 0.4282, 0.1425, 0.3949,\n",
      "        0.4102, 0.5974, 0.2108, 0.2526, 0.2603, 0.3270, 0.4668, 0.2865, 0.1070,\n",
      "        0.4013, 0.4749, 0.1979, 0.1413, 0.4396, 0.1722]), tensor([0.2384, 0.6231, 0.1592, 0.3398, 0.6056, 0.4449, 0.4597, 0.1113, 0.2480,\n",
      "        0.3490, 0.2857, 0.1778, 0.5434, 0.6054, 0.3140, 0.1457, 0.0642, 0.1888,\n",
      "        0.1897, 0.2933, 0.2980, 0.5770, 0.2298, 0.1064, 0.4771, 0.3256, 0.2225,\n",
      "        0.5109, 0.5474, 0.2437, 0.2453, 0.2870, 0.6033]), tensor([0.2866, 0.4254, 0.5497, 0.1740, 0.2609, 0.3660, 0.5374, 0.4354, 0.2940,\n",
      "        0.2846, 0.0775, 0.1219, 0.2392, 0.3692, 0.3036, 0.4222, 0.2782, 0.3369,\n",
      "        0.3823, 0.3669, 0.3061, 0.3454, 0.2670, 0.5136, 0.1309, 0.4703, 0.1158,\n",
      "        0.3364, 0.2206, 0.3261, 0.2142, 0.6211, 0.6499]), tensor([0.1939, 0.2805, 0.1908, 0.3286, 0.2974, 0.3466, 0.2147, 0.1133, 0.2814,\n",
      "        0.3059, 0.2034, 0.2028, 0.4772, 0.5056, 0.1247, 0.3580, 0.1224, 0.2171,\n",
      "        0.0825, 0.3385, 0.2792, 0.1249, 0.4014, 0.2510, 0.5738, 0.2456, 0.4582,\n",
      "        0.1873, 0.3202, 0.4611, 0.3716, 0.2971, 0.2800])], [tensor([0.1927, 0.2688, 0.3923, 0.1395, 0.2267, 0.2703, 0.3268, 0.3772, 0.2816,\n",
      "        0.3916, 0.1164, 0.2306, 0.2922, 0.3634, 0.3451, 0.4604, 0.1307, 0.3077,\n",
      "        0.1382, 0.3035, 0.3261, 0.5085, 0.3454, 0.3485, 0.3962, 0.3106, 0.1700,\n",
      "        0.3380, 0.2143, 0.4472, 0.3514, 0.4086, 0.4035]), tensor([0.2115, 0.3153, 0.4311, 0.1250, 0.1875, 0.2884, 0.1771, 0.4743, 0.4737,\n",
      "        0.2882, 0.1562, 0.2295, 0.4607, 0.4644, 0.3206, 0.5287, 0.1873, 0.2223,\n",
      "        0.2209, 0.1535, 0.3599, 0.3119, 0.4201, 0.4409, 0.4012, 0.1049, 0.4857,\n",
      "        0.2826, 0.2650, 0.3488, 0.2094, 0.3815, 0.2645]), tensor([0.2542, 0.3452, 0.3832, 0.1995, 0.4669, 0.4761, 0.1506, 0.2723, 0.1257,\n",
      "        0.1758, 0.2152, 0.1149, 0.3851, 0.3082, 0.2989, 0.3623, 0.1287, 0.3164,\n",
      "        0.1370, 0.2936, 0.2570, 0.1884, 0.4658, 0.1645, 0.2510, 0.4969, 0.2049,\n",
      "        0.3504, 0.4788, 0.1660, 0.2385, 0.1877, 0.3519]), tensor([0.2702, 0.4172, 0.4231, 0.2755, 0.2259, 0.4392, 0.2383, 0.3402, 0.0890,\n",
      "        0.5183, 0.2733, 0.2509, 0.2292, 0.3987, 0.3194, 0.4068, 0.1443, 0.3108,\n",
      "        0.1143, 0.1847, 0.3294, 0.4776, 0.2681, 0.2015, 0.2220, 0.4101, 0.2719,\n",
      "        0.1162, 0.4325, 0.4286, 0.2456, 0.5250, 0.3875]), tensor([0.2594, 0.4130, 0.2624, 0.3373, 0.3274, 0.4299, 0.4019, 0.3048, 0.3071,\n",
      "        0.5387, 0.1896, 0.1862, 0.2115, 0.5000, 0.3000, 0.3925, 0.1217, 0.1327,\n",
      "        0.1001, 0.3563, 0.4928, 0.4799, 0.2230, 0.2351, 0.2379, 0.4944, 0.0863,\n",
      "        0.3264, 0.6284, 0.2666, 0.2384, 0.2896, 0.3700]), tensor([0.3011, 0.3776, 0.6755, 0.2593, 0.3428, 0.2756, 0.3519, 0.3969, 0.3737,\n",
      "        0.3608, 0.1547, 0.1733, 0.4643, 0.4023, 0.4179, 0.4418, 0.1125, 0.3589,\n",
      "        0.4455, 0.2210, 0.2047, 0.2933, 0.2014, 0.4883, 0.1538, 0.3425, 0.1636,\n",
      "        0.3952, 0.2880, 0.2691, 0.3066, 0.3788, 0.4901]), tensor([0.3212, 0.3139, 0.2758, 0.2598, 0.4062, 0.4744, 0.2556, 0.1411, 0.2758,\n",
      "        0.3614, 0.2195, 0.2891, 0.5946, 0.1916, 0.0608, 0.3540, 0.2105, 0.2742,\n",
      "        0.1906, 0.4955, 0.2146, 0.2150, 0.4455, 0.3746, 0.5751, 0.1686, 0.2875,\n",
      "        0.2212, 0.2980, 0.4139, 0.3163, 0.3501, 0.4539]), tensor([0.1681, 0.2582, 0.3544, 0.2327, 0.3356, 0.4332, 0.1964, 0.1695, 0.3025,\n",
      "        0.3787, 0.1651, 0.3843, 0.3246, 0.2627, 0.2597, 0.4059, 0.1050, 0.2287,\n",
      "        0.1361, 0.5230, 0.3091, 0.3394, 0.5612, 0.2920, 0.5162, 0.4334, 0.2200,\n",
      "        0.3648, 0.4996, 0.3776, 0.2179, 0.2597, 0.2244]), tensor([0.3450, 0.4683, 0.4209, 0.4544, 0.4448, 0.3846, 0.1192, 0.2095, 0.4397,\n",
      "        0.3449, 0.1583, 0.1868, 0.2826, 0.5262, 0.4727, 0.5683, 0.1197, 0.2629,\n",
      "        0.2206, 0.2844, 0.3654, 0.4668, 0.2765, 0.1470, 0.2487, 0.1422, 0.2571,\n",
      "        0.6049, 0.4987, 0.3485, 0.2478, 0.2115, 0.2608]), tensor([0.3444, 0.2602, 0.4713, 0.1841, 0.2454, 0.3967, 0.2817, 0.2570, 0.4152,\n",
      "        0.3601, 0.1985, 0.1244, 0.2821, 0.3591, 0.2916, 0.4167, 0.1731, 0.4623,\n",
      "        0.4164, 0.5397, 0.1492, 0.2653, 0.2353, 0.2848, 0.3867, 0.3266, 0.1638,\n",
      "        0.4315, 0.5607, 0.2380, 0.1536, 0.4287, 0.1440]), tensor([0.2382, 0.7032, 0.2467, 0.3048, 0.4046, 0.4257, 0.4802, 0.1102, 0.2497,\n",
      "        0.3852, 0.3101, 0.1844, 0.5165, 0.6364, 0.3898, 0.1761, 0.0503, 0.1452,\n",
      "        0.2984, 0.2867, 0.3142, 0.6358, 0.2199, 0.1219, 0.3534, 0.4394, 0.2580,\n",
      "        0.3873, 0.6186, 0.2361, 0.2382, 0.2413, 0.5709]), tensor([0.2810, 0.4167, 0.5566, 0.1648, 0.2719, 0.3947, 0.4775, 0.4152, 0.2931,\n",
      "        0.3172, 0.0859, 0.1269, 0.2353, 0.4431, 0.3033, 0.4567, 0.2730, 0.3255,\n",
      "        0.3824, 0.3463, 0.3088, 0.3863, 0.2215, 0.5172, 0.1226, 0.4798, 0.1147,\n",
      "        0.3011, 0.2288, 0.3164, 0.2202, 0.6129, 0.6331]), tensor([0.2034, 0.2497, 0.1872, 0.2943, 0.2796, 0.4120, 0.1979, 0.1365, 0.3049,\n",
      "        0.3120, 0.2019, 0.1953, 0.4523, 0.5433, 0.1602, 0.4199, 0.1270, 0.2670,\n",
      "        0.0817, 0.3463, 0.2808, 0.1430, 0.3658, 0.2301, 0.5017, 0.2339, 0.4868,\n",
      "        0.1682, 0.3708, 0.4514, 0.3855, 0.3097, 0.2907])]]\n",
      "k='phon_vecs', v=[[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0])]]\n",
      "k='phon_tokens', v=[[tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([ 1, 32]), tensor([ 1,  9, 21, 25]), tensor([ 1,  2, 12, 32]), tensor([12, 23, 24]), tensor([22, 24, 25, 28]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([33]), tensor([15]), tensor([25]), tensor([19, 32]), tensor([13, 21, 28]), tensor([ 2, 12, 18, 23]), tensor([12, 22, 24]), tensor([19, 22, 24]), tensor([ 1, 15, 27]), tensor([19]), tensor([ 1,  4, 12, 13, 21, 27, 28, 32]), tensor([ 2,  6, 23, 31, 32]), tensor([13, 24])], [tensor([31]), tensor([21]), tensor([15]), tensor([33]), tensor([ 9, 31]), tensor([ 9, 13, 28]), tensor([2]), tensor([12, 24]), tensor([19, 22, 24]), tensor([13, 15, 27]), tensor([19, 28]), tensor([ 1, 12, 13, 21, 28, 32]), tensor([ 2, 23, 31, 32]), tensor([13, 24])]]\n",
      "k='global_encoding', v=tensor([[[-0.0475,  1.0735, -0.7243, -1.5423,  0.1627, -0.5785, -0.5002,\n",
      "          -0.8544, -0.7752,  0.2827,  0.1948,  1.1737, -0.7207,  0.9085,\n",
      "           0.0473, -1.3582,  1.6668,  1.3668, -2.0655,  0.1601, -0.5513,\n",
      "          -1.0733,  1.9325,  0.3773,  0.4145,  1.8964, -1.8257, -1.1589,\n",
      "          -0.7522, -0.8853, -0.3474, -2.0801,  0.1027,  0.6037,  2.1823,\n",
      "           0.5036,  2.4690, -0.4255,  1.2409,  1.4465,  0.0657,  1.3383,\n",
      "          -0.0280,  0.1803, -0.3264, -0.5709, -1.0649,  0.2825, -0.4481,\n",
      "          -0.2406,  0.7868, -0.4191,  2.0216,  0.0161,  1.0645, -0.3183,\n",
      "           1.0668,  0.1895, -0.1781, -1.4853, -0.1615, -0.4958, -1.2667,\n",
      "          -1.6275]],\n",
      "\n",
      "        [[-0.6820,  1.0408, -0.1115, -1.9005,  0.1152, -0.7176, -1.3335,\n",
      "          -0.4833, -0.9931,  0.3927,  0.6460,  0.8859, -1.0316,  0.5731,\n",
      "           0.0413, -1.4394,  1.7946,  0.6330, -1.8726,  0.4899, -0.1981,\n",
      "          -0.7284,  2.0143,  0.7108,  0.1771,  1.6771, -2.0169, -1.4974,\n",
      "          -0.6459, -1.2819, -0.2453, -1.6290,  0.0124,  1.0724,  1.6055,\n",
      "           0.9247,  2.6309, -0.1932,  0.9612,  1.4457, -0.0919,  1.0950,\n",
      "           0.7190,  0.5222, -0.9158, -0.8882, -1.1981, -0.4007, -0.3721,\n",
      "           0.1302,  0.8454, -0.5693,  1.9083,  0.6473,  1.3245, -0.4616,\n",
      "           0.9313,  0.5217, -0.3303, -1.2056,  0.0430, -0.7909, -0.9329,\n",
      "          -1.0661]],\n",
      "\n",
      "        [[-0.6708, -0.1183, -0.2989, -2.3009, -0.2367, -1.1155, -1.3181,\n",
      "          -1.3499, -0.7181,  0.0835,  0.3434,  0.9516, -1.2464,  1.1506,\n",
      "           0.1378,  0.0296,  1.6046,  0.9226, -1.1051,  0.2239, -0.3645,\n",
      "          -0.6309,  1.3388,  0.8605,  0.4422,  2.1382, -2.0048, -1.4456,\n",
      "          -0.9979, -0.8592,  0.1544, -1.5779, -0.1854,  0.2773,  1.0727,\n",
      "           0.6830,  2.9244, -0.8900,  0.9499,  1.7374,  0.0680,  0.8777,\n",
      "           0.8074,  0.8912, -1.6307, -1.1347, -0.8156,  0.1222, -0.4703,\n",
      "          -0.0401,  0.6512, -0.2282,  1.8925,  0.3248,  1.5418,  0.0230,\n",
      "           1.0452,  0.3192,  0.1282, -0.9620,  0.7119, -0.5761,  0.0083,\n",
      "          -1.8331]]])\n"
     ]
    }
   ],
   "source": [
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "\n",
    "chkpt = pt.load(checkpoints[-1])\n",
    "model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "\n",
    "datum = ds.character_tokenizer.encode([\"aspfdjhqwrpiv\", \"hiccup\", \"h\"])\n",
    "print(f\"{datum['enc_input_ids']=}\")\n",
    "print(f\"{datum['enc_pad_mask']=}\")\n",
    "\n",
    "pred = model.generate(\n",
    "    \"o2p\",\n",
    "    datum[\"enc_input_ids\"],\n",
    "    datum[\"enc_pad_mask\"],\n",
    "    None,\n",
    "    None,\n",
    "    deterministic=True,\n",
    ")\n",
    "for k, v in pred.items():\n",
    "    print(f\"{k=}, {v=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take all orthographic words from both WJ3 and the\n",
    "# original dataset and remove duplicates.\n",
    "all_words = set(wj3_json.keys())\n",
    "all_words.update(ds.words)  # Take the union of both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"wj3_predictions.xlsx\", engine=\"openpyxl\")\n",
    "for checkpoint in tqdm.tqdm(checkpoints):\n",
    "    # Empty the dataframe to prepare for the next checkpoint (aka excel page)\n",
    "    df.drop(df.index, inplace=True)\n",
    "    chkpt = pt.load(checkpoint)\n",
    "    model = chkpt[\"model\"]\n",
    "    # In this loop, orth is the orthographic form of the target word and\n",
    "    # phon is the phonological form of the target word (not phonetic vectors)\n",
    "    for orth in all_words:\n",
    "        new_row = {}\n",
    "        # Since all_words is a union of both WJ3 and the original dataset,\n",
    "        # the phonological form of any orth in all_words is in either WJ3 or\n",
    "        # the original cmudict dataset. We check both below\n",
    "        in_wj3 = orth in wj3_json.keys()\n",
    "        new_row[\"in_wj3\"] = in_wj3\n",
    "        in_traindata = orth in ds.words\n",
    "        new_row[\"in_traindata\"] = in_traindata\n",
    "        if in_traindata:\n",
    "            phon = ds.cmudict[orth]\n",
    "        else:\n",
    "            phon = wj3_json[orth]\n",
    "\n",
    "        datum = ds.character_tokenizer.encode(orth)\n",
    "        pred = chkpt[\"model\"].generate(\n",
    "            \"o2p\",\n",
    "            datum[\"enc_input_ids\"],\n",
    "            datum[\"enc_pad_mask\"],\n",
    "            datum[\"enc_input_ids\"],\n",
    "            datum[\"enc_pad_mask\"],\n",
    "            deterministic=True,\n",
    "        )\n",
    "        # Save the original input orthography\n",
    "        new_row[\"word_raw\"] = orth\n",
    "        # Save the target phonology for the above input orthography\n",
    "        new_row[\"phon_target\"] = \":\".join(phon)\n",
    "        # Remove the start and end tokens from each phonological vector\n",
    "        # and convert them from tensors to lists\n",
    "        phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][1:-1]]\n",
    "        # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "        phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "            phon_pred_features, phonology=True, hot_nodes=True\n",
    "        )\n",
    "        # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "        # separated by colons\n",
    "        phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "        new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "        # Save a boolean indicating whether the prediction was correct\n",
    "        new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "        # Save the phonological features for the target phonology\n",
    "        phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "        if phon_target_features:\n",
    "            phon_target_features = \";\".join(\n",
    "                [\n",
    "                    \":\".join([str(v.item()) for v in vector])\n",
    "                    for vector in phon_target_features[\"targets\"][0]\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            phon_target_features = \"None\"\n",
    "        new_row[\"phon_target_features\"] = phon_target_features\n",
    "        # Save the phonological features for the predicted phonology\n",
    "        phon_prediction_features = \";\".join(\n",
    "            [\n",
    "                \":\".join([str(int(v.item())) for v in vector])\n",
    "                for vector in pred[\"phon_vecs\"][1:-1]\n",
    "            ]\n",
    "        )\n",
    "        new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "        # Save the phonological probabilities for the predicted phonology\n",
    "        phon_prediction_probabilities = \";\".join(\n",
    "            [\n",
    "                \":\".join([str(v.item()) for v in vector])\n",
    "                for vector in pred[\"phon_probs\"][1:-1]\n",
    "            ]\n",
    "        )\n",
    "        new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "        # Save the global encoding vector for the predicted phonology\n",
    "        global_encoding = \":\".join(\n",
    "            [str(v.item()) for v in pred[\"global_encoding\"].squeeze()]\n",
    "        )\n",
    "        new_row[\"global_encoding\"] = global_encoding\n",
    "        new_row = pd.Series(new_row)\n",
    "        new_row = new_row.to_frame().transpose()\n",
    "        df = pd.concat([df, new_row])\n",
    "    df.to_excel(writer, sheet_name=\"epoch \" + checkpoint[-7:-4])\n",
    "writer.book.save(\"wj3_predictions.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
