{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model from checkpoints\n",
    "This notebook allows you to take checkpoints from a trained model and make predictions for a set of words for each checkpoint. The data are written to CSV, one for each checkpoint (where a checkpoint corresponds to an epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "cmu = nltk.corpus.cmudict.dict()\n",
    "import os\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-aggregate the word data\n",
    "Each of these models was trained with a different dataset, all contained in `data/SSSR/`. For the frequencies for each of those sets for the purposes of analysis, we can reference those files individually later. Right now, we just need the words. To make things simpler we will read in all of them and get only the unique words, rather than testing on all the datasets individually, which would be onerous because there are several different sets, they contain repeated words across sets and repreated words within any given set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"my_sidewalks_75_percent_background_25_percent\"\n",
    "CONDITION = \"my_sidewalks\"\n",
    "words = []\n",
    "\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "    if filename.startswith(CONDITION) & filename.endswith('.csv'):\n",
    "\n",
    "        FILEPATH = os.path.join(DIRECTORY, filename)\n",
    "        # read the .csv file into a pandas DataFrame and store it in the dictionary\n",
    "        words.extend(pd.read_csv(FILEPATH)['word_raw'].tolist())\n",
    "\n",
    "words = [word for word in words if isinstance(word, str) and word in cmu.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate the Woodcock words and use those for testing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxorth = max([len(word) for word in words])\n",
    "maxphon = max([len(cmu[word][0]) for word in words])\n",
    "\n",
    "with open('data/wj_iii_form_a.json', 'r') as file:\n",
    "    wj3 = json.load(file)\n",
    "\n",
    "wj3 = [word for word in wj3.keys() if len(word) > 1 and word in cmu.keys()]\n",
    "\n",
    "with open('data/TOWRE2_sight_words.json', 'r') as file:\n",
    "    towre = json.load(file)\n",
    "\n",
    "towre = [word for word in towre.keys() if len(word) > 1 and word in cmu.keys()]\n",
    "\n",
    "\n",
    "\n",
    "for word in wj3:\n",
    "    if len(word) <= maxorth & len(cmu[word][0]) <= maxphon:\n",
    "        words.extend(word)\n",
    "for word in towre:\n",
    "    if len(word) <= maxorth & len(cmu[word][0]) <= maxphon:\n",
    "        words.extend(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create wordlist to write to file for config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(list(set(words)))\n",
    "OUTFILE = os.path.join(DIRECTORY, \"words_for_test.csv\")\n",
    "\n",
    "with open(OUTFILE, 'w') as file:\n",
    "    file.write(\"word_raw\\n\")\n",
    "    for word in words:\n",
    "        file.write('{}\\n'.format(word))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Load in a dataset to Traindata class which is embedded inside the phonology tokenizer. Later implementations should consider breaking this process apart such that Traindata is initialized prior to tokenization. Also establish batches: larger batches make for faster processing, but your machine may impose an upper limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade books\n",
    "Work through each trade books directory and generate model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = type(\"config\",\n",
    "    (object,),\n",
    "    {\"dataset_filename\": Path(OUTFILE)}, )\n",
    "ds = ConnTextULDataset(config)\n",
    "\n",
    "checkpoints = []\n",
    "\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "\n",
    "    if filename.endswith(\".pth\"):\n",
    "        checkpoints.append(filename)\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "all_words=set()\n",
    "all_words.update(ds.words)\n",
    "batches = [list(all_words)[i:i+batch_size] for i in range(0, len(all_words), batch_size)]\n",
    "checkpoints.sort()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_substring(x, y, target = '.csv'):\n",
    "    i = x.find(target)\n",
    "    if i == -1:\n",
    "        return x\n",
    "    else:\n",
    "        return x[:i] + y + x[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_epochs': 150,\n",
       " 'batch_size_train': 32,\n",
       " 'batch_size_val': 64,\n",
       " 'd_model': 128,\n",
       " 'd_embedding': 2,\n",
       " 'nhead': 2,\n",
       " 'pathway': 'o2p',\n",
       " 'test': False,\n",
       " 'train_test_split': 1.0,\n",
       " 'save_every': 10,\n",
       " 'seed': 2025,\n",
       " 'learning_rate': 0.001,\n",
       " 'project': 'SSSR2024',\n",
       " 'wandb': True,\n",
       " 'num_phon_enc_layers': 1,\n",
       " 'num_orth_enc_layers': 1,\n",
       " 'num_mixing_enc_layers': 1,\n",
       " 'num_phon_dec_layers': 1,\n",
       " 'num_orth_dec_layers': 1,\n",
       " 'device': 'cpu',\n",
       " 'model_path': '/workspaces/BRIDGE/./models',\n",
       " 'dataset_filename': '/workspaces/BRIDGE/data/my_sidewalks_75_percent_background_25_percent.csv',\n",
       " 'max_nb_steps': 20,\n",
       " 'sweep_filename': '',\n",
       " 'test_filenames': ['/workspaces/BRIDGE/data/tests/test1.csv',\n",
       "  '/workspaces/BRIDGE/data/tests/test2.csv'],\n",
       " 'model_id': 'root_2024-05-24_01h23m08973ms',\n",
       " 'model_file_name': 'root_2024-05-24_01h23m08973ms_chkpt000.pth',\n",
       " 'n_steps_per_epoch': 311}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_2024-05-24_01h23m08973ms_chkpt001.pth arrived\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tsize mismatch for orth_position_embedding.weight: copying a param with shape torch.Size([13, 128]) from checkpoint, the shape in current model is torch.Size([15, 128]).\n\tsize mismatch for phon_position_embedding.weight: copying a param with shape torch.Size([11, 128]) from checkpoint, the shape in current model is torch.Size([15, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m dfa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphon_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(AttrDict(chkpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]), ds)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchkpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     17\u001b[0m dl \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tsize mismatch for orth_position_embedding.weight: copying a param with shape torch.Size([13, 128]) from checkpoint, the shape in current model is torch.Size([15, 128]).\n\tsize mismatch for phon_position_embedding.weight: copying a param with shape torch.Size([11, 128]) from checkpoint, the shape in current model is torch.Size([15, 128])."
     ]
    }
   ],
   "source": [
    "for checkpoint in tqdm.tqdm(checkpoints):\n",
    "    print(checkpoint, \"arrived\")\n",
    "    \n",
    "    outfile = checkpoint.replace(\".pth\", \".csv\")\n",
    "    outpath = os.path.join(DIRECTORY, outfile)\n",
    "\n",
    "    outpath2 = insert_substring(outpath, \"_from_units\")\n",
    "\n",
    "    outfile2 = open(outpath2, \"w\")\n",
    "\n",
    "    chkpt = torch.load(os.path.join(DIRECTORY, checkpoint))\n",
    "    dfa = pd.DataFrame(columns=[\"phon_prediction\"])\n",
    "    model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "    model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dl = []\n",
    "\n",
    "    start_row = 0  # Initialize starting row for each new sheet\n",
    "\n",
    "    print(\"Checkpoint\", checkpoint, \"...started\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(batches)):\n",
    "            batch = batches[batch_idx]\n",
    "            new_row = {}\n",
    "            datum = ds.character_tokenizer.encode(batch)\n",
    "            pred = model.generate(\n",
    "                \"o2p\",\n",
    "                datum['enc_input_ids'],\n",
    "                datum['enc_pad_mask'],\n",
    "                None,\n",
    "                None,\n",
    "                deterministic=True,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for idx, orth in enumerate(batch):\n",
    "                # Save the original input orthography\n",
    "                new_row[\"word_raw\"] = orth\n",
    "                # Save the target phonology for the above input orthography\n",
    "                phon = ds.cmudict[orth]\n",
    "                new_row[\"phon_target\"] = \":\".join(phon)\n",
    "                # Remove the start and end tokens from each phonological vector\n",
    "                # and convert them from tensors to lists\n",
    "                phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][idx][1:-1]]\n",
    "                # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "                phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "                    phon_pred_features, phonology=True, hot_nodes=True\n",
    "                )\n",
    "                # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "                # separated by colons\n",
    "                phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "                new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "                # Save a boolean indicating whether the prediction was correct\n",
    "                new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "                # Save the phonological features for the target phonology\n",
    "                phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "                if phon_target_features:\n",
    "                    phon_target_features = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(v.item()) for v in vector])\n",
    "                            for vector in phon_target_features[\"targets\"][0]\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    phon_target_features = \"None\"\n",
    "                new_row[\"phon_target_features\"] = phon_target_features\n",
    "                # Save the phonological features for the predicted phonology\n",
    "                phon_prediction_features = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(int(v.item())) for v in vector])\n",
    "                        for vector in pred[\"phon_vecs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "                # Save the phonological probabilities for the predicted phonology\n",
    "                phon_prediction_probabilities = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(v.item()) for v in vector])\n",
    "                        for vector in pred[\"phon_probs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "                # Save the global encoding vector for the predicted phonology\n",
    "                global_encoding = \";\".join(\n",
    "                    \n",
    "                        [\n",
    "                        \":\".join([str(v.item()) for v in vector]) \n",
    "                        for vector in pred[\"global_encoding\"][idx]\n",
    "                        ]\n",
    "                    \n",
    "                    )\n",
    "                new_row[\"global_encoding\"] = global_encoding\n",
    "\n",
    "                dfa = pd.DataFrame([new_row])\n",
    "                dl.append(dfa)\n",
    "\n",
    "                # calculate accuracies from units rather than string predictions\n",
    "\n",
    "                by_phoneme = []\n",
    "                by_unit = []\n",
    "            \n",
    "                target = ds.phonology_tokenizer.encode([orth])['targets'][0].tolist()\n",
    "                prediction = [e.tolist() for e in pred['phon_vecs'][idx]]\n",
    "                \n",
    "                for i, e in enumerate(target):\n",
    "                    by_phoneme.append(e == prediction[i])    \n",
    "                    by_unit.extend([u[0] == u[1] for u in zip(prediction[i], e)])\n",
    "                    \n",
    "                phonemes_correct_total = sum(by_phoneme)\n",
    "                phonemes_correct_mean = sum(by_phoneme)/len(by_phoneme)\n",
    "                units_correct_mean = sum(by_unit)/len(by_unit)\n",
    "\n",
    "                outfile2.write(\"{}, {}, {}, {}\\n\".format(orth, phonemes_correct_total, phonemes_correct_mean, units_correct_mean))\n",
    "\n",
    "\n",
    "\n",
    "                print(\"Batch\", batch_idx, \"of\", len(batches), \"...done\")\n",
    "        pd.concat(dl).to_csv(outpath, index=False)\n",
    "        outfile2.close()\n",
    "        print(\"Checkpoint done:\", checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
