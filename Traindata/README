Orthography-to-phonlogy networks require training data that
has some special care taken to consider aspects of phonological
and orthgraphic processing when specifying to the training 
representations used during learning. This repo provides some
functionality to that end.


Phonology
---------
The phonological structure contained/ assumed in Traindata is
opinionated and is based on the ARPAbet. For more information
on this representational scheme see below. The specific version
of this scheme is maintained locally in this repository, but is
derivative of what is contained in the MCB repository linked below.

ARPAbet Documentation:
https://en.wikipedia.org/wiki/ARPABET
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
https://github.com/MCooperBorkenhagen/ARPAbet

The phonological representations are extensions of previous
connectionist implementations, namely from Harm & Seidenberg (1999).
In essence, the classical distinctive features framework is used,
originally formulated in Chomsky and Halle (1968). Originally,
the framework used in this repository codes for binary features,
but other codings are possible.


Orthography
-----------
The orthographic representations used here assume a one-hot
coding over letters, with each represented fundamentally as 
a 26-unit vector where the nth element in the vector corresponds
to the position of that letter in the alphabet. Originally, the
letters specified are all lowercase, but that could change in
the future.

Terminal Segments
-----------------
An important detail about the way that orthography and phonology
are represented in the method contained here concerns terminal
segments. When processing time-varying representations of language,
including words, the boundaries of any given language form (sentence,
word, etc.) often needs to be explicitly identified. The featural
representations contained here allow for this, with a feature
representing the start of the word (labeled SOS) and a feature
encoding the end of the word (labeled EOS). The labels "SOS" and
"EOS" are adopted from the machine learning literature and implementations
of sequential ANNs. 