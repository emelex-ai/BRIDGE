{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model from checkpoints\n",
    "This notebook allows you to take checkpoints from a trained model and make predictions for a set of words for each checkpoint. The data are written to CSV, one for each checkpoint (where a checkpoint corresponds to an epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import tqdm\n",
    "from src.model import Model\n",
    "from addict import Dict as AttrDict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "cmu = nltk.corpus.cmudict.dict()\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-aggregate the word data\n",
    "Each of these models was trained with a different dataset, all contained in `data/SSSR/`. For the frequencies for each of those sets for the purposes of analysis, we can reference those files individually later. Right now, we just need the words. To make things simpler we will read in all of them and get only the unique words, rather than testing on all the datasets individually, which would be onerous because there are several different sets, they contain repeated words across sets and repreated words within any given set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR = \"data/SSSR2024/\"\n",
    "words = []\n",
    "\n",
    "for filename in os.listdir(DIR):\n",
    "    if filename.endswith('.csv'):\n",
    "\n",
    "        filepath = os.path.join(DIR, filename)\n",
    "        # read the .csv file into a pandas DataFrame and store it in the dictionary\n",
    "        words.extend(pd.read_csv(filepath, header=None)[0].tolist())\n",
    "\n",
    "words = [word for word in words if isinstance(word, str)]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "with open('data/SSSR2024/all_input_words.csv', 'w') as f:\n",
    "    f.write(\"word_raw\\n\")\n",
    "    for word in words:\n",
    "        f.write(\"{}\\n\".format(word))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate the Woodcock words and use those for testing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/wj_iii_form_a.json', 'r') as file:\n",
    "    wj3 = json.load(file)\n",
    "\n",
    "wj3 = [word for word in wj3.keys() if len(word) > 1 and word in cmu.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Load in a dataset to Traindata class which is embedded inside the phonology tokenizer. Later implementations should consider breaking this process apart such that Traindata is initialized prior to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trade_books_75_percent_background_25_percent',\n",
       " 'trade_books_25_percent_background_75_percent',\n",
       " 'trade_books_100_percent',\n",
       " 'trade_books_50_percent_background_50_percent',\n",
       " 'trade_books_weighted_sample',\n",
       " 'trade_books_0_percent_background_100_percent']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(trade_book_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH ='models/SSSR2024/trade_books/trade_books_0_percent_background_100_percent/trade_books_0_percent_background_100_percent.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/SSSR2024/trade_books/trade_books_0_percent_background_100_percent/trade_books_0_percent_background_100_percent.csv'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'root_2024-05-24_02h27m23023ms_chkpt111.pth'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(trade_book_directories, dir))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['root_2024-05-24_03h25m28092ms_chkpt131.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt101.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt091.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt031.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt021.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt051.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt111.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt001.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt041.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt011.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt081.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt071.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "orthpad changed to 0 because onehot encodings were selected for orthography\n",
      "Representations initialized. Done.\n",
      "[]\n",
      "['root_2024-05-24_03h25m28092ms_chkpt141.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt121.pth']\n",
      "['root_2024-05-24_03h25m28092ms_chkpt061.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt091.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt011.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt041.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt061.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt121.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt081.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt031.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt001.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "[]\n",
      "['root_2024-05-24_02h41m32173ms_chkpt131.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt051.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt071.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt111.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt101.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt021.pth']\n",
      "['root_2024-05-24_02h41m32173ms_chkpt141.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt031.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt021.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "[]\n",
      "['root_2024-05-24_02h15m03225ms_chkpt131.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt051.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt041.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt001.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt091.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt071.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt111.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt121.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt081.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt141.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt061.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt101.pth']\n",
      "['root_2024-05-24_02h15m03225ms_chkpt011.pth']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt051.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt141.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt121.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt061.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt091.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt031.pth']\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt071.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt131.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt111.pth']\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt021.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt041.pth']\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt011.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt081.pth']\n",
      "['root_2024-05-24_03h05m33587ms_chkpt101.pth']\n",
      "[]\n",
      "['root_2024-05-24_03h05m33587ms_chkpt001.pth']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['root_2024-05-24_02h01m31403ms_chkpt031.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "[]\n",
      "['root_2024-05-24_02h01m31403ms_chkpt011.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt131.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt101.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt111.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt071.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt081.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt001.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt041.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt051.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt061.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt141.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt021.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt091.pth']\n",
      "['root_2024-05-24_02h01m31403ms_chkpt121.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt111.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt101.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt051.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt061.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt091.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt021.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt141.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt001.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt121.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt041.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt071.pth']\n",
      "Cache folder: /workspaces/BRIDGE/data/.cache already exists\n",
      "[]\n",
      "['root_2024-05-24_02h27m23023ms_chkpt011.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt081.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt031.pth']\n",
      "['root_2024-05-24_02h27m23023ms_chkpt131.pth']\n"
     ]
    }
   ],
   "source": [
    "trade_book_directories = \"models/SSSR2024/trade_books\"\n",
    "\n",
    "for dir in os.listdir(trade_book_directories):\n",
    "    filenames = os.listdir(os.path.join(trade_book_directories, dir))\n",
    "    for filename in filenames:\n",
    "        checkpoints = []\n",
    "        FILEPATH = os.path.join(trade_book_directories, dir, filename)\n",
    "\n",
    "\n",
    "\n",
    "        if filename.startswith(\"trade_books\") & filename.endswith(\".csv\"):\n",
    "\n",
    "            words = []\n",
    "            words.extend(pd.read_csv(FILEPATH)['word_raw'].tolist())\n",
    "            maxorth = max([len(word) for word in words])\n",
    "            maxphon = max([len(cmu[word][0]) for word in words])\n",
    "            for word in wj3:\n",
    "                if len(word) <= maxorth & len(cmu[word][0]) <= maxphon:\n",
    "                    words.extend(word)\n",
    "\n",
    "            words = sorted(list(set(words)))\n",
    "            outfile = os.path.join(trade_book_directories, dir, \"words_for_test.csv\")\n",
    "            with open(outfile, 'w') as file:\n",
    "                file.write(\"word_raw\\n\")\n",
    "                for word in words:\n",
    "                    file.write('{}\\n'.format(word))\n",
    "            file.close()\n",
    "            config = type(\"config\",\n",
    "                (object,),\n",
    "                {\"dataset_filename\": Path(outfile)}, )\n",
    "            ds = ConnTextULDataset(config)\n",
    "        if filename.endswith(\".pth\"):\n",
    "               checkpoints.append(filename)\n",
    "        checkpoints.sort()\n",
    "        print(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in checkpoints\n",
    "We nee the checkpoint file names in order to iterate through them, load and predict. Note that the data are written back in the same location from which the checkpoints are read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints=['models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt001.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt011.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt021.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt031.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt041.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt051.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt061.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt071.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt081.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt091.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt101.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt111.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt121.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt131.pth', 'models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt141.pth']\n"
     ]
    }
   ],
   "source": [
    "#PATH = \"models/modelresults59355/root_2024-04-24_15h38m59355ms_chkpt*.pth\"\n",
    "PATH = \"models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt*.pth\"\n",
    "checkpoints = glob.glob(PATH)\n",
    "checkpoints.sort()\n",
    "print(f\"{checkpoints=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish batches\n",
    "Larger batches make for faster processing, but your machine may impose an upper limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_words=set()\n",
    "all_words.update(ds.words)\n",
    "batches = [list(all_words)[i:i+batch_size] for i in range(0, len(all_words), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions\n",
    "Iterate through and generate, write predictions for the words you've initialized in `config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:408: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt001.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:09<02:18,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt001.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt011.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:17<01:52,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt011.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt021.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:24<01:36,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt021.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt031.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:33<01:29,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt031.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt041.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:41<01:21,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt041.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt051.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:49<01:12,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt051.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt061.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:57<01:05,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt061.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt071.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [01:05<00:57,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt071.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt081.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [01:13<00:48,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt081.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt091.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [01:21<00:39,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt091.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt101.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [01:29<00:31,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt101.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt111.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [01:37<00:24,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt111.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt121.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [01:48<00:17,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt121.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt131.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [01:58<00:09,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt131.pth\n",
      "Checkpoint models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt141.pth ...started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/BRIDGE/src/model.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gen_phon_tokes.append(torch.tensor(new_phon_tokes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 2 ...done\n",
      "Batch 1 of 2 ...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:06<00:00,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint done: models/SSSR2024/trade_books/fifty_percent/root_2024-05-24_03h05m33587ms_chkpt141.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for checkpoint in tqdm.tqdm(checkpoints):\n",
    "\n",
    "    outfile = checkpoint.replace(\".pth\", \".csv\")\n",
    "\n",
    "    chkpt = torch.load(checkpoint)\n",
    "    dfa = pd.DataFrame(columns=[\"phon_prediction\"])\n",
    "    model = Model(AttrDict(chkpt[\"config\"]), ds)\n",
    "    model.load_state_dict(chkpt[\"model_state_dict\"])\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dl = []\n",
    "\n",
    "    start_row = 0  # Initialize starting row for each new sheet\n",
    "\n",
    "    print(\"Checkpoint\", checkpoint, \"...started\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(batches)):\n",
    "            batch = batches[batch_idx]\n",
    "            new_row = {}\n",
    "            datum = ds.character_tokenizer.encode(batch)\n",
    "            pred = model.generate(\n",
    "                \"o2p\",\n",
    "                datum['enc_input_ids'],\n",
    "                datum['enc_pad_mask'],\n",
    "                None,\n",
    "                None,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            for idx, orth in enumerate(batch):\n",
    "                # Save the original input orthography\n",
    "                new_row[\"word_raw\"] = orth\n",
    "                # Save the target phonology for the above input orthography\n",
    "                phon = ds.cmudict[orth]\n",
    "                new_row[\"phon_target\"] = \":\".join(phon)\n",
    "                # Remove the start and end tokens from each phonological vector\n",
    "                # and convert them from tensors to lists\n",
    "                phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][idx][1:-1]]\n",
    "                # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "                phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "                    phon_pred_features, phonology=True, hot_nodes=True\n",
    "                )\n",
    "                # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "                # separated by colons\n",
    "                phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "                new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "                # Save a boolean indicating whether the prediction was correct\n",
    "                new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "                # Save the phonological features for the target phonology\n",
    "                phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "                if phon_target_features:\n",
    "                    phon_target_features = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(v.item()) for v in vector])\n",
    "                            for vector in phon_target_features[\"targets\"][0]\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    phon_target_features = \"None\"\n",
    "                new_row[\"phon_target_features\"] = phon_target_features\n",
    "                # Save the phonological features for the predicted phonology\n",
    "                phon_prediction_features = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(int(v.item())) for v in vector])\n",
    "                        for vector in pred[\"phon_vecs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "                # Save the phonological probabilities for the predicted phonology\n",
    "                phon_prediction_probabilities = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(v.item()) for v in vector])\n",
    "                        for vector in pred[\"phon_probs\"][idx][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "                # Save the global encoding vector for the predicted phonology\n",
    "                global_encoding = \";\".join(\n",
    "                    \n",
    "                        [\n",
    "                        \":\".join([str(v.item()) for v in vector]) \n",
    "                        for vector in pred[\"global_encoding\"][idx]\n",
    "                        ]\n",
    "                    \n",
    "                    )\n",
    "                new_row[\"global_encoding\"] = global_encoding\n",
    "\n",
    "                \n",
    "                dfa = pd.DataFrame([new_row])\n",
    "                dl.append(dfa)\n",
    "            print(\"Batch\", batch_idx, \"of\", len(batches), \"...done\")\n",
    "    pd.concat(dl).to_csv(outfile, index=False)\n",
    "    print(\"Checkpoint done:\", checkpoint)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
