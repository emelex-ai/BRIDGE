{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import ConnTextULDataset\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import csv\n",
    "from time import time\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache folder: ./data/.cache already exists\n",
      "File ./data/COCA_10140_021624.csv exists\n",
      "COCA_10140_021624.csv\n"
     ]
    }
   ],
   "source": [
    "# Load in a dataset to access Matt's Traindata class which\n",
    "# is embedded inside the phonology tokenizer (consider changing this)\n",
    "ds = ConnTextULDataset(None, which_dataset=\"all\")\n",
    "print(ds.data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Woodcock Johnson III Form A dataset for assessment\n",
    "with open(\"data/wj_iii_form_a.json\", \"r\") as f:\n",
    "    wj3_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template dataframe to populate for each checkpoint\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"word_raw\",\n",
    "        \"phon_target\",\n",
    "        \"phon_prediction\",\n",
    "        \"correct\",\n",
    "        \"phon_target_features\",\n",
    "        \"phon_prediction_features\",\n",
    "        \"phon_prediction_probabilities\",\n",
    "        \"global_encoding\",\n",
    "        \"in_wj3\",\n",
    "        \"in_traindata\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/root_2024-02-20_20h00m12987ms_chkpt001.pth',\n",
       " 'models/root_2024-02-20_20h00m12987ms_chkpt002.pth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the checkpoint file names. We will load one at a time\n",
    "# and generate predictions of the wj3 assessments one at at ime\n",
    "#root_2024-02-12_22h29m10508ms_chkpt004\n",
    "#_chkpt001\n",
    "checkpoints = glob.glob(\"models/root_2024-02-20_20h00m12987ms*.pth\")\n",
    "checkpoints.sort()\n",
    "checkpoints=checkpoints[:-18]\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915\n"
     ]
    }
   ],
   "source": [
    "# Here we take all orthographic words from both WJ3 and the \n",
    "# original dataset and remove duplicates.\n",
    "\n",
    "#df1=pd.read_csv( \"/workspaces/ConnTextUL/data/random_kid_10140_021624.csv\" )\n",
    "df3=pd.read_csv(\"/workspaces/ConnTextUL/data/COCA_10140_021624.csv\")\n",
    "'''\n",
    "df2=pd.read_csv(\"/workspaces/ConnTextUL/data/sidewalk_data.csv\")\n",
    "#\n",
    "df4=pd.read_csv(\"/workspaces/ConnTextUL/data/kidwords_10140_021624.csv\")\n",
    "df5=pd.read_csv(\"/workspaces/ConnTextUL/data/random_adult_10140_021624.csv\")\n",
    "ccdf = pd.concat([df1, df2,df4,df5])\n",
    "ccdf['word_raw']=ccdf['word_raw'].str.lower()\n",
    "ccdf['word_raw']=ccdf['word_raw'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "ccdf=ccdf.dropna(subset=['word_raw'])\n",
    "'''\n",
    "#indices=ds.words\n",
    "#split_index = int(0.8 * len(ds.words))\n",
    "#train_indices = indices[:split_index]\n",
    "#test_indices = indices[split_index:]\n",
    "#all_words=wj3_json.keys()\n",
    "#all_words = set(wj3_json.keys())\n",
    "word_raw = 'word_raw'\n",
    "#all_words = dfa[word_raw].tolist()\n",
    "all_words=set(df3[word_raw])\n",
    "#all_words.update(ds.words)  # Take the union of both sets\n",
    "#in_train=[True if i in set(train_indices) else False for i in all_words ]\n",
    "#in_test= [True if i in set(test_indices) else False for i in all_words] \n",
    "#data_f={\"word_raw\" : list(all_words) ,     \"in_train\":in_train, \"in_test\": in_test }\n",
    "#df_f = pd.DataFrame(data_f)\n",
    "#df_f.to_csv('model_02-12-2024_train_test.csv', index=False)\n",
    "#all_words=set()\n",
    "#all_words.update(df1[word_raw].tolist())\n",
    "#all_words.update(df2[word_raw].tolist())\n",
    "#all_words.update(df3[word_raw].tolist())\n",
    "#all_words.update(df4[word_raw].tolist())\n",
    "#all_words.update(ccdf[word_raw].tolist())\n",
    "\n",
    "#df_f=pd.DataFrame(all_words)\n",
    "#df_f.to_csv('all_words.csv', index=False)\n",
    "#print(len(ccdf[word_raw].tolist()))\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_func(func): \n",
    "    # This function shows the execution time of  \n",
    "    # the function object passed \n",
    "    def wrap_func(*args, **kwargs): \n",
    "        t1 = time() \n",
    "        result = func(*args, **kwargs) \n",
    "        t2 = time() \n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s') \n",
    "        return result \n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = pd.ExcelWriter(\"model_root_2024-02-12_22h29m10508ms_nonsense.xlsx\", engine=\"openpyxl\")\n",
    "@timer_func\n",
    "def running_comp(checkpoints,df):\n",
    "    for checkpoint in tqdm.tqdm(checkpoints):\n",
    "        csv_filename = f\"test__{ds.data_filename}_model_{checkpoint[12:-15]}_checkpoint_{checkpoint[-7:-4]}.csv\"\n",
    "        with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            #writer.writerow([\"word_raw\", \"phon_target\", \"phon_prediction\", \"correct\", \"phon_target_features\", \"phon_prediction_features\", \"phon_prediction_probabilities\", \"global_encoding\"])\n",
    "            \n",
    "            # Write header row\n",
    "            #writer.writerow([\"word_raw\", \"phon_target\", \"phon_prediction\", \"correct\", \"phon_target_features\", \"phon_prediction_features\", \"phon_prediction_probabilities\", \"global_encoding\"])\n",
    "            # Empty the dataframe to prepare for the next checkpoint (aka excel page)\n",
    "            df.drop(df.index, inplace=True)\n",
    "            chkpt = pt.load(checkpoint)\n",
    "            model = chkpt[\"model\"]\n",
    "            #In this loop, orth is the orthographic form of modelthe target word and\n",
    "            #phon is the phonological form of the target word (not phonetic vectors)\n",
    "            for orth in all_words:\n",
    "                new_row = {}\n",
    "                # Since all_words is a union of both WJ3 and the original dataset,\n",
    "                # the phonological form of any orth in all_words is in either WJ3 or\n",
    "                # the original cmudict dataset. We check both below\n",
    "            # in_wj3 = orth in wj3_json.keys()\n",
    "            # new_row[\"in_wj3\"] = in_wj3\n",
    "                in_traindata = orth in ds.words\n",
    "                new_row[\"in_traindata\"] = in_traindata\n",
    "                #if in_traindata:\n",
    "                phon = ds.cmudict[orth]\n",
    "            # else:\n",
    "                #    phon = wj3_json[orth]\n",
    "\n",
    "                datum = ds.character_tokenizer.encode(orth)\n",
    "                pred = chkpt[\"model\"].generate(\n",
    "                    \"o2p\",\n",
    "                    datum[\"enc_input_ids\"],\n",
    "                    datum[\"enc_pad_mask\"],\n",
    "                    datum[\"enc_input_ids\"],\n",
    "                    datum[\"enc_pad_mask\"],\n",
    "                    deterministic=True,\n",
    "                )\n",
    "                # Save the original input orthography\n",
    "                new_row[\"word_raw\"] = orth\n",
    "                # Save the target phonology for the above input orthography\n",
    "                new_row[\"phon_target\"] = \":\".join(phon)\n",
    "                # Remove the start and end tokens from each phonological vector\n",
    "                # and convert them from tensors to lists\n",
    "                phon_pred_features = [tensor.tolist() for tensor in pred[\"phon_tokens\"][1:-1]]\n",
    "                # Convert the phonological vectors to phonemes using Matt's handy dandy routine\n",
    "                phon_pred = ds.phonology_tokenizer.traindata.convert_numeric_prediction(\n",
    "                    phon_pred_features, phonology=True, hot_nodes=True\n",
    "                )\n",
    "                # Save the model's predicted pronunciation for this word. Phonemes are\n",
    "                # separated by colons\n",
    "                phon_pred = [\"None\" if p == None else p for p in phon_pred]\n",
    "                new_row[\"phon_prediction\"] = \":\".join(phon_pred)\n",
    "                # Save a boolean indicating whether the prediction was correct\n",
    "                new_row[\"correct\"] = new_row[\"phon_target\"] == new_row[\"phon_prediction\"]\n",
    "                # Save the phonological features for the target phonology\n",
    "                phon_target_features = ds.phonology_tokenizer.encode([orth])\n",
    "                if phon_target_features:\n",
    "                    phon_target_features = \";\".join(\n",
    "                        [\n",
    "                            \":\".join([str(v.item()) for v in vector])\n",
    "                            for vector in phon_target_features[\"targets\"][0]\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    phon_target_features = \"None\"\n",
    "                new_row[\"phon_target_features\"] = phon_target_features\n",
    "                # Save the phonological features for the predicted phonology\n",
    "                phon_prediction_features = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(int(v.item())) for v in vector])\n",
    "                        for vector in pred[\"phon_vecs\"][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_features\"] = phon_prediction_features\n",
    "                # Save the phonological probabilities for the predicted phonology\n",
    "                phon_prediction_probabilities = \";\".join(\n",
    "                    [\n",
    "                        \":\".join([str(v.item()) for v in vector])\n",
    "                        for vector in pred[\"phon_probs\"][1:-1]\n",
    "                    ]\n",
    "                )\n",
    "                new_row[\"phon_prediction_probabilities\"] = phon_prediction_probabilities\n",
    "                # Save the global encoding vector for the predicted phonology\n",
    "                global_encoding = \":\".join(\n",
    "                    [str(v.item()) for v in pred[\"global_encoding\"].squeeze()]\n",
    "                )\n",
    "                new_row[\"global_encoding\"] = global_encoding\n",
    "                new_row = pd.Series(new_row)\n",
    "                new_row = new_row.to_frame().transpose()\n",
    "                #writer.writerow([new_row[\"word_raw\"], new_row[\"phon_target\"], new_row[\"phon_prediction\"], new_row[\"correct\"], new_row[\"phon_target_features\"], new_row[\"phon_prediction_features\"], new_row[\"phon_prediction_probabilities\"], new_row[\"global_encoding\"]])\n",
    "                df = pd.concat([df, new_row])\n",
    "            df.to_csv(csvfile, index=False)\n",
    "            #df.to_excel(writer, sheet_name=\"epoch \" + checkpoint[-7:-4])\n",
    "    #writer.book.save(\"model_root_2024-02-12_22h29m10508ms_nonsense.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'suspects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrunning_comp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtimer_func.<locals>.wrap_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \n\u001b[1;32m      5\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time() \n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time() \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m executed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t2\u001b[38;5;241m-\u001b[39mt1)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mrunning_comp\u001b[0;34m(checkpoints, df)\u001b[0m\n\u001b[1;32m     26\u001b[0m     new_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_traindata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m in_traindata\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#if in_traindata:\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     phon \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmudict\u001b[49m\u001b[43m[\u001b[49m\u001b[43morth\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#    phon = wj3_json[orth]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     datum \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mcharacter_tokenizer\u001b[38;5;241m.\u001b[39mencode(orth)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'suspects'"
     ]
    }
   ],
   "source": [
    "running_comp(checkpoints,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
